// This file is generated by PolyFHE
#include <cuda.h>
#include <cuda_runtime.h>
#include <chrono>
#include <iostream>
#include <numeric>
#include <vector>
#include <stdio.h>
#include "polyfhe/kernel/device_context.hpp"
#include "polyfhe/kernel/polynomial.cuh"
#include "polyfhe/kernel/ntt.hpp"
#include "polyfhe/kernel/ntt-phantom.hpp"
#include "phantom-fhe/include/phantom.h"
#include "phantom-fhe/include/uintmodmath.cuh"
// Define kernel for subgraph[0], type: Elem
__global__ void Mult_4_Mult_1_Add_2(Params *params, uint64_t *edge_Init_0_4_Mult_4_0, uint64_t *edge_Init_0_5_Mult_4_1, uint64_t *edge_Init_0_6_Mult_1_0, uint64_t *edge_Init_0_7_Mult_1_1, uint64_t *edge_Add_2_0_End_3_0){
    extern __shared__ uint64_t shared[];
    const int start_limb = 0;
    const int end_limb = 18;
    for (int idx = threadIdx.x + blockIdx.x * blockDim.x;idx < params->N * (end_limb - start_limb);idx += blockDim.x * gridDim.x){
        const int l_idx = idx / params->N + start_limb;
        const int n_idx = l_idx * params->N + idx % params->N;
        const uint64_t q = params->qVec[l_idx];
        uint64_t res;
        // Mult_4
        res = xxx_multiply_and_barrett_reduce_uint64(edge_Init_0_4_Mult_4_0[n_idx] , edge_Init_0_5_Mult_4_1[n_idx], q, params->modulus_const_ratio + l_idx * 2);
         shared[0 + threadIdx.x] = res;
        // Mult_1
        res = xxx_multiply_and_barrett_reduce_uint64(edge_Init_0_6_Mult_1_0[n_idx] , edge_Init_0_7_Mult_1_1[n_idx], q, params->modulus_const_ratio + l_idx * 2);
         shared[128 + threadIdx.x] = res;
        // Add_2
        res = shared[0 + threadIdx.x] + shared[128 + threadIdx.x];
        if (res >= q) res -= q;
        edge_Add_2_0_End_3_0[n_idx] = res;
    }
}


// Define kernel for subgraph[1], type: Elem
__global__ void Mult_5(Params *params, uint64_t *edge_Init_0_2_Mult_5_0, uint64_t *edge_Init_0_3_Mult_5_1, uint64_t *edge_Mult_5_0_End_3_1){
    const int start_limb = 0;
    const int end_limb = 18;
    for (int idx = threadIdx.x + blockIdx.x * blockDim.x;idx < params->N * (end_limb - start_limb);idx += blockDim.x * gridDim.x){
        const int l_idx = idx / params->N + start_limb;
        const int n_idx = l_idx * params->N + idx % params->N;
        const uint64_t q = params->qVec[l_idx];
        uint64_t res;
        // Mult_5
        res = xxx_multiply_and_barrett_reduce_uint64(edge_Init_0_2_Mult_5_0[n_idx] , edge_Init_0_3_Mult_5_1[n_idx], q, params->modulus_const_ratio + l_idx * 2);
         edge_Mult_5_0_End_3_1[n_idx] = res;
    }
}


// Define kernel for subgraph[2], type: ElemLimb2
__global__ void Mult_6_iNTTPhase2_7(Params *params, uint64_t *edge_Init_0_0_Mult_6_0, uint64_t *edge_Init_0_1_Mult_6_1, uint64_t *edge_Mult_6_0_End_3_2, uint64_t *edge_iNTTPhase2_7_0_iNTTPhase1_8_0){
    extern __shared__ uint64_t shared[];
    uint64_t reg[8];
    const int start_limb = 0;
    const int end_limb = 18;
    const size_t n_tower = params->N / 8;
    for (size_t tid = blockIdx.x * blockDim.x + threadIdx.x; tid < (end_limb - start_limb) * n_tower; tid += blockDim.x * gridDim.x){
        size_t batch_idx = tid / n_tower;

        // Load data to register
        uint64_t *in = edge_Init_0_0_Mult_6_0;
        #pragma unroll
        for (int l = 0; l < 8; l++){
            reg[l] = *(in + blockIdx.x * blockDim.x * 8 + threadIdx.x * 8 + l);
        }
        __syncthreads();

        // Mult_6
        const size_t idx = blockIdx.x * blockDim.x * 8 + threadIdx.x * 8;
        #pragma unroll
        for (int l = 0; l < 8; l++){
            uint64_t res;
            res = xxx_multiply_and_barrett_reduce_uint64(edge_Init_0_0_Mult_6_0[idx + l], edge_Init_0_1_Mult_6_1[idx + l], params->qVec[batch_idx], params->modulus_const_ratio + batch_idx * 2);
            reg[l] = res;
        }
        #pragma unroll
        for (int l = 0; l < 4; l++){
            asm("st.cs.global.v2.u64 [%0], {%1, %2};" :  : "l"(edge_Mult_6_0_End_3_2 + idx + 2 * l),"l"(reg[2 * l]),"l"(reg[2 * l + 1]));
        }

        // iNTTPhase2_7
        d_poly_inwt_radix8_phase2(params, 18, 0, shared, reg, tid);

        // Store data from register
        const size_t n_group = params->n2 / 8;
        const size_t idx_base = blockIdx.x * blockDim.x * params->per_thread_ntt_size + (threadIdx.x / n_group) * n_group * params->per_thread_ntt_size + (threadIdx.x % n_group);
        uint64_t *out = edge_iNTTPhase2_7_0_iNTTPhase1_8_0;
        #pragma unroll
        for (int l = 0; l < 8; l++){
            *(out + idx_base + n_group * l) = reg[l];
        }
        __syncthreads();
    }
}


// Define kernel for subgraph[3], type: ElemLimb1
__global__ void iNTTPhase1_8_MultConst_9(Params *params, uint64_t *edge_iNTTPhase2_7_0_iNTTPhase1_8_0, uint64_t *edge_MultConst_9_0_BConv_34_0, uint64_t *edge_MultConst_9_1_BConv_31_0, uint64_t *edge_MultConst_9_2_BConv_28_0, uint64_t *edge_MultConst_9_3_BConv_25_0, uint64_t *edge_MultConst_9_4_BConv_22_0, uint64_t *edge_MultConst_9_5_BConv_19_0, uint64_t *edge_MultConst_9_6_BConv_16_0, uint64_t *edge_MultConst_9_7_BConv_13_0, uint64_t *edge_MultConst_9_8_BConv_10_0, uint64_t *partQlHatInv_mod_Ql_concat, uint64_t *partQlHatInv_mod_Ql_concat_shoup){
    extern __shared__ uint64_t shared[];
    uint64_t reg[8];
    const int start_limb = 0;
    const int end_limb = 18;
    uint64_t *in = edge_iNTTPhase2_7_0_iNTTPhase1_8_0;
    for (size_t i = blockIdx.x * blockDim.x + threadIdx.x; i < (params->N / 8 * (end_limb - start_limb)); i += blockDim.x * gridDim.x){
        const size_t n_twr = params->N / 8;
        const size_t n_idx = i % n_twr;
        const size_t twr_idx = i / n_twr;
        const size_t group = params->n1 / 8;
        const size_t pad_tid = threadIdx.x % params->pad;
        const size_t pad_idx = threadIdx.x / params->pad;
        const size_t n_init = n_twr / group * pad_idx + pad_tid + params->pad * (n_idx / (group * params->pad));
        uint64_t *out;
        // iNTTPhase1_8
        d_poly_inplace_inwt_radix8_phase1(in, params, params->L, 0, shared, reg, i);
        // MultConst_9
        #pragma unroll
        for (int l = 0; l < 8; l++){
            reg[l] = phantom::arith::multiply_and_reduce_shoup(reg[l], partQlHatInv_mod_Ql_concat[twr_idx], partQlHatInv_mod_Ql_concat_shoup[twr_idx], params->qVec[twr_idx]);
        }

        // Store data from register
        const size_t idx_out = twr_idx * params->N + n_init;
        out = edge_MultConst_9_0_BConv_34_0;
        #pragma unroll
        for (int l = 0; l < 8; l++) {
            *(out + idx_out + n_twr * l) = reg[l];
        }
        __syncthreads();
    }
}


// Define kernel for subgraph[4], type: ElemSlot
__global__ void BConv_10(Params *params, uint64_t *edge_MultConst_9_8_BConv_10_0, uint64_t *edge_BConv_10_0_NTTPhase1_11_0, const uint64_t *qiHat_mod_pj, const DModulus *ibase, uint64_t ibase_size, const DModulus *obase, uint64_t obase_size, size_t startPartIdx, size_t size_PartQl){
    extern __shared__ uint64_t shared[];
    for (size_t i = threadIdx.x; i < obase_size * ibase_size; i += blockDim.x){
        shared[i] = qiHat_mod_pj[i];
    }
    __syncthreads();
    uint64_t *out = edge_BConv_10_0_NTTPhase1_11_0;
    const int unroll_number = 2;
    for (size_t tid = blockIdx.x * blockDim.x + threadIdx.x; tid < (params->N * obase_size + unroll_number - 1) / unroll_number; tid += blockDim.x * gridDim.x){
        const size_t n_idx = unroll_number * (tid / obase_size);
        const size_t l_idx = tid % obase_size;
        uint64_t res1, res2;
        // BConv_10
        BConvOpNoReg(params, &res1, &res2, edge_MultConst_9_8_BConv_10_0 + params->N * startPartIdx, shared, n_idx, l_idx, ibase, ibase_size, obase, obase_size, startPartIdx, size_PartQl);
        const size_t l_out_idx = l_idx + ((l_idx >= startPartIdx) ? size_PartQl : 0);
        asm("st.cs.global.v2.u64 [%0], {%1, %2};":: "l"(out + l_out_idx * params->N + n_idx),"l"(res1), "l"(res2));
    }
}


// Define kernel for subgraph[5], type: ElemLimb1
__global__ void NTTPhase1_11(Params *params, uint64_t *edge_BConv_10_0_NTTPhase1_11_0, uint64_t *edge_NTTPhase1_11_0_NTTPhase2_12_0, const uint64_t *twiddles, const uint64_t *twiddles_shoup, const DModulus *modulus){
    extern __shared__ uint64_t shared[];
    uint64_t reg[8];
    const int start_limb = 0;
    const int end_limb = 20;
    uint64_t *in = edge_BConv_10_0_NTTPhase1_11_0;
    for (size_t i = blockIdx.x * blockDim.x + threadIdx.x; i < (params->N / 8 * (end_limb - start_limb)); i += blockDim.x * gridDim.x){
        const size_t n_twr = params->N / 8;
        const size_t n_idx = i % n_twr;
        const size_t twr_idx = i / n_twr;
        const size_t group = params->n1 / 8;
        const size_t pad_tid = threadIdx.x % params->pad;
        const size_t pad_idx = threadIdx.x / params->pad;
        const size_t n_init = n_twr / group * pad_idx + pad_tid + params->pad * (n_idx / (group * params->pad));
        uint64_t *out;
        // NTTPhase1_11
        const size_t exclude_start = 16;
        const size_t exclude_end = 18;
        if (twr_idx >= exclude_start && twr_idx < exclude_end){
            continue;
        }

        // Load to register
        #pragma unroll
        for (int l = 0; l < 8; l++){
            reg[l] = *(in + twr_idx * params->N + n_init + n_twr * l);
        }
        const uint64_t size_P = params->K;
        const uint64_t size_QP = params->KL;
        out = edge_NTTPhase1_11_0_NTTPhase2_12_0;
        size_t twr_idx2 = (twr_idx >= start_limb + end_limb - size_P ? size_QP - (start_limb + end_limb - twr_idx) : twr_idx);
        d_poly_fnwt_phase1(params, out, shared, reg, twiddles, twiddles_shoup, modulus, twr_idx, twr_idx2, n_init, i);
    }
}


// Define kernel for subgraph[6], type: ElemLimb2
__global__ void NTTPhase2_12(Params *params, uint64_t *edge_NTTPhase1_11_0_NTTPhase2_12_0, uint64_t *edge_NTTPhase2_12_0_End_3_11, const uint64_t *twiddles, const uint64_t *twiddles_shoup, const DModulus *modulus){
    extern __shared__ uint64_t shared[];
    uint64_t reg[8];
    const int start_limb = 0;
    const int end_limb = 20;
    const size_t n_tower = params->N / 8;
    for (size_t tid = blockIdx.x * blockDim.x + threadIdx.x; tid < (end_limb - start_limb) * n_tower; tid += blockDim.x * gridDim.x){
        size_t batch_idx = tid / n_tower;

        // NTTPhase2_12
        size_t twr_idx = end_limb - 1 - (tid / n_tower) + start_limb;
        const size_t exclude_end = 18;
        const size_t exclude_start = 16;
        if (twr_idx >= exclude_start && twr_idx < exclude_end){continue;}
        uint64_t n_init;
        d_poly_fnwt_phase2(params, edge_NTTPhase1_11_0_NTTPhase2_12_0, shared, reg, twiddles,twiddles_shoup, modulus, end_limb,start_limb, twr_idx, &n_init, tid);
        uint64_t *out_ptr = edge_NTTPhase2_12_0_End_3_11 + twr_idx * params->N;
        #pragma unroll
        for (size_t j = 0; j < 8; j++){
            *(out_ptr + n_init + params->n2 / 8 * j) = reg[j];
        }
        __syncthreads();
    }
}


// Define kernel for subgraph[7], type: ElemSlot
__global__ void BConv_13(Params *params, uint64_t *edge_MultConst_9_7_BConv_13_0, uint64_t *edge_BConv_13_0_NTTPhase1_14_0, const uint64_t *qiHat_mod_pj, const DModulus *ibase, uint64_t ibase_size, const DModulus *obase, uint64_t obase_size, size_t startPartIdx, size_t size_PartQl){
    extern __shared__ uint64_t shared[];
    for (size_t i = threadIdx.x; i < obase_size * ibase_size; i += blockDim.x){
        shared[i] = qiHat_mod_pj[i];
    }
    __syncthreads();
    uint64_t *out = edge_BConv_13_0_NTTPhase1_14_0;
    const int unroll_number = 2;
    for (size_t tid = blockIdx.x * blockDim.x + threadIdx.x; tid < (params->N * obase_size + unroll_number - 1) / unroll_number; tid += blockDim.x * gridDim.x){
        const size_t n_idx = unroll_number * (tid / obase_size);
        const size_t l_idx = tid % obase_size;
        uint64_t res1, res2;
        // BConv_13
        BConvOpNoReg(params, &res1, &res2, edge_MultConst_9_7_BConv_13_0 + params->N * startPartIdx, shared, n_idx, l_idx, ibase, ibase_size, obase, obase_size, startPartIdx, size_PartQl);
        const size_t l_out_idx = l_idx + ((l_idx >= startPartIdx) ? size_PartQl : 0);
        asm("st.cs.global.v2.u64 [%0], {%1, %2};":: "l"(out + l_out_idx * params->N + n_idx),"l"(res1), "l"(res2));
    }
}


// Define kernel for subgraph[8], type: ElemLimb1
__global__ void NTTPhase1_14(Params *params, uint64_t *edge_BConv_13_0_NTTPhase1_14_0, uint64_t *edge_NTTPhase1_14_0_NTTPhase2_15_0, const uint64_t *twiddles, const uint64_t *twiddles_shoup, const DModulus *modulus){
    extern __shared__ uint64_t shared[];
    uint64_t reg[8];
    const int start_limb = 0;
    const int end_limb = 20;
    uint64_t *in = edge_BConv_13_0_NTTPhase1_14_0;
    for (size_t i = blockIdx.x * blockDim.x + threadIdx.x; i < (params->N / 8 * (end_limb - start_limb)); i += blockDim.x * gridDim.x){
        const size_t n_twr = params->N / 8;
        const size_t n_idx = i % n_twr;
        const size_t twr_idx = i / n_twr;
        const size_t group = params->n1 / 8;
        const size_t pad_tid = threadIdx.x % params->pad;
        const size_t pad_idx = threadIdx.x / params->pad;
        const size_t n_init = n_twr / group * pad_idx + pad_tid + params->pad * (n_idx / (group * params->pad));
        uint64_t *out;
        // NTTPhase1_14
        const size_t exclude_start = 14;
        const size_t exclude_end = 16;
        if (twr_idx >= exclude_start && twr_idx < exclude_end){
            continue;
        }

        // Load to register
        #pragma unroll
        for (int l = 0; l < 8; l++){
            reg[l] = *(in + twr_idx * params->N + n_init + n_twr * l);
        }
        const uint64_t size_P = params->K;
        const uint64_t size_QP = params->KL;
        out = edge_NTTPhase1_14_0_NTTPhase2_15_0;
        size_t twr_idx2 = (twr_idx >= start_limb + end_limb - size_P ? size_QP - (start_limb + end_limb - twr_idx) : twr_idx);
        d_poly_fnwt_phase1(params, out, shared, reg, twiddles, twiddles_shoup, modulus, twr_idx, twr_idx2, n_init, i);
    }
}


// Define kernel for subgraph[9], type: ElemLimb2
__global__ void NTTPhase2_15(Params *params, uint64_t *edge_NTTPhase1_14_0_NTTPhase2_15_0, uint64_t *edge_NTTPhase2_15_0_End_3_10, const uint64_t *twiddles, const uint64_t *twiddles_shoup, const DModulus *modulus){
    extern __shared__ uint64_t shared[];
    uint64_t reg[8];
    const int start_limb = 0;
    const int end_limb = 20;
    const size_t n_tower = params->N / 8;
    for (size_t tid = blockIdx.x * blockDim.x + threadIdx.x; tid < (end_limb - start_limb) * n_tower; tid += blockDim.x * gridDim.x){
        size_t batch_idx = tid / n_tower;

        // NTTPhase2_15
        size_t twr_idx = end_limb - 1 - (tid / n_tower) + start_limb;
        const size_t exclude_end = 16;
        const size_t exclude_start = 14;
        if (twr_idx >= exclude_start && twr_idx < exclude_end){continue;}
        uint64_t n_init;
        d_poly_fnwt_phase2(params, edge_NTTPhase1_14_0_NTTPhase2_15_0, shared, reg, twiddles,twiddles_shoup, modulus, end_limb,start_limb, twr_idx, &n_init, tid);
        uint64_t *out_ptr = edge_NTTPhase2_15_0_End_3_10 + twr_idx * params->N;
        #pragma unroll
        for (size_t j = 0; j < 8; j++){
            *(out_ptr + n_init + params->n2 / 8 * j) = reg[j];
        }
        __syncthreads();
    }
}


// Define kernel for subgraph[10], type: ElemSlot
__global__ void BConv_16(Params *params, uint64_t *edge_MultConst_9_6_BConv_16_0, uint64_t *edge_BConv_16_0_NTTPhase1_17_0, const uint64_t *qiHat_mod_pj, const DModulus *ibase, uint64_t ibase_size, const DModulus *obase, uint64_t obase_size, size_t startPartIdx, size_t size_PartQl){
    extern __shared__ uint64_t shared[];
    for (size_t i = threadIdx.x; i < obase_size * ibase_size; i += blockDim.x){
        shared[i] = qiHat_mod_pj[i];
    }
    __syncthreads();
    uint64_t *out = edge_BConv_16_0_NTTPhase1_17_0;
    const int unroll_number = 2;
    for (size_t tid = blockIdx.x * blockDim.x + threadIdx.x; tid < (params->N * obase_size + unroll_number - 1) / unroll_number; tid += blockDim.x * gridDim.x){
        const size_t n_idx = unroll_number * (tid / obase_size);
        const size_t l_idx = tid % obase_size;
        uint64_t res1, res2;
        // BConv_16
        BConvOpNoReg(params, &res1, &res2, edge_MultConst_9_6_BConv_16_0 + params->N * startPartIdx, shared, n_idx, l_idx, ibase, ibase_size, obase, obase_size, startPartIdx, size_PartQl);
        const size_t l_out_idx = l_idx + ((l_idx >= startPartIdx) ? size_PartQl : 0);
        asm("st.cs.global.v2.u64 [%0], {%1, %2};":: "l"(out + l_out_idx * params->N + n_idx),"l"(res1), "l"(res2));
    }
}


// Define kernel for subgraph[11], type: ElemLimb1
__global__ void NTTPhase1_17(Params *params, uint64_t *edge_BConv_16_0_NTTPhase1_17_0, uint64_t *edge_NTTPhase1_17_0_NTTPhase2_18_0, const uint64_t *twiddles, const uint64_t *twiddles_shoup, const DModulus *modulus){
    extern __shared__ uint64_t shared[];
    uint64_t reg[8];
    const int start_limb = 0;
    const int end_limb = 20;
    uint64_t *in = edge_BConv_16_0_NTTPhase1_17_0;
    for (size_t i = blockIdx.x * blockDim.x + threadIdx.x; i < (params->N / 8 * (end_limb - start_limb)); i += blockDim.x * gridDim.x){
        const size_t n_twr = params->N / 8;
        const size_t n_idx = i % n_twr;
        const size_t twr_idx = i / n_twr;
        const size_t group = params->n1 / 8;
        const size_t pad_tid = threadIdx.x % params->pad;
        const size_t pad_idx = threadIdx.x / params->pad;
        const size_t n_init = n_twr / group * pad_idx + pad_tid + params->pad * (n_idx / (group * params->pad));
        uint64_t *out;
        // NTTPhase1_17
        const size_t exclude_start = 12;
        const size_t exclude_end = 14;
        if (twr_idx >= exclude_start && twr_idx < exclude_end){
            continue;
        }

        // Load to register
        #pragma unroll
        for (int l = 0; l < 8; l++){
            reg[l] = *(in + twr_idx * params->N + n_init + n_twr * l);
        }
        const uint64_t size_P = params->K;
        const uint64_t size_QP = params->KL;
        out = edge_NTTPhase1_17_0_NTTPhase2_18_0;
        size_t twr_idx2 = (twr_idx >= start_limb + end_limb - size_P ? size_QP - (start_limb + end_limb - twr_idx) : twr_idx);
        d_poly_fnwt_phase1(params, out, shared, reg, twiddles, twiddles_shoup, modulus, twr_idx, twr_idx2, n_init, i);
    }
}


// Define kernel for subgraph[12], type: ElemLimb2
__global__ void NTTPhase2_18(Params *params, uint64_t *edge_NTTPhase1_17_0_NTTPhase2_18_0, uint64_t *edge_NTTPhase2_18_0_End_3_9, const uint64_t *twiddles, const uint64_t *twiddles_shoup, const DModulus *modulus){
    extern __shared__ uint64_t shared[];
    uint64_t reg[8];
    const int start_limb = 0;
    const int end_limb = 20;
    const size_t n_tower = params->N / 8;
    for (size_t tid = blockIdx.x * blockDim.x + threadIdx.x; tid < (end_limb - start_limb) * n_tower; tid += blockDim.x * gridDim.x){
        size_t batch_idx = tid / n_tower;

        // NTTPhase2_18
        size_t twr_idx = end_limb - 1 - (tid / n_tower) + start_limb;
        const size_t exclude_end = 14;
        const size_t exclude_start = 12;
        if (twr_idx >= exclude_start && twr_idx < exclude_end){continue;}
        uint64_t n_init;
        d_poly_fnwt_phase2(params, edge_NTTPhase1_17_0_NTTPhase2_18_0, shared, reg, twiddles,twiddles_shoup, modulus, end_limb,start_limb, twr_idx, &n_init, tid);
        uint64_t *out_ptr = edge_NTTPhase2_18_0_End_3_9 + twr_idx * params->N;
        #pragma unroll
        for (size_t j = 0; j < 8; j++){
            *(out_ptr + n_init + params->n2 / 8 * j) = reg[j];
        }
        __syncthreads();
    }
}


// Define kernel for subgraph[13], type: ElemSlot
__global__ void BConv_19(Params *params, uint64_t *edge_MultConst_9_5_BConv_19_0, uint64_t *edge_BConv_19_0_NTTPhase1_20_0, const uint64_t *qiHat_mod_pj, const DModulus *ibase, uint64_t ibase_size, const DModulus *obase, uint64_t obase_size, size_t startPartIdx, size_t size_PartQl){
    extern __shared__ uint64_t shared[];
    for (size_t i = threadIdx.x; i < obase_size * ibase_size; i += blockDim.x){
        shared[i] = qiHat_mod_pj[i];
    }
    __syncthreads();
    uint64_t *out = edge_BConv_19_0_NTTPhase1_20_0;
    const int unroll_number = 2;
    for (size_t tid = blockIdx.x * blockDim.x + threadIdx.x; tid < (params->N * obase_size + unroll_number - 1) / unroll_number; tid += blockDim.x * gridDim.x){
        const size_t n_idx = unroll_number * (tid / obase_size);
        const size_t l_idx = tid % obase_size;
        uint64_t res1, res2;
        // BConv_19
        BConvOpNoReg(params, &res1, &res2, edge_MultConst_9_5_BConv_19_0 + params->N * startPartIdx, shared, n_idx, l_idx, ibase, ibase_size, obase, obase_size, startPartIdx, size_PartQl);
        const size_t l_out_idx = l_idx + ((l_idx >= startPartIdx) ? size_PartQl : 0);
        asm("st.cs.global.v2.u64 [%0], {%1, %2};":: "l"(out + l_out_idx * params->N + n_idx),"l"(res1), "l"(res2));
    }
}


// Define kernel for subgraph[14], type: ElemLimb1
__global__ void NTTPhase1_20(Params *params, uint64_t *edge_BConv_19_0_NTTPhase1_20_0, uint64_t *edge_NTTPhase1_20_0_NTTPhase2_21_0, const uint64_t *twiddles, const uint64_t *twiddles_shoup, const DModulus *modulus){
    extern __shared__ uint64_t shared[];
    uint64_t reg[8];
    const int start_limb = 0;
    const int end_limb = 20;
    uint64_t *in = edge_BConv_19_0_NTTPhase1_20_0;
    for (size_t i = blockIdx.x * blockDim.x + threadIdx.x; i < (params->N / 8 * (end_limb - start_limb)); i += blockDim.x * gridDim.x){
        const size_t n_twr = params->N / 8;
        const size_t n_idx = i % n_twr;
        const size_t twr_idx = i / n_twr;
        const size_t group = params->n1 / 8;
        const size_t pad_tid = threadIdx.x % params->pad;
        const size_t pad_idx = threadIdx.x / params->pad;
        const size_t n_init = n_twr / group * pad_idx + pad_tid + params->pad * (n_idx / (group * params->pad));
        uint64_t *out;
        // NTTPhase1_20
        const size_t exclude_start = 10;
        const size_t exclude_end = 12;
        if (twr_idx >= exclude_start && twr_idx < exclude_end){
            continue;
        }

        // Load to register
        #pragma unroll
        for (int l = 0; l < 8; l++){
            reg[l] = *(in + twr_idx * params->N + n_init + n_twr * l);
        }
        const uint64_t size_P = params->K;
        const uint64_t size_QP = params->KL;
        out = edge_NTTPhase1_20_0_NTTPhase2_21_0;
        size_t twr_idx2 = (twr_idx >= start_limb + end_limb - size_P ? size_QP - (start_limb + end_limb - twr_idx) : twr_idx);
        d_poly_fnwt_phase1(params, out, shared, reg, twiddles, twiddles_shoup, modulus, twr_idx, twr_idx2, n_init, i);
    }
}


// Define kernel for subgraph[15], type: ElemLimb2
__global__ void NTTPhase2_21(Params *params, uint64_t *edge_NTTPhase1_20_0_NTTPhase2_21_0, uint64_t *edge_NTTPhase2_21_0_End_3_8, const uint64_t *twiddles, const uint64_t *twiddles_shoup, const DModulus *modulus){
    extern __shared__ uint64_t shared[];
    uint64_t reg[8];
    const int start_limb = 0;
    const int end_limb = 20;
    const size_t n_tower = params->N / 8;
    for (size_t tid = blockIdx.x * blockDim.x + threadIdx.x; tid < (end_limb - start_limb) * n_tower; tid += blockDim.x * gridDim.x){
        size_t batch_idx = tid / n_tower;

        // NTTPhase2_21
        size_t twr_idx = end_limb - 1 - (tid / n_tower) + start_limb;
        const size_t exclude_end = 12;
        const size_t exclude_start = 10;
        if (twr_idx >= exclude_start && twr_idx < exclude_end){continue;}
        uint64_t n_init;
        d_poly_fnwt_phase2(params, edge_NTTPhase1_20_0_NTTPhase2_21_0, shared, reg, twiddles,twiddles_shoup, modulus, end_limb,start_limb, twr_idx, &n_init, tid);
        uint64_t *out_ptr = edge_NTTPhase2_21_0_End_3_8 + twr_idx * params->N;
        #pragma unroll
        for (size_t j = 0; j < 8; j++){
            *(out_ptr + n_init + params->n2 / 8 * j) = reg[j];
        }
        __syncthreads();
    }
}


// Define kernel for subgraph[16], type: ElemSlot
__global__ void BConv_22(Params *params, uint64_t *edge_MultConst_9_4_BConv_22_0, uint64_t *edge_BConv_22_0_NTTPhase1_23_0, const uint64_t *qiHat_mod_pj, const DModulus *ibase, uint64_t ibase_size, const DModulus *obase, uint64_t obase_size, size_t startPartIdx, size_t size_PartQl){
    extern __shared__ uint64_t shared[];
    for (size_t i = threadIdx.x; i < obase_size * ibase_size; i += blockDim.x){
        shared[i] = qiHat_mod_pj[i];
    }
    __syncthreads();
    uint64_t *out = edge_BConv_22_0_NTTPhase1_23_0;
    const int unroll_number = 2;
    for (size_t tid = blockIdx.x * blockDim.x + threadIdx.x; tid < (params->N * obase_size + unroll_number - 1) / unroll_number; tid += blockDim.x * gridDim.x){
        const size_t n_idx = unroll_number * (tid / obase_size);
        const size_t l_idx = tid % obase_size;
        uint64_t res1, res2;
        // BConv_22
        BConvOpNoReg(params, &res1, &res2, edge_MultConst_9_4_BConv_22_0 + params->N * startPartIdx, shared, n_idx, l_idx, ibase, ibase_size, obase, obase_size, startPartIdx, size_PartQl);
        const size_t l_out_idx = l_idx + ((l_idx >= startPartIdx) ? size_PartQl : 0);
        asm("st.cs.global.v2.u64 [%0], {%1, %2};":: "l"(out + l_out_idx * params->N + n_idx),"l"(res1), "l"(res2));
    }
}


// Define kernel for subgraph[17], type: ElemLimb1
__global__ void NTTPhase1_23(Params *params, uint64_t *edge_BConv_22_0_NTTPhase1_23_0, uint64_t *edge_NTTPhase1_23_0_NTTPhase2_24_0, const uint64_t *twiddles, const uint64_t *twiddles_shoup, const DModulus *modulus){
    extern __shared__ uint64_t shared[];
    uint64_t reg[8];
    const int start_limb = 0;
    const int end_limb = 20;
    uint64_t *in = edge_BConv_22_0_NTTPhase1_23_0;
    for (size_t i = blockIdx.x * blockDim.x + threadIdx.x; i < (params->N / 8 * (end_limb - start_limb)); i += blockDim.x * gridDim.x){
        const size_t n_twr = params->N / 8;
        const size_t n_idx = i % n_twr;
        const size_t twr_idx = i / n_twr;
        const size_t group = params->n1 / 8;
        const size_t pad_tid = threadIdx.x % params->pad;
        const size_t pad_idx = threadIdx.x / params->pad;
        const size_t n_init = n_twr / group * pad_idx + pad_tid + params->pad * (n_idx / (group * params->pad));
        uint64_t *out;
        // NTTPhase1_23
        const size_t exclude_start = 8;
        const size_t exclude_end = 10;
        if (twr_idx >= exclude_start && twr_idx < exclude_end){
            continue;
        }

        // Load to register
        #pragma unroll
        for (int l = 0; l < 8; l++){
            reg[l] = *(in + twr_idx * params->N + n_init + n_twr * l);
        }
        const uint64_t size_P = params->K;
        const uint64_t size_QP = params->KL;
        out = edge_NTTPhase1_23_0_NTTPhase2_24_0;
        size_t twr_idx2 = (twr_idx >= start_limb + end_limb - size_P ? size_QP - (start_limb + end_limb - twr_idx) : twr_idx);
        d_poly_fnwt_phase1(params, out, shared, reg, twiddles, twiddles_shoup, modulus, twr_idx, twr_idx2, n_init, i);
    }
}


// Define kernel for subgraph[18], type: ElemLimb2
__global__ void NTTPhase2_24(Params *params, uint64_t *edge_NTTPhase1_23_0_NTTPhase2_24_0, uint64_t *edge_NTTPhase2_24_0_End_3_7, const uint64_t *twiddles, const uint64_t *twiddles_shoup, const DModulus *modulus){
    extern __shared__ uint64_t shared[];
    uint64_t reg[8];
    const int start_limb = 0;
    const int end_limb = 20;
    const size_t n_tower = params->N / 8;
    for (size_t tid = blockIdx.x * blockDim.x + threadIdx.x; tid < (end_limb - start_limb) * n_tower; tid += blockDim.x * gridDim.x){
        size_t batch_idx = tid / n_tower;

        // NTTPhase2_24
        size_t twr_idx = end_limb - 1 - (tid / n_tower) + start_limb;
        const size_t exclude_end = 10;
        const size_t exclude_start = 8;
        if (twr_idx >= exclude_start && twr_idx < exclude_end){continue;}
        uint64_t n_init;
        d_poly_fnwt_phase2(params, edge_NTTPhase1_23_0_NTTPhase2_24_0, shared, reg, twiddles,twiddles_shoup, modulus, end_limb,start_limb, twr_idx, &n_init, tid);
        uint64_t *out_ptr = edge_NTTPhase2_24_0_End_3_7 + twr_idx * params->N;
        #pragma unroll
        for (size_t j = 0; j < 8; j++){
            *(out_ptr + n_init + params->n2 / 8 * j) = reg[j];
        }
        __syncthreads();
    }
}


// Define kernel for subgraph[19], type: ElemSlot
__global__ void BConv_25(Params *params, uint64_t *edge_MultConst_9_3_BConv_25_0, uint64_t *edge_BConv_25_0_NTTPhase1_26_0, const uint64_t *qiHat_mod_pj, const DModulus *ibase, uint64_t ibase_size, const DModulus *obase, uint64_t obase_size, size_t startPartIdx, size_t size_PartQl){
    extern __shared__ uint64_t shared[];
    for (size_t i = threadIdx.x; i < obase_size * ibase_size; i += blockDim.x){
        shared[i] = qiHat_mod_pj[i];
    }
    __syncthreads();
    uint64_t *out = edge_BConv_25_0_NTTPhase1_26_0;
    const int unroll_number = 2;
    for (size_t tid = blockIdx.x * blockDim.x + threadIdx.x; tid < (params->N * obase_size + unroll_number - 1) / unroll_number; tid += blockDim.x * gridDim.x){
        const size_t n_idx = unroll_number * (tid / obase_size);
        const size_t l_idx = tid % obase_size;
        uint64_t res1, res2;
        // BConv_25
        BConvOpNoReg(params, &res1, &res2, edge_MultConst_9_3_BConv_25_0 + params->N * startPartIdx, shared, n_idx, l_idx, ibase, ibase_size, obase, obase_size, startPartIdx, size_PartQl);
        const size_t l_out_idx = l_idx + ((l_idx >= startPartIdx) ? size_PartQl : 0);
        asm("st.cs.global.v2.u64 [%0], {%1, %2};":: "l"(out + l_out_idx * params->N + n_idx),"l"(res1), "l"(res2));
    }
}


// Define kernel for subgraph[20], type: ElemLimb1
__global__ void NTTPhase1_26(Params *params, uint64_t *edge_BConv_25_0_NTTPhase1_26_0, uint64_t *edge_NTTPhase1_26_0_NTTPhase2_27_0, const uint64_t *twiddles, const uint64_t *twiddles_shoup, const DModulus *modulus){
    extern __shared__ uint64_t shared[];
    uint64_t reg[8];
    const int start_limb = 0;
    const int end_limb = 20;
    uint64_t *in = edge_BConv_25_0_NTTPhase1_26_0;
    for (size_t i = blockIdx.x * blockDim.x + threadIdx.x; i < (params->N / 8 * (end_limb - start_limb)); i += blockDim.x * gridDim.x){
        const size_t n_twr = params->N / 8;
        const size_t n_idx = i % n_twr;
        const size_t twr_idx = i / n_twr;
        const size_t group = params->n1 / 8;
        const size_t pad_tid = threadIdx.x % params->pad;
        const size_t pad_idx = threadIdx.x / params->pad;
        const size_t n_init = n_twr / group * pad_idx + pad_tid + params->pad * (n_idx / (group * params->pad));
        uint64_t *out;
        // NTTPhase1_26
        const size_t exclude_start = 6;
        const size_t exclude_end = 8;
        if (twr_idx >= exclude_start && twr_idx < exclude_end){
            continue;
        }

        // Load to register
        #pragma unroll
        for (int l = 0; l < 8; l++){
            reg[l] = *(in + twr_idx * params->N + n_init + n_twr * l);
        }
        const uint64_t size_P = params->K;
        const uint64_t size_QP = params->KL;
        out = edge_NTTPhase1_26_0_NTTPhase2_27_0;
        size_t twr_idx2 = (twr_idx >= start_limb + end_limb - size_P ? size_QP - (start_limb + end_limb - twr_idx) : twr_idx);
        d_poly_fnwt_phase1(params, out, shared, reg, twiddles, twiddles_shoup, modulus, twr_idx, twr_idx2, n_init, i);
    }
}


// Define kernel for subgraph[21], type: ElemLimb2
__global__ void NTTPhase2_27(Params *params, uint64_t *edge_NTTPhase1_26_0_NTTPhase2_27_0, uint64_t *edge_NTTPhase2_27_0_End_3_6, const uint64_t *twiddles, const uint64_t *twiddles_shoup, const DModulus *modulus){
    extern __shared__ uint64_t shared[];
    uint64_t reg[8];
    const int start_limb = 0;
    const int end_limb = 20;
    const size_t n_tower = params->N / 8;
    for (size_t tid = blockIdx.x * blockDim.x + threadIdx.x; tid < (end_limb - start_limb) * n_tower; tid += blockDim.x * gridDim.x){
        size_t batch_idx = tid / n_tower;

        // NTTPhase2_27
        size_t twr_idx = end_limb - 1 - (tid / n_tower) + start_limb;
        const size_t exclude_end = 8;
        const size_t exclude_start = 6;
        if (twr_idx >= exclude_start && twr_idx < exclude_end){continue;}
        uint64_t n_init;
        d_poly_fnwt_phase2(params, edge_NTTPhase1_26_0_NTTPhase2_27_0, shared, reg, twiddles,twiddles_shoup, modulus, end_limb,start_limb, twr_idx, &n_init, tid);
        uint64_t *out_ptr = edge_NTTPhase2_27_0_End_3_6 + twr_idx * params->N;
        #pragma unroll
        for (size_t j = 0; j < 8; j++){
            *(out_ptr + n_init + params->n2 / 8 * j) = reg[j];
        }
        __syncthreads();
    }
}


// Define kernel for subgraph[22], type: ElemSlot
__global__ void BConv_28(Params *params, uint64_t *edge_MultConst_9_2_BConv_28_0, uint64_t *edge_BConv_28_0_NTTPhase1_29_0, const uint64_t *qiHat_mod_pj, const DModulus *ibase, uint64_t ibase_size, const DModulus *obase, uint64_t obase_size, size_t startPartIdx, size_t size_PartQl){
    extern __shared__ uint64_t shared[];
    for (size_t i = threadIdx.x; i < obase_size * ibase_size; i += blockDim.x){
        shared[i] = qiHat_mod_pj[i];
    }
    __syncthreads();
    uint64_t *out = edge_BConv_28_0_NTTPhase1_29_0;
    const int unroll_number = 2;
    for (size_t tid = blockIdx.x * blockDim.x + threadIdx.x; tid < (params->N * obase_size + unroll_number - 1) / unroll_number; tid += blockDim.x * gridDim.x){
        const size_t n_idx = unroll_number * (tid / obase_size);
        const size_t l_idx = tid % obase_size;
        uint64_t res1, res2;
        // BConv_28
        BConvOpNoReg(params, &res1, &res2, edge_MultConst_9_2_BConv_28_0 + params->N * startPartIdx, shared, n_idx, l_idx, ibase, ibase_size, obase, obase_size, startPartIdx, size_PartQl);
        const size_t l_out_idx = l_idx + ((l_idx >= startPartIdx) ? size_PartQl : 0);
        asm("st.cs.global.v2.u64 [%0], {%1, %2};":: "l"(out + l_out_idx * params->N + n_idx),"l"(res1), "l"(res2));
    }
}


// Define kernel for subgraph[23], type: ElemLimb1
__global__ void NTTPhase1_29(Params *params, uint64_t *edge_BConv_28_0_NTTPhase1_29_0, uint64_t *edge_NTTPhase1_29_0_NTTPhase2_30_0, const uint64_t *twiddles, const uint64_t *twiddles_shoup, const DModulus *modulus){
    extern __shared__ uint64_t shared[];
    uint64_t reg[8];
    const int start_limb = 0;
    const int end_limb = 20;
    uint64_t *in = edge_BConv_28_0_NTTPhase1_29_0;
    for (size_t i = blockIdx.x * blockDim.x + threadIdx.x; i < (params->N / 8 * (end_limb - start_limb)); i += blockDim.x * gridDim.x){
        const size_t n_twr = params->N / 8;
        const size_t n_idx = i % n_twr;
        const size_t twr_idx = i / n_twr;
        const size_t group = params->n1 / 8;
        const size_t pad_tid = threadIdx.x % params->pad;
        const size_t pad_idx = threadIdx.x / params->pad;
        const size_t n_init = n_twr / group * pad_idx + pad_tid + params->pad * (n_idx / (group * params->pad));
        uint64_t *out;
        // NTTPhase1_29
        const size_t exclude_start = 4;
        const size_t exclude_end = 6;
        if (twr_idx >= exclude_start && twr_idx < exclude_end){
            continue;
        }

        // Load to register
        #pragma unroll
        for (int l = 0; l < 8; l++){
            reg[l] = *(in + twr_idx * params->N + n_init + n_twr * l);
        }
        const uint64_t size_P = params->K;
        const uint64_t size_QP = params->KL;
        out = edge_NTTPhase1_29_0_NTTPhase2_30_0;
        size_t twr_idx2 = (twr_idx >= start_limb + end_limb - size_P ? size_QP - (start_limb + end_limb - twr_idx) : twr_idx);
        d_poly_fnwt_phase1(params, out, shared, reg, twiddles, twiddles_shoup, modulus, twr_idx, twr_idx2, n_init, i);
    }
}


// Define kernel for subgraph[24], type: ElemLimb2
__global__ void NTTPhase2_30(Params *params, uint64_t *edge_NTTPhase1_29_0_NTTPhase2_30_0, uint64_t *edge_NTTPhase2_30_0_End_3_5, const uint64_t *twiddles, const uint64_t *twiddles_shoup, const DModulus *modulus){
    extern __shared__ uint64_t shared[];
    uint64_t reg[8];
    const int start_limb = 0;
    const int end_limb = 20;
    const size_t n_tower = params->N / 8;
    for (size_t tid = blockIdx.x * blockDim.x + threadIdx.x; tid < (end_limb - start_limb) * n_tower; tid += blockDim.x * gridDim.x){
        size_t batch_idx = tid / n_tower;

        // NTTPhase2_30
        size_t twr_idx = end_limb - 1 - (tid / n_tower) + start_limb;
        const size_t exclude_end = 6;
        const size_t exclude_start = 4;
        if (twr_idx >= exclude_start && twr_idx < exclude_end){continue;}
        uint64_t n_init;
        d_poly_fnwt_phase2(params, edge_NTTPhase1_29_0_NTTPhase2_30_0, shared, reg, twiddles,twiddles_shoup, modulus, end_limb,start_limb, twr_idx, &n_init, tid);
        uint64_t *out_ptr = edge_NTTPhase2_30_0_End_3_5 + twr_idx * params->N;
        #pragma unroll
        for (size_t j = 0; j < 8; j++){
            *(out_ptr + n_init + params->n2 / 8 * j) = reg[j];
        }
        __syncthreads();
    }
}


// Define kernel for subgraph[25], type: ElemSlot
__global__ void BConv_31(Params *params, uint64_t *edge_MultConst_9_1_BConv_31_0, uint64_t *edge_BConv_31_0_NTTPhase1_32_0, const uint64_t *qiHat_mod_pj, const DModulus *ibase, uint64_t ibase_size, const DModulus *obase, uint64_t obase_size, size_t startPartIdx, size_t size_PartQl){
    extern __shared__ uint64_t shared[];
    for (size_t i = threadIdx.x; i < obase_size * ibase_size; i += blockDim.x){
        shared[i] = qiHat_mod_pj[i];
    }
    __syncthreads();
    uint64_t *out = edge_BConv_31_0_NTTPhase1_32_0;
    const int unroll_number = 2;
    for (size_t tid = blockIdx.x * blockDim.x + threadIdx.x; tid < (params->N * obase_size + unroll_number - 1) / unroll_number; tid += blockDim.x * gridDim.x){
        const size_t n_idx = unroll_number * (tid / obase_size);
        const size_t l_idx = tid % obase_size;
        uint64_t res1, res2;
        // BConv_31
        BConvOpNoReg(params, &res1, &res2, edge_MultConst_9_1_BConv_31_0 + params->N * startPartIdx, shared, n_idx, l_idx, ibase, ibase_size, obase, obase_size, startPartIdx, size_PartQl);
        const size_t l_out_idx = l_idx + ((l_idx >= startPartIdx) ? size_PartQl : 0);
        asm("st.cs.global.v2.u64 [%0], {%1, %2};":: "l"(out + l_out_idx * params->N + n_idx),"l"(res1), "l"(res2));
    }
}


// Define kernel for subgraph[26], type: ElemLimb1
__global__ void NTTPhase1_32(Params *params, uint64_t *edge_BConv_31_0_NTTPhase1_32_0, uint64_t *edge_NTTPhase1_32_0_NTTPhase2_33_0, const uint64_t *twiddles, const uint64_t *twiddles_shoup, const DModulus *modulus){
    extern __shared__ uint64_t shared[];
    uint64_t reg[8];
    const int start_limb = 0;
    const int end_limb = 20;
    uint64_t *in = edge_BConv_31_0_NTTPhase1_32_0;
    for (size_t i = blockIdx.x * blockDim.x + threadIdx.x; i < (params->N / 8 * (end_limb - start_limb)); i += blockDim.x * gridDim.x){
        const size_t n_twr = params->N / 8;
        const size_t n_idx = i % n_twr;
        const size_t twr_idx = i / n_twr;
        const size_t group = params->n1 / 8;
        const size_t pad_tid = threadIdx.x % params->pad;
        const size_t pad_idx = threadIdx.x / params->pad;
        const size_t n_init = n_twr / group * pad_idx + pad_tid + params->pad * (n_idx / (group * params->pad));
        uint64_t *out;
        // NTTPhase1_32
        const size_t exclude_start = 2;
        const size_t exclude_end = 4;
        if (twr_idx >= exclude_start && twr_idx < exclude_end){
            continue;
        }

        // Load to register
        #pragma unroll
        for (int l = 0; l < 8; l++){
            reg[l] = *(in + twr_idx * params->N + n_init + n_twr * l);
        }
        const uint64_t size_P = params->K;
        const uint64_t size_QP = params->KL;
        out = edge_NTTPhase1_32_0_NTTPhase2_33_0;
        size_t twr_idx2 = (twr_idx >= start_limb + end_limb - size_P ? size_QP - (start_limb + end_limb - twr_idx) : twr_idx);
        d_poly_fnwt_phase1(params, out, shared, reg, twiddles, twiddles_shoup, modulus, twr_idx, twr_idx2, n_init, i);
    }
}


// Define kernel for subgraph[27], type: ElemLimb2
__global__ void NTTPhase2_33(Params *params, uint64_t *edge_NTTPhase1_32_0_NTTPhase2_33_0, uint64_t *edge_NTTPhase2_33_0_End_3_4, const uint64_t *twiddles, const uint64_t *twiddles_shoup, const DModulus *modulus){
    extern __shared__ uint64_t shared[];
    uint64_t reg[8];
    const int start_limb = 0;
    const int end_limb = 20;
    const size_t n_tower = params->N / 8;
    for (size_t tid = blockIdx.x * blockDim.x + threadIdx.x; tid < (end_limb - start_limb) * n_tower; tid += blockDim.x * gridDim.x){
        size_t batch_idx = tid / n_tower;

        // NTTPhase2_33
        size_t twr_idx = end_limb - 1 - (tid / n_tower) + start_limb;
        const size_t exclude_end = 4;
        const size_t exclude_start = 2;
        if (twr_idx >= exclude_start && twr_idx < exclude_end){continue;}
        uint64_t n_init;
        d_poly_fnwt_phase2(params, edge_NTTPhase1_32_0_NTTPhase2_33_0, shared, reg, twiddles,twiddles_shoup, modulus, end_limb,start_limb, twr_idx, &n_init, tid);
        uint64_t *out_ptr = edge_NTTPhase2_33_0_End_3_4 + twr_idx * params->N;
        #pragma unroll
        for (size_t j = 0; j < 8; j++){
            *(out_ptr + n_init + params->n2 / 8 * j) = reg[j];
        }
        __syncthreads();
    }
}


// Define kernel for subgraph[28], type: ElemSlot
__global__ void BConv_34(Params *params, uint64_t *edge_MultConst_9_0_BConv_34_0, uint64_t *edge_BConv_34_0_NTTPhase1_35_0, const uint64_t *qiHat_mod_pj, const DModulus *ibase, uint64_t ibase_size, const DModulus *obase, uint64_t obase_size, size_t startPartIdx, size_t size_PartQl){
    extern __shared__ uint64_t shared[];
    for (size_t i = threadIdx.x; i < obase_size * ibase_size; i += blockDim.x){
        shared[i] = qiHat_mod_pj[i];
    }
    __syncthreads();
    uint64_t *out = edge_BConv_34_0_NTTPhase1_35_0;
    const int unroll_number = 2;
    for (size_t tid = blockIdx.x * blockDim.x + threadIdx.x; tid < (params->N * obase_size + unroll_number - 1) / unroll_number; tid += blockDim.x * gridDim.x){
        const size_t n_idx = unroll_number * (tid / obase_size);
        const size_t l_idx = tid % obase_size;
        uint64_t res1, res2;
        // BConv_34
        BConvOpNoReg(params, &res1, &res2, edge_MultConst_9_0_BConv_34_0 + params->N * startPartIdx, shared, n_idx, l_idx, ibase, ibase_size, obase, obase_size, startPartIdx, size_PartQl);
        const size_t l_out_idx = l_idx + ((l_idx >= startPartIdx) ? size_PartQl : 0);
        asm("st.cs.global.v2.u64 [%0], {%1, %2};":: "l"(out + l_out_idx * params->N + n_idx),"l"(res1), "l"(res2));
    }
}


// Define kernel for subgraph[29], type: ElemLimb1
__global__ void NTTPhase1_35(Params *params, uint64_t *edge_BConv_34_0_NTTPhase1_35_0, uint64_t *edge_NTTPhase1_35_0_NTTPhase2_36_0, const uint64_t *twiddles, const uint64_t *twiddles_shoup, const DModulus *modulus){
    extern __shared__ uint64_t shared[];
    uint64_t reg[8];
    const int start_limb = 0;
    const int end_limb = 20;
    uint64_t *in = edge_BConv_34_0_NTTPhase1_35_0;
    for (size_t i = blockIdx.x * blockDim.x + threadIdx.x; i < (params->N / 8 * (end_limb - start_limb)); i += blockDim.x * gridDim.x){
        const size_t n_twr = params->N / 8;
        const size_t n_idx = i % n_twr;
        const size_t twr_idx = i / n_twr;
        const size_t group = params->n1 / 8;
        const size_t pad_tid = threadIdx.x % params->pad;
        const size_t pad_idx = threadIdx.x / params->pad;
        const size_t n_init = n_twr / group * pad_idx + pad_tid + params->pad * (n_idx / (group * params->pad));
        uint64_t *out;
        // NTTPhase1_35
        const size_t exclude_start = 0;
        const size_t exclude_end = 2;
        if (twr_idx >= exclude_start && twr_idx < exclude_end){
            continue;
        }

        // Load to register
        #pragma unroll
        for (int l = 0; l < 8; l++){
            reg[l] = *(in + twr_idx * params->N + n_init + n_twr * l);
        }
        const uint64_t size_P = params->K;
        const uint64_t size_QP = params->KL;
        out = edge_NTTPhase1_35_0_NTTPhase2_36_0;
        size_t twr_idx2 = (twr_idx >= start_limb + end_limb - size_P ? size_QP - (start_limb + end_limb - twr_idx) : twr_idx);
        d_poly_fnwt_phase1(params, out, shared, reg, twiddles, twiddles_shoup, modulus, twr_idx, twr_idx2, n_init, i);
    }
}


// Define kernel for subgraph[30], type: ElemLimb2
__global__ void NTTPhase2_36(Params *params, uint64_t *edge_NTTPhase1_35_0_NTTPhase2_36_0, uint64_t *edge_NTTPhase2_36_0_End_3_3, const uint64_t *twiddles, const uint64_t *twiddles_shoup, const DModulus *modulus){
    extern __shared__ uint64_t shared[];
    uint64_t reg[8];
    const int start_limb = 0;
    const int end_limb = 20;
    const size_t n_tower = params->N / 8;
    for (size_t tid = blockIdx.x * blockDim.x + threadIdx.x; tid < (end_limb - start_limb) * n_tower; tid += blockDim.x * gridDim.x){
        size_t batch_idx = tid / n_tower;

        // NTTPhase2_36
        size_t twr_idx = end_limb - 1 - (tid / n_tower) + start_limb;
        const size_t exclude_end = 2;
        if (twr_idx < exclude_end) {continue;}
        uint64_t n_init;
        d_poly_fnwt_phase2(params, edge_NTTPhase1_35_0_NTTPhase2_36_0, shared, reg, twiddles,twiddles_shoup, modulus, end_limb,start_limb, twr_idx, &n_init, tid);
        uint64_t *out_ptr = edge_NTTPhase2_36_0_End_3_3 + twr_idx * params->N;
        #pragma unroll
        for (size_t j = 0; j < 8; j++){
            *(out_ptr + n_init + params->n2 / 8 * j) = reg[j];
        }
        __syncthreads();
    }
}


void entry_kernel(Params *params_d, Params *params_h, PhantomContext &context, uint64_t *in0, uint64_t *in1, uint64_t *out0, uint64_t *out1, bool if_benchmark){
    phantom::DRNSTool *rns_tool = params_h->rns_tools[1];

    // =====================================
    // Input arguments
    // =====================================
    // Edge: Init_0 -> Mult_6
    uint64_t *edge_Init_0_0_Mult_6_0_d = in0 + 589824;
    // Edge: Init_0 -> Mult_6
    uint64_t *edge_Init_0_1_Mult_6_1_d = in1 + 589824;
    // Edge: Init_0 -> Mult_5
    uint64_t *edge_Init_0_2_Mult_5_0_d = in0 + 0;
    // Edge: Init_0 -> Mult_5
    uint64_t *edge_Init_0_3_Mult_5_1_d = in1 + 0;
    // Edge: Init_0 -> Mult_4
    uint64_t *edge_Init_0_4_Mult_4_0_d = in0 + 0;
    // Edge: Init_0 -> Mult_4
    uint64_t *edge_Init_0_5_Mult_4_1_d = in1 + 589824;
    // Edge: Init_0 -> Mult_1
    uint64_t *edge_Init_0_6_Mult_1_0_d = in0 + 589824;
    // Edge: Init_0 -> Mult_1
    uint64_t *edge_Init_0_7_Mult_1_1_d = in1 + 0;

    // =====================================
    // Output arguments
    // =====================================
    // Edge: Add_2 -> End_3
    uint64_t *edge_Add_2_0_End_3_0_d = out0 + 589824;
    // Edge: Mult_5 -> End_3
    uint64_t *edge_Mult_5_0_End_3_1_d = out0 + 0;
    // Edge: Mult_6 -> End_3
    uint64_t *edge_Mult_6_0_End_3_2_d = out0 + 1179648;
    // Edge: NTTPhase2_36 -> End_3
    uint64_t *edge_NTTPhase2_36_0_End_3_3_d = out1 + 0;
    // Edge: NTTPhase2_33 -> End_3
    uint64_t *edge_NTTPhase2_33_0_End_3_4_d = out1 + 655360;
    // Edge: NTTPhase2_30 -> End_3
    uint64_t *edge_NTTPhase2_30_0_End_3_5_d = out1 + 1310720;
    // Edge: NTTPhase2_27 -> End_3
    uint64_t *edge_NTTPhase2_27_0_End_3_6_d = out1 + 1966080;
    // Edge: NTTPhase2_24 -> End_3
    uint64_t *edge_NTTPhase2_24_0_End_3_7_d = out1 + 2621440;
    // Edge: NTTPhase2_21 -> End_3
    uint64_t *edge_NTTPhase2_21_0_End_3_8_d = out1 + 3276800;
    // Edge: NTTPhase2_18 -> End_3
    uint64_t *edge_NTTPhase2_18_0_End_3_9_d = out1 + 3932160;
    // Edge: NTTPhase2_15 -> End_3
    uint64_t *edge_NTTPhase2_15_0_End_3_10_d = out1 + 4587520;
    // Edge: NTTPhase2_12 -> End_3
    uint64_t *edge_NTTPhase2_12_0_End_3_11_d = out1 + 5242880;

    // =====================================
    // Edges
    // Define global edges for GPU
    // =====================================
    // Edge: iNTTPhase2_7 -> iNTTPhase1_8
    uint64_t *edge_iNTTPhase2_7_0_iNTTPhase1_8_0_d;
    checkCudaErrors(cudaMalloc((void**)&edge_iNTTPhase2_7_0_iNTTPhase1_8_0_d, 18 * params_h->N * sizeof(uint64_t)));
    // Edge: MultConst_9 -> BConv_34
    uint64_t *edge_MultConst_9_0_BConv_34_0_d;
    checkCudaErrors(cudaMalloc((void**)&edge_MultConst_9_0_BConv_34_0_d, 18 * params_h->N * sizeof(uint64_t)));
    uint64_t *edge_MultConst_9_1_BConv_31_0_d = edge_MultConst_9_0_BConv_34_0_d;
    uint64_t *edge_MultConst_9_2_BConv_28_0_d = edge_MultConst_9_0_BConv_34_0_d;
    uint64_t *edge_MultConst_9_3_BConv_25_0_d = edge_MultConst_9_0_BConv_34_0_d;
    uint64_t *edge_MultConst_9_4_BConv_22_0_d = edge_MultConst_9_0_BConv_34_0_d;
    uint64_t *edge_MultConst_9_5_BConv_19_0_d = edge_MultConst_9_0_BConv_34_0_d;
    uint64_t *edge_MultConst_9_6_BConv_16_0_d = edge_MultConst_9_0_BConv_34_0_d;
    uint64_t *edge_MultConst_9_7_BConv_13_0_d = edge_MultConst_9_0_BConv_34_0_d;
    uint64_t *edge_MultConst_9_8_BConv_10_0_d = edge_MultConst_9_0_BConv_34_0_d;
    // Edge: BConv_10 -> NTTPhase1_11
    uint64_t *edge_BConv_10_0_NTTPhase1_11_0_d;
    checkCudaErrors(cudaMalloc((void**)&edge_BConv_10_0_NTTPhase1_11_0_d, 20 * params_h->N * sizeof(uint64_t)));
    // Edge: NTTPhase1_11 -> NTTPhase2_12
    uint64_t *edge_NTTPhase1_11_0_NTTPhase2_12_0_d;
    checkCudaErrors(cudaMalloc((void**)&edge_NTTPhase1_11_0_NTTPhase2_12_0_d, 20 * params_h->N * sizeof(uint64_t)));
    // Edge: BConv_13 -> NTTPhase1_14
    uint64_t *edge_BConv_13_0_NTTPhase1_14_0_d;
    checkCudaErrors(cudaMalloc((void**)&edge_BConv_13_0_NTTPhase1_14_0_d, 20 * params_h->N * sizeof(uint64_t)));
    // Edge: NTTPhase1_14 -> NTTPhase2_15
    uint64_t *edge_NTTPhase1_14_0_NTTPhase2_15_0_d;
    checkCudaErrors(cudaMalloc((void**)&edge_NTTPhase1_14_0_NTTPhase2_15_0_d, 20 * params_h->N * sizeof(uint64_t)));
    // Edge: BConv_16 -> NTTPhase1_17
    uint64_t *edge_BConv_16_0_NTTPhase1_17_0_d;
    checkCudaErrors(cudaMalloc((void**)&edge_BConv_16_0_NTTPhase1_17_0_d, 20 * params_h->N * sizeof(uint64_t)));
    // Edge: NTTPhase1_17 -> NTTPhase2_18
    uint64_t *edge_NTTPhase1_17_0_NTTPhase2_18_0_d;
    checkCudaErrors(cudaMalloc((void**)&edge_NTTPhase1_17_0_NTTPhase2_18_0_d, 20 * params_h->N * sizeof(uint64_t)));
    // Edge: BConv_19 -> NTTPhase1_20
    uint64_t *edge_BConv_19_0_NTTPhase1_20_0_d;
    checkCudaErrors(cudaMalloc((void**)&edge_BConv_19_0_NTTPhase1_20_0_d, 20 * params_h->N * sizeof(uint64_t)));
    // Edge: NTTPhase1_20 -> NTTPhase2_21
    uint64_t *edge_NTTPhase1_20_0_NTTPhase2_21_0_d;
    checkCudaErrors(cudaMalloc((void**)&edge_NTTPhase1_20_0_NTTPhase2_21_0_d, 20 * params_h->N * sizeof(uint64_t)));
    // Edge: BConv_22 -> NTTPhase1_23
    uint64_t *edge_BConv_22_0_NTTPhase1_23_0_d;
    checkCudaErrors(cudaMalloc((void**)&edge_BConv_22_0_NTTPhase1_23_0_d, 20 * params_h->N * sizeof(uint64_t)));
    // Edge: NTTPhase1_23 -> NTTPhase2_24
    uint64_t *edge_NTTPhase1_23_0_NTTPhase2_24_0_d;
    checkCudaErrors(cudaMalloc((void**)&edge_NTTPhase1_23_0_NTTPhase2_24_0_d, 20 * params_h->N * sizeof(uint64_t)));
    // Edge: BConv_25 -> NTTPhase1_26
    uint64_t *edge_BConv_25_0_NTTPhase1_26_0_d;
    checkCudaErrors(cudaMalloc((void**)&edge_BConv_25_0_NTTPhase1_26_0_d, 20 * params_h->N * sizeof(uint64_t)));
    // Edge: NTTPhase1_26 -> NTTPhase2_27
    uint64_t *edge_NTTPhase1_26_0_NTTPhase2_27_0_d;
    checkCudaErrors(cudaMalloc((void**)&edge_NTTPhase1_26_0_NTTPhase2_27_0_d, 20 * params_h->N * sizeof(uint64_t)));
    // Edge: BConv_28 -> NTTPhase1_29
    uint64_t *edge_BConv_28_0_NTTPhase1_29_0_d;
    checkCudaErrors(cudaMalloc((void**)&edge_BConv_28_0_NTTPhase1_29_0_d, 20 * params_h->N * sizeof(uint64_t)));
    // Edge: NTTPhase1_29 -> NTTPhase2_30
    uint64_t *edge_NTTPhase1_29_0_NTTPhase2_30_0_d;
    checkCudaErrors(cudaMalloc((void**)&edge_NTTPhase1_29_0_NTTPhase2_30_0_d, 20 * params_h->N * sizeof(uint64_t)));
    // Edge: BConv_31 -> NTTPhase1_32
    uint64_t *edge_BConv_31_0_NTTPhase1_32_0_d;
    checkCudaErrors(cudaMalloc((void**)&edge_BConv_31_0_NTTPhase1_32_0_d, 20 * params_h->N * sizeof(uint64_t)));
    // Edge: NTTPhase1_32 -> NTTPhase2_33
    uint64_t *edge_NTTPhase1_32_0_NTTPhase2_33_0_d;
    checkCudaErrors(cudaMalloc((void**)&edge_NTTPhase1_32_0_NTTPhase2_33_0_d, 20 * params_h->N * sizeof(uint64_t)));
    // Edge: BConv_34 -> NTTPhase1_35
    uint64_t *edge_BConv_34_0_NTTPhase1_35_0_d;
    checkCudaErrors(cudaMalloc((void**)&edge_BConv_34_0_NTTPhase1_35_0_d, 20 * params_h->N * sizeof(uint64_t)));
    // Edge: NTTPhase1_35 -> NTTPhase2_36
    uint64_t *edge_NTTPhase1_35_0_NTTPhase2_36_0_d;
    checkCudaErrors(cudaMalloc((void**)&edge_NTTPhase1_35_0_NTTPhase2_36_0_d, 20 * params_h->N * sizeof(uint64_t)));
    // =====================================
    std::cout << "### Warm up and Test" << std::endl;
    std::cout << "N : " << params_h->N << std::endl;
    std::cout << "L : " << params_h->L << std::endl;
    std::cout << "dnum : " << params_h->dnum << std::endl;
    std::cout << "alpha : " << params_h->alpha << std::endl;

    // =====================================
    // Warm up
    // =====================================
    {
        // Call kernel
        // Timer start
        auto start = std::chrono::high_resolution_clock::now();
        phantom::DRNSTool *drns_tool = params_h->rns_tools[1];
        const int beta = std::ceil((params_h->L + 1) / params_h->alpha);
        Mult_4_Mult_1_Add_2<<<4096, 256, 3072>>>(params_d, edge_Init_0_4_Mult_4_0_d, edge_Init_0_5_Mult_4_1_d, edge_Init_0_6_Mult_1_0_d, edge_Init_0_7_Mult_1_1_d, edge_Add_2_0_End_3_0_d);
        Mult_5<<<4096, 256, 0>>>(params_d, edge_Init_0_2_Mult_5_0_d, edge_Init_0_3_Mult_5_1_d, edge_Mult_5_0_End_3_1_d);
        Mult_6_iNTTPhase2_7<<<4096, 128, params_h->per_thread_ntt_size * 128 * sizeof(uint64_t)>>>(params_d, edge_Init_0_0_Mult_6_0_d, edge_Init_0_1_Mult_6_1_d, edge_Mult_6_0_End_3_2_d, edge_iNTTPhase2_7_0_iNTTPhase1_8_0_d);
        checkCudaErrors(cudaDeviceSynchronize());
        iNTTPhase1_8_MultConst_9<<<4096, (params_h->n1 / 8) * params_h->pad, (params_h->n1 + params_h->pad + 1) * params_h->pad * sizeof(uint64_t)>>>(params_d, edge_iNTTPhase2_7_0_iNTTPhase1_8_0_d, edge_MultConst_9_0_BConv_34_0_d, edge_MultConst_9_1_BConv_31_0_d, edge_MultConst_9_2_BConv_28_0_d, edge_MultConst_9_3_BConv_25_0_d, edge_MultConst_9_4_BConv_22_0_d, edge_MultConst_9_5_BConv_19_0_d, edge_MultConst_9_6_BConv_16_0_d, edge_MultConst_9_7_BConv_13_0_d, edge_MultConst_9_8_BConv_10_0_d, rns_tool->partQlHatInv_mod_Ql_concat(), rns_tool->partQlHatInv_mod_Ql_concat_shoup());
        {
            const size_t beta_idx = 8;
            const size_t startPartIdx = params_h->alpha * beta_idx;
            const size_t size_PartQl = (beta_idx == beta - 1)?(params_h->L - params_h->alpha * (beta - 1)): params_h->alpha;
            auto &bconv_pre = drns_tool->v_base_part_Ql_to_compl_part_QlP_conv()[beta_idx];
            auto &ibase = bconv_pre.ibase();
            auto &obase = bconv_pre.obase();
            constexpr int unroll_factor = 2;
            BConv_10<<<params_h->N * obase.size() / 128 / unroll_factor, 128, (128 * 2 + obase.size()) * ibase.size() * sizeof(uint64_t)>>>(params_d, edge_MultConst_9_8_BConv_10_0_d, edge_BConv_10_0_NTTPhase1_11_0_d, bconv_pre.QHatModp(), bconv_pre.ibase().base(), bconv_pre.ibase().size(), bconv_pre.obase().base(), bconv_pre.obase().size(), startPartIdx, size_PartQl);
        }
        NTTPhase1_11<<<4096, (params_h->n1 / 8) * params_h->pad, (params_h->n1 + params_h->pad + 1) * params_h->pad * sizeof(uint64_t)>>>(params_d, edge_BConv_10_0_NTTPhase1_11_0_d, edge_NTTPhase1_11_0_NTTPhase2_12_0_d, params_h->ntt_tables->twiddle(), params_h->ntt_tables->twiddle_shoup(), params_h->ntt_tables->modulus());
        checkCudaErrors(cudaDeviceSynchronize());
        NTTPhase2_12<<<4096, 128, params_h->per_thread_ntt_size * 128 * sizeof(uint64_t)>>>(params_d, edge_NTTPhase1_11_0_NTTPhase2_12_0_d, edge_NTTPhase2_12_0_End_3_11_d, params_h->ntt_tables->twiddle(), params_h->ntt_tables->twiddle_shoup(), params_h->ntt_tables->modulus());
        {
            const size_t beta_idx = 7;
            const size_t startPartIdx = params_h->alpha * beta_idx;
            const size_t size_PartQl = (beta_idx == beta - 1)?(params_h->L - params_h->alpha * (beta - 1)): params_h->alpha;
            auto &bconv_pre = drns_tool->v_base_part_Ql_to_compl_part_QlP_conv()[beta_idx];
            auto &ibase = bconv_pre.ibase();
            auto &obase = bconv_pre.obase();
            constexpr int unroll_factor = 2;
            BConv_13<<<params_h->N * obase.size() / 128 / unroll_factor, 128, (128 * 2 + obase.size()) * ibase.size() * sizeof(uint64_t)>>>(params_d, edge_MultConst_9_7_BConv_13_0_d, edge_BConv_13_0_NTTPhase1_14_0_d, bconv_pre.QHatModp(), bconv_pre.ibase().base(), bconv_pre.ibase().size(), bconv_pre.obase().base(), bconv_pre.obase().size(), startPartIdx, size_PartQl);
        }
        NTTPhase1_14<<<4096, (params_h->n1 / 8) * params_h->pad, (params_h->n1 + params_h->pad + 1) * params_h->pad * sizeof(uint64_t)>>>(params_d, edge_BConv_13_0_NTTPhase1_14_0_d, edge_NTTPhase1_14_0_NTTPhase2_15_0_d, params_h->ntt_tables->twiddle(), params_h->ntt_tables->twiddle_shoup(), params_h->ntt_tables->modulus());
        checkCudaErrors(cudaDeviceSynchronize());
        NTTPhase2_15<<<4096, 128, params_h->per_thread_ntt_size * 128 * sizeof(uint64_t)>>>(params_d, edge_NTTPhase1_14_0_NTTPhase2_15_0_d, edge_NTTPhase2_15_0_End_3_10_d, params_h->ntt_tables->twiddle(), params_h->ntt_tables->twiddle_shoup(), params_h->ntt_tables->modulus());
        {
            const size_t beta_idx = 6;
            const size_t startPartIdx = params_h->alpha * beta_idx;
            const size_t size_PartQl = (beta_idx == beta - 1)?(params_h->L - params_h->alpha * (beta - 1)): params_h->alpha;
            auto &bconv_pre = drns_tool->v_base_part_Ql_to_compl_part_QlP_conv()[beta_idx];
            auto &ibase = bconv_pre.ibase();
            auto &obase = bconv_pre.obase();
            constexpr int unroll_factor = 2;
            BConv_16<<<params_h->N * obase.size() / 128 / unroll_factor, 128, (128 * 2 + obase.size()) * ibase.size() * sizeof(uint64_t)>>>(params_d, edge_MultConst_9_6_BConv_16_0_d, edge_BConv_16_0_NTTPhase1_17_0_d, bconv_pre.QHatModp(), bconv_pre.ibase().base(), bconv_pre.ibase().size(), bconv_pre.obase().base(), bconv_pre.obase().size(), startPartIdx, size_PartQl);
        }
        NTTPhase1_17<<<4096, (params_h->n1 / 8) * params_h->pad, (params_h->n1 + params_h->pad + 1) * params_h->pad * sizeof(uint64_t)>>>(params_d, edge_BConv_16_0_NTTPhase1_17_0_d, edge_NTTPhase1_17_0_NTTPhase2_18_0_d, params_h->ntt_tables->twiddle(), params_h->ntt_tables->twiddle_shoup(), params_h->ntt_tables->modulus());
        checkCudaErrors(cudaDeviceSynchronize());
        NTTPhase2_18<<<4096, 128, params_h->per_thread_ntt_size * 128 * sizeof(uint64_t)>>>(params_d, edge_NTTPhase1_17_0_NTTPhase2_18_0_d, edge_NTTPhase2_18_0_End_3_9_d, params_h->ntt_tables->twiddle(), params_h->ntt_tables->twiddle_shoup(), params_h->ntt_tables->modulus());
        {
            const size_t beta_idx = 5;
            const size_t startPartIdx = params_h->alpha * beta_idx;
            const size_t size_PartQl = (beta_idx == beta - 1)?(params_h->L - params_h->alpha * (beta - 1)): params_h->alpha;
            auto &bconv_pre = drns_tool->v_base_part_Ql_to_compl_part_QlP_conv()[beta_idx];
            auto &ibase = bconv_pre.ibase();
            auto &obase = bconv_pre.obase();
            constexpr int unroll_factor = 2;
            BConv_19<<<params_h->N * obase.size() / 128 / unroll_factor, 128, (128 * 2 + obase.size()) * ibase.size() * sizeof(uint64_t)>>>(params_d, edge_MultConst_9_5_BConv_19_0_d, edge_BConv_19_0_NTTPhase1_20_0_d, bconv_pre.QHatModp(), bconv_pre.ibase().base(), bconv_pre.ibase().size(), bconv_pre.obase().base(), bconv_pre.obase().size(), startPartIdx, size_PartQl);
        }
        NTTPhase1_20<<<4096, (params_h->n1 / 8) * params_h->pad, (params_h->n1 + params_h->pad + 1) * params_h->pad * sizeof(uint64_t)>>>(params_d, edge_BConv_19_0_NTTPhase1_20_0_d, edge_NTTPhase1_20_0_NTTPhase2_21_0_d, params_h->ntt_tables->twiddle(), params_h->ntt_tables->twiddle_shoup(), params_h->ntt_tables->modulus());
        checkCudaErrors(cudaDeviceSynchronize());
        NTTPhase2_21<<<4096, 128, params_h->per_thread_ntt_size * 128 * sizeof(uint64_t)>>>(params_d, edge_NTTPhase1_20_0_NTTPhase2_21_0_d, edge_NTTPhase2_21_0_End_3_8_d, params_h->ntt_tables->twiddle(), params_h->ntt_tables->twiddle_shoup(), params_h->ntt_tables->modulus());
        {
            const size_t beta_idx = 4;
            const size_t startPartIdx = params_h->alpha * beta_idx;
            const size_t size_PartQl = (beta_idx == beta - 1)?(params_h->L - params_h->alpha * (beta - 1)): params_h->alpha;
            auto &bconv_pre = drns_tool->v_base_part_Ql_to_compl_part_QlP_conv()[beta_idx];
            auto &ibase = bconv_pre.ibase();
            auto &obase = bconv_pre.obase();
            constexpr int unroll_factor = 2;
            BConv_22<<<params_h->N * obase.size() / 128 / unroll_factor, 128, (128 * 2 + obase.size()) * ibase.size() * sizeof(uint64_t)>>>(params_d, edge_MultConst_9_4_BConv_22_0_d, edge_BConv_22_0_NTTPhase1_23_0_d, bconv_pre.QHatModp(), bconv_pre.ibase().base(), bconv_pre.ibase().size(), bconv_pre.obase().base(), bconv_pre.obase().size(), startPartIdx, size_PartQl);
        }
        NTTPhase1_23<<<4096, (params_h->n1 / 8) * params_h->pad, (params_h->n1 + params_h->pad + 1) * params_h->pad * sizeof(uint64_t)>>>(params_d, edge_BConv_22_0_NTTPhase1_23_0_d, edge_NTTPhase1_23_0_NTTPhase2_24_0_d, params_h->ntt_tables->twiddle(), params_h->ntt_tables->twiddle_shoup(), params_h->ntt_tables->modulus());
        checkCudaErrors(cudaDeviceSynchronize());
        NTTPhase2_24<<<4096, 128, params_h->per_thread_ntt_size * 128 * sizeof(uint64_t)>>>(params_d, edge_NTTPhase1_23_0_NTTPhase2_24_0_d, edge_NTTPhase2_24_0_End_3_7_d, params_h->ntt_tables->twiddle(), params_h->ntt_tables->twiddle_shoup(), params_h->ntt_tables->modulus());
        {
            const size_t beta_idx = 3;
            const size_t startPartIdx = params_h->alpha * beta_idx;
            const size_t size_PartQl = (beta_idx == beta - 1)?(params_h->L - params_h->alpha * (beta - 1)): params_h->alpha;
            auto &bconv_pre = drns_tool->v_base_part_Ql_to_compl_part_QlP_conv()[beta_idx];
            auto &ibase = bconv_pre.ibase();
            auto &obase = bconv_pre.obase();
            constexpr int unroll_factor = 2;
            BConv_25<<<params_h->N * obase.size() / 128 / unroll_factor, 128, (128 * 2 + obase.size()) * ibase.size() * sizeof(uint64_t)>>>(params_d, edge_MultConst_9_3_BConv_25_0_d, edge_BConv_25_0_NTTPhase1_26_0_d, bconv_pre.QHatModp(), bconv_pre.ibase().base(), bconv_pre.ibase().size(), bconv_pre.obase().base(), bconv_pre.obase().size(), startPartIdx, size_PartQl);
        }
        NTTPhase1_26<<<4096, (params_h->n1 / 8) * params_h->pad, (params_h->n1 + params_h->pad + 1) * params_h->pad * sizeof(uint64_t)>>>(params_d, edge_BConv_25_0_NTTPhase1_26_0_d, edge_NTTPhase1_26_0_NTTPhase2_27_0_d, params_h->ntt_tables->twiddle(), params_h->ntt_tables->twiddle_shoup(), params_h->ntt_tables->modulus());
        checkCudaErrors(cudaDeviceSynchronize());
        NTTPhase2_27<<<4096, 128, params_h->per_thread_ntt_size * 128 * sizeof(uint64_t)>>>(params_d, edge_NTTPhase1_26_0_NTTPhase2_27_0_d, edge_NTTPhase2_27_0_End_3_6_d, params_h->ntt_tables->twiddle(), params_h->ntt_tables->twiddle_shoup(), params_h->ntt_tables->modulus());
        {
            const size_t beta_idx = 2;
            const size_t startPartIdx = params_h->alpha * beta_idx;
            const size_t size_PartQl = (beta_idx == beta - 1)?(params_h->L - params_h->alpha * (beta - 1)): params_h->alpha;
            auto &bconv_pre = drns_tool->v_base_part_Ql_to_compl_part_QlP_conv()[beta_idx];
            auto &ibase = bconv_pre.ibase();
            auto &obase = bconv_pre.obase();
            constexpr int unroll_factor = 2;
            BConv_28<<<params_h->N * obase.size() / 128 / unroll_factor, 128, (128 * 2 + obase.size()) * ibase.size() * sizeof(uint64_t)>>>(params_d, edge_MultConst_9_2_BConv_28_0_d, edge_BConv_28_0_NTTPhase1_29_0_d, bconv_pre.QHatModp(), bconv_pre.ibase().base(), bconv_pre.ibase().size(), bconv_pre.obase().base(), bconv_pre.obase().size(), startPartIdx, size_PartQl);
        }
        NTTPhase1_29<<<4096, (params_h->n1 / 8) * params_h->pad, (params_h->n1 + params_h->pad + 1) * params_h->pad * sizeof(uint64_t)>>>(params_d, edge_BConv_28_0_NTTPhase1_29_0_d, edge_NTTPhase1_29_0_NTTPhase2_30_0_d, params_h->ntt_tables->twiddle(), params_h->ntt_tables->twiddle_shoup(), params_h->ntt_tables->modulus());
        checkCudaErrors(cudaDeviceSynchronize());
        NTTPhase2_30<<<4096, 128, params_h->per_thread_ntt_size * 128 * sizeof(uint64_t)>>>(params_d, edge_NTTPhase1_29_0_NTTPhase2_30_0_d, edge_NTTPhase2_30_0_End_3_5_d, params_h->ntt_tables->twiddle(), params_h->ntt_tables->twiddle_shoup(), params_h->ntt_tables->modulus());
        {
            const size_t beta_idx = 1;
            const size_t startPartIdx = params_h->alpha * beta_idx;
            const size_t size_PartQl = (beta_idx == beta - 1)?(params_h->L - params_h->alpha * (beta - 1)): params_h->alpha;
            auto &bconv_pre = drns_tool->v_base_part_Ql_to_compl_part_QlP_conv()[beta_idx];
            auto &ibase = bconv_pre.ibase();
            auto &obase = bconv_pre.obase();
            constexpr int unroll_factor = 2;
            BConv_31<<<params_h->N * obase.size() / 128 / unroll_factor, 128, (128 * 2 + obase.size()) * ibase.size() * sizeof(uint64_t)>>>(params_d, edge_MultConst_9_1_BConv_31_0_d, edge_BConv_31_0_NTTPhase1_32_0_d, bconv_pre.QHatModp(), bconv_pre.ibase().base(), bconv_pre.ibase().size(), bconv_pre.obase().base(), bconv_pre.obase().size(), startPartIdx, size_PartQl);
        }
        NTTPhase1_32<<<4096, (params_h->n1 / 8) * params_h->pad, (params_h->n1 + params_h->pad + 1) * params_h->pad * sizeof(uint64_t)>>>(params_d, edge_BConv_31_0_NTTPhase1_32_0_d, edge_NTTPhase1_32_0_NTTPhase2_33_0_d, params_h->ntt_tables->twiddle(), params_h->ntt_tables->twiddle_shoup(), params_h->ntt_tables->modulus());
        checkCudaErrors(cudaDeviceSynchronize());
        NTTPhase2_33<<<4096, 128, params_h->per_thread_ntt_size * 128 * sizeof(uint64_t)>>>(params_d, edge_NTTPhase1_32_0_NTTPhase2_33_0_d, edge_NTTPhase2_33_0_End_3_4_d, params_h->ntt_tables->twiddle(), params_h->ntt_tables->twiddle_shoup(), params_h->ntt_tables->modulus());
        {
            const size_t beta_idx = 0;
            const size_t startPartIdx = params_h->alpha * beta_idx;
            const size_t size_PartQl = (beta_idx == beta - 1)?(params_h->L - params_h->alpha * (beta - 1)): params_h->alpha;
            auto &bconv_pre = drns_tool->v_base_part_Ql_to_compl_part_QlP_conv()[beta_idx];
            auto &ibase = bconv_pre.ibase();
            auto &obase = bconv_pre.obase();
            constexpr int unroll_factor = 2;
            BConv_34<<<params_h->N * obase.size() / 128 / unroll_factor, 128, (128 * 2 + obase.size()) * ibase.size() * sizeof(uint64_t)>>>(params_d, edge_MultConst_9_0_BConv_34_0_d, edge_BConv_34_0_NTTPhase1_35_0_d, bconv_pre.QHatModp(), bconv_pre.ibase().base(), bconv_pre.ibase().size(), bconv_pre.obase().base(), bconv_pre.obase().size(), startPartIdx, size_PartQl);
        }
        NTTPhase1_35<<<4096, (params_h->n1 / 8) * params_h->pad, (params_h->n1 + params_h->pad + 1) * params_h->pad * sizeof(uint64_t)>>>(params_d, edge_BConv_34_0_NTTPhase1_35_0_d, edge_NTTPhase1_35_0_NTTPhase2_36_0_d, params_h->ntt_tables->twiddle(), params_h->ntt_tables->twiddle_shoup(), params_h->ntt_tables->modulus());
        checkCudaErrors(cudaDeviceSynchronize());
        NTTPhase2_36<<<4096, 128, params_h->per_thread_ntt_size * 128 * sizeof(uint64_t)>>>(params_d, edge_NTTPhase1_35_0_NTTPhase2_36_0_d, edge_NTTPhase2_36_0_End_3_3_d, params_h->ntt_tables->twiddle(), params_h->ntt_tables->twiddle_shoup(), params_h->ntt_tables->modulus());
        checkCudaErrors(cudaDeviceSynchronize());
        // Timer Stop
        auto end = std::chrono::high_resolution_clock::now();

    }


    // =====================================
    // Benchmark
    // =====================================
    if (if_benchmark){
        std::cout << "### Benchmark" << std::endl;
        std::vector<double> elapsed_times;
        for (int i = 0; i < 7; i++)
        {

            // Call kernel
            // Timer start
            auto start = std::chrono::high_resolution_clock::now();
            phantom::DRNSTool *drns_tool = params_h->rns_tools[1];
            const int beta = std::ceil((params_h->L + 1) / params_h->alpha);
            Mult_4_Mult_1_Add_2<<<4096, 256, 3072>>>(params_d, edge_Init_0_4_Mult_4_0_d, edge_Init_0_5_Mult_4_1_d, edge_Init_0_6_Mult_1_0_d, edge_Init_0_7_Mult_1_1_d, edge_Add_2_0_End_3_0_d);
            Mult_5<<<4096, 256, 0>>>(params_d, edge_Init_0_2_Mult_5_0_d, edge_Init_0_3_Mult_5_1_d, edge_Mult_5_0_End_3_1_d);
            Mult_6_iNTTPhase2_7<<<4096, 128, params_h->per_thread_ntt_size * 128 * sizeof(uint64_t)>>>(params_d, edge_Init_0_0_Mult_6_0_d, edge_Init_0_1_Mult_6_1_d, edge_Mult_6_0_End_3_2_d, edge_iNTTPhase2_7_0_iNTTPhase1_8_0_d);
            checkCudaErrors(cudaDeviceSynchronize());
            iNTTPhase1_8_MultConst_9<<<4096, (params_h->n1 / 8) * params_h->pad, (params_h->n1 + params_h->pad + 1) * params_h->pad * sizeof(uint64_t)>>>(params_d, edge_iNTTPhase2_7_0_iNTTPhase1_8_0_d, edge_MultConst_9_0_BConv_34_0_d, edge_MultConst_9_1_BConv_31_0_d, edge_MultConst_9_2_BConv_28_0_d, edge_MultConst_9_3_BConv_25_0_d, edge_MultConst_9_4_BConv_22_0_d, edge_MultConst_9_5_BConv_19_0_d, edge_MultConst_9_6_BConv_16_0_d, edge_MultConst_9_7_BConv_13_0_d, edge_MultConst_9_8_BConv_10_0_d, rns_tool->partQlHatInv_mod_Ql_concat(), rns_tool->partQlHatInv_mod_Ql_concat_shoup());
            {
                const size_t beta_idx = 8;
                const size_t startPartIdx = params_h->alpha * beta_idx;
                const size_t size_PartQl = (beta_idx == beta - 1)?(params_h->L - params_h->alpha * (beta - 1)): params_h->alpha;
                auto &bconv_pre = drns_tool->v_base_part_Ql_to_compl_part_QlP_conv()[beta_idx];
                auto &ibase = bconv_pre.ibase();
                auto &obase = bconv_pre.obase();
                constexpr int unroll_factor = 2;
                BConv_10<<<params_h->N * obase.size() / 128 / unroll_factor, 128, (128 * 2 + obase.size()) * ibase.size() * sizeof(uint64_t)>>>(params_d, edge_MultConst_9_8_BConv_10_0_d, edge_BConv_10_0_NTTPhase1_11_0_d, bconv_pre.QHatModp(), bconv_pre.ibase().base(), bconv_pre.ibase().size(), bconv_pre.obase().base(), bconv_pre.obase().size(), startPartIdx, size_PartQl);
            }
            NTTPhase1_11<<<4096, (params_h->n1 / 8) * params_h->pad, (params_h->n1 + params_h->pad + 1) * params_h->pad * sizeof(uint64_t)>>>(params_d, edge_BConv_10_0_NTTPhase1_11_0_d, edge_NTTPhase1_11_0_NTTPhase2_12_0_d, params_h->ntt_tables->twiddle(), params_h->ntt_tables->twiddle_shoup(), params_h->ntt_tables->modulus());
            checkCudaErrors(cudaDeviceSynchronize());
            NTTPhase2_12<<<4096, 128, params_h->per_thread_ntt_size * 128 * sizeof(uint64_t)>>>(params_d, edge_NTTPhase1_11_0_NTTPhase2_12_0_d, edge_NTTPhase2_12_0_End_3_11_d, params_h->ntt_tables->twiddle(), params_h->ntt_tables->twiddle_shoup(), params_h->ntt_tables->modulus());
            {
                const size_t beta_idx = 7;
                const size_t startPartIdx = params_h->alpha * beta_idx;
                const size_t size_PartQl = (beta_idx == beta - 1)?(params_h->L - params_h->alpha * (beta - 1)): params_h->alpha;
                auto &bconv_pre = drns_tool->v_base_part_Ql_to_compl_part_QlP_conv()[beta_idx];
                auto &ibase = bconv_pre.ibase();
                auto &obase = bconv_pre.obase();
                constexpr int unroll_factor = 2;
                BConv_13<<<params_h->N * obase.size() / 128 / unroll_factor, 128, (128 * 2 + obase.size()) * ibase.size() * sizeof(uint64_t)>>>(params_d, edge_MultConst_9_7_BConv_13_0_d, edge_BConv_13_0_NTTPhase1_14_0_d, bconv_pre.QHatModp(), bconv_pre.ibase().base(), bconv_pre.ibase().size(), bconv_pre.obase().base(), bconv_pre.obase().size(), startPartIdx, size_PartQl);
            }
            NTTPhase1_14<<<4096, (params_h->n1 / 8) * params_h->pad, (params_h->n1 + params_h->pad + 1) * params_h->pad * sizeof(uint64_t)>>>(params_d, edge_BConv_13_0_NTTPhase1_14_0_d, edge_NTTPhase1_14_0_NTTPhase2_15_0_d, params_h->ntt_tables->twiddle(), params_h->ntt_tables->twiddle_shoup(), params_h->ntt_tables->modulus());
            checkCudaErrors(cudaDeviceSynchronize());
            NTTPhase2_15<<<4096, 128, params_h->per_thread_ntt_size * 128 * sizeof(uint64_t)>>>(params_d, edge_NTTPhase1_14_0_NTTPhase2_15_0_d, edge_NTTPhase2_15_0_End_3_10_d, params_h->ntt_tables->twiddle(), params_h->ntt_tables->twiddle_shoup(), params_h->ntt_tables->modulus());
            {
                const size_t beta_idx = 6;
                const size_t startPartIdx = params_h->alpha * beta_idx;
                const size_t size_PartQl = (beta_idx == beta - 1)?(params_h->L - params_h->alpha * (beta - 1)): params_h->alpha;
                auto &bconv_pre = drns_tool->v_base_part_Ql_to_compl_part_QlP_conv()[beta_idx];
                auto &ibase = bconv_pre.ibase();
                auto &obase = bconv_pre.obase();
                constexpr int unroll_factor = 2;
                BConv_16<<<params_h->N * obase.size() / 128 / unroll_factor, 128, (128 * 2 + obase.size()) * ibase.size() * sizeof(uint64_t)>>>(params_d, edge_MultConst_9_6_BConv_16_0_d, edge_BConv_16_0_NTTPhase1_17_0_d, bconv_pre.QHatModp(), bconv_pre.ibase().base(), bconv_pre.ibase().size(), bconv_pre.obase().base(), bconv_pre.obase().size(), startPartIdx, size_PartQl);
            }
            NTTPhase1_17<<<4096, (params_h->n1 / 8) * params_h->pad, (params_h->n1 + params_h->pad + 1) * params_h->pad * sizeof(uint64_t)>>>(params_d, edge_BConv_16_0_NTTPhase1_17_0_d, edge_NTTPhase1_17_0_NTTPhase2_18_0_d, params_h->ntt_tables->twiddle(), params_h->ntt_tables->twiddle_shoup(), params_h->ntt_tables->modulus());
            checkCudaErrors(cudaDeviceSynchronize());
            NTTPhase2_18<<<4096, 128, params_h->per_thread_ntt_size * 128 * sizeof(uint64_t)>>>(params_d, edge_NTTPhase1_17_0_NTTPhase2_18_0_d, edge_NTTPhase2_18_0_End_3_9_d, params_h->ntt_tables->twiddle(), params_h->ntt_tables->twiddle_shoup(), params_h->ntt_tables->modulus());
            {
                const size_t beta_idx = 5;
                const size_t startPartIdx = params_h->alpha * beta_idx;
                const size_t size_PartQl = (beta_idx == beta - 1)?(params_h->L - params_h->alpha * (beta - 1)): params_h->alpha;
                auto &bconv_pre = drns_tool->v_base_part_Ql_to_compl_part_QlP_conv()[beta_idx];
                auto &ibase = bconv_pre.ibase();
                auto &obase = bconv_pre.obase();
                constexpr int unroll_factor = 2;
                BConv_19<<<params_h->N * obase.size() / 128 / unroll_factor, 128, (128 * 2 + obase.size()) * ibase.size() * sizeof(uint64_t)>>>(params_d, edge_MultConst_9_5_BConv_19_0_d, edge_BConv_19_0_NTTPhase1_20_0_d, bconv_pre.QHatModp(), bconv_pre.ibase().base(), bconv_pre.ibase().size(), bconv_pre.obase().base(), bconv_pre.obase().size(), startPartIdx, size_PartQl);
            }
            NTTPhase1_20<<<4096, (params_h->n1 / 8) * params_h->pad, (params_h->n1 + params_h->pad + 1) * params_h->pad * sizeof(uint64_t)>>>(params_d, edge_BConv_19_0_NTTPhase1_20_0_d, edge_NTTPhase1_20_0_NTTPhase2_21_0_d, params_h->ntt_tables->twiddle(), params_h->ntt_tables->twiddle_shoup(), params_h->ntt_tables->modulus());
            checkCudaErrors(cudaDeviceSynchronize());
            NTTPhase2_21<<<4096, 128, params_h->per_thread_ntt_size * 128 * sizeof(uint64_t)>>>(params_d, edge_NTTPhase1_20_0_NTTPhase2_21_0_d, edge_NTTPhase2_21_0_End_3_8_d, params_h->ntt_tables->twiddle(), params_h->ntt_tables->twiddle_shoup(), params_h->ntt_tables->modulus());
            {
                const size_t beta_idx = 4;
                const size_t startPartIdx = params_h->alpha * beta_idx;
                const size_t size_PartQl = (beta_idx == beta - 1)?(params_h->L - params_h->alpha * (beta - 1)): params_h->alpha;
                auto &bconv_pre = drns_tool->v_base_part_Ql_to_compl_part_QlP_conv()[beta_idx];
                auto &ibase = bconv_pre.ibase();
                auto &obase = bconv_pre.obase();
                constexpr int unroll_factor = 2;
                BConv_22<<<params_h->N * obase.size() / 128 / unroll_factor, 128, (128 * 2 + obase.size()) * ibase.size() * sizeof(uint64_t)>>>(params_d, edge_MultConst_9_4_BConv_22_0_d, edge_BConv_22_0_NTTPhase1_23_0_d, bconv_pre.QHatModp(), bconv_pre.ibase().base(), bconv_pre.ibase().size(), bconv_pre.obase().base(), bconv_pre.obase().size(), startPartIdx, size_PartQl);
            }
            NTTPhase1_23<<<4096, (params_h->n1 / 8) * params_h->pad, (params_h->n1 + params_h->pad + 1) * params_h->pad * sizeof(uint64_t)>>>(params_d, edge_BConv_22_0_NTTPhase1_23_0_d, edge_NTTPhase1_23_0_NTTPhase2_24_0_d, params_h->ntt_tables->twiddle(), params_h->ntt_tables->twiddle_shoup(), params_h->ntt_tables->modulus());
            checkCudaErrors(cudaDeviceSynchronize());
            NTTPhase2_24<<<4096, 128, params_h->per_thread_ntt_size * 128 * sizeof(uint64_t)>>>(params_d, edge_NTTPhase1_23_0_NTTPhase2_24_0_d, edge_NTTPhase2_24_0_End_3_7_d, params_h->ntt_tables->twiddle(), params_h->ntt_tables->twiddle_shoup(), params_h->ntt_tables->modulus());
            {
                const size_t beta_idx = 3;
                const size_t startPartIdx = params_h->alpha * beta_idx;
                const size_t size_PartQl = (beta_idx == beta - 1)?(params_h->L - params_h->alpha * (beta - 1)): params_h->alpha;
                auto &bconv_pre = drns_tool->v_base_part_Ql_to_compl_part_QlP_conv()[beta_idx];
                auto &ibase = bconv_pre.ibase();
                auto &obase = bconv_pre.obase();
                constexpr int unroll_factor = 2;
                BConv_25<<<params_h->N * obase.size() / 128 / unroll_factor, 128, (128 * 2 + obase.size()) * ibase.size() * sizeof(uint64_t)>>>(params_d, edge_MultConst_9_3_BConv_25_0_d, edge_BConv_25_0_NTTPhase1_26_0_d, bconv_pre.QHatModp(), bconv_pre.ibase().base(), bconv_pre.ibase().size(), bconv_pre.obase().base(), bconv_pre.obase().size(), startPartIdx, size_PartQl);
            }
            NTTPhase1_26<<<4096, (params_h->n1 / 8) * params_h->pad, (params_h->n1 + params_h->pad + 1) * params_h->pad * sizeof(uint64_t)>>>(params_d, edge_BConv_25_0_NTTPhase1_26_0_d, edge_NTTPhase1_26_0_NTTPhase2_27_0_d, params_h->ntt_tables->twiddle(), params_h->ntt_tables->twiddle_shoup(), params_h->ntt_tables->modulus());
            checkCudaErrors(cudaDeviceSynchronize());
            NTTPhase2_27<<<4096, 128, params_h->per_thread_ntt_size * 128 * sizeof(uint64_t)>>>(params_d, edge_NTTPhase1_26_0_NTTPhase2_27_0_d, edge_NTTPhase2_27_0_End_3_6_d, params_h->ntt_tables->twiddle(), params_h->ntt_tables->twiddle_shoup(), params_h->ntt_tables->modulus());
            {
                const size_t beta_idx = 2;
                const size_t startPartIdx = params_h->alpha * beta_idx;
                const size_t size_PartQl = (beta_idx == beta - 1)?(params_h->L - params_h->alpha * (beta - 1)): params_h->alpha;
                auto &bconv_pre = drns_tool->v_base_part_Ql_to_compl_part_QlP_conv()[beta_idx];
                auto &ibase = bconv_pre.ibase();
                auto &obase = bconv_pre.obase();
                constexpr int unroll_factor = 2;
                BConv_28<<<params_h->N * obase.size() / 128 / unroll_factor, 128, (128 * 2 + obase.size()) * ibase.size() * sizeof(uint64_t)>>>(params_d, edge_MultConst_9_2_BConv_28_0_d, edge_BConv_28_0_NTTPhase1_29_0_d, bconv_pre.QHatModp(), bconv_pre.ibase().base(), bconv_pre.ibase().size(), bconv_pre.obase().base(), bconv_pre.obase().size(), startPartIdx, size_PartQl);
            }
            NTTPhase1_29<<<4096, (params_h->n1 / 8) * params_h->pad, (params_h->n1 + params_h->pad + 1) * params_h->pad * sizeof(uint64_t)>>>(params_d, edge_BConv_28_0_NTTPhase1_29_0_d, edge_NTTPhase1_29_0_NTTPhase2_30_0_d, params_h->ntt_tables->twiddle(), params_h->ntt_tables->twiddle_shoup(), params_h->ntt_tables->modulus());
            checkCudaErrors(cudaDeviceSynchronize());
            NTTPhase2_30<<<4096, 128, params_h->per_thread_ntt_size * 128 * sizeof(uint64_t)>>>(params_d, edge_NTTPhase1_29_0_NTTPhase2_30_0_d, edge_NTTPhase2_30_0_End_3_5_d, params_h->ntt_tables->twiddle(), params_h->ntt_tables->twiddle_shoup(), params_h->ntt_tables->modulus());
            {
                const size_t beta_idx = 1;
                const size_t startPartIdx = params_h->alpha * beta_idx;
                const size_t size_PartQl = (beta_idx == beta - 1)?(params_h->L - params_h->alpha * (beta - 1)): params_h->alpha;
                auto &bconv_pre = drns_tool->v_base_part_Ql_to_compl_part_QlP_conv()[beta_idx];
                auto &ibase = bconv_pre.ibase();
                auto &obase = bconv_pre.obase();
                constexpr int unroll_factor = 2;
                BConv_31<<<params_h->N * obase.size() / 128 / unroll_factor, 128, (128 * 2 + obase.size()) * ibase.size() * sizeof(uint64_t)>>>(params_d, edge_MultConst_9_1_BConv_31_0_d, edge_BConv_31_0_NTTPhase1_32_0_d, bconv_pre.QHatModp(), bconv_pre.ibase().base(), bconv_pre.ibase().size(), bconv_pre.obase().base(), bconv_pre.obase().size(), startPartIdx, size_PartQl);
            }
            NTTPhase1_32<<<4096, (params_h->n1 / 8) * params_h->pad, (params_h->n1 + params_h->pad + 1) * params_h->pad * sizeof(uint64_t)>>>(params_d, edge_BConv_31_0_NTTPhase1_32_0_d, edge_NTTPhase1_32_0_NTTPhase2_33_0_d, params_h->ntt_tables->twiddle(), params_h->ntt_tables->twiddle_shoup(), params_h->ntt_tables->modulus());
            checkCudaErrors(cudaDeviceSynchronize());
            NTTPhase2_33<<<4096, 128, params_h->per_thread_ntt_size * 128 * sizeof(uint64_t)>>>(params_d, edge_NTTPhase1_32_0_NTTPhase2_33_0_d, edge_NTTPhase2_33_0_End_3_4_d, params_h->ntt_tables->twiddle(), params_h->ntt_tables->twiddle_shoup(), params_h->ntt_tables->modulus());
            {
                const size_t beta_idx = 0;
                const size_t startPartIdx = params_h->alpha * beta_idx;
                const size_t size_PartQl = (beta_idx == beta - 1)?(params_h->L - params_h->alpha * (beta - 1)): params_h->alpha;
                auto &bconv_pre = drns_tool->v_base_part_Ql_to_compl_part_QlP_conv()[beta_idx];
                auto &ibase = bconv_pre.ibase();
                auto &obase = bconv_pre.obase();
                constexpr int unroll_factor = 2;
                BConv_34<<<params_h->N * obase.size() / 128 / unroll_factor, 128, (128 * 2 + obase.size()) * ibase.size() * sizeof(uint64_t)>>>(params_d, edge_MultConst_9_0_BConv_34_0_d, edge_BConv_34_0_NTTPhase1_35_0_d, bconv_pre.QHatModp(), bconv_pre.ibase().base(), bconv_pre.ibase().size(), bconv_pre.obase().base(), bconv_pre.obase().size(), startPartIdx, size_PartQl);
            }
            NTTPhase1_35<<<4096, (params_h->n1 / 8) * params_h->pad, (params_h->n1 + params_h->pad + 1) * params_h->pad * sizeof(uint64_t)>>>(params_d, edge_BConv_34_0_NTTPhase1_35_0_d, edge_NTTPhase1_35_0_NTTPhase2_36_0_d, params_h->ntt_tables->twiddle(), params_h->ntt_tables->twiddle_shoup(), params_h->ntt_tables->modulus());
            checkCudaErrors(cudaDeviceSynchronize());
            NTTPhase2_36<<<4096, 128, params_h->per_thread_ntt_size * 128 * sizeof(uint64_t)>>>(params_d, edge_NTTPhase1_35_0_NTTPhase2_36_0_d, edge_NTTPhase2_36_0_End_3_3_d, params_h->ntt_tables->twiddle(), params_h->ntt_tables->twiddle_shoup(), params_h->ntt_tables->modulus());
            checkCudaErrors(cudaDeviceSynchronize());
            // Timer Stop
            auto end = std::chrono::high_resolution_clock::now();


            auto elapsed_usec = std::chrono::duration_cast<std::chrono::microseconds>(end - start);
            std::cout << "Elapsed time: " << elapsed_usec.count() << "us" << std::endl;
            if (i != 0) {elapsed_times.push_back(elapsed_usec.count());}
        }
        std::cout << "Average time[us]: " << std::accumulate(elapsed_times.begin(), elapsed_times.end(), 0.0) / elapsed_times.size() << std::endl;
    }
}
