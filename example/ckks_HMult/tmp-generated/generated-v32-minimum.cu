// This file is generated by PolyFHE
#include <cuda.h>
#include <cuda_profiler_api.h>
#include <cuda_runtime.h>
#include <nvToolsExt.h>
#include <stdio.h>

#include <chrono>
#include <iostream>
#include <numeric>
#include <vector>

#include "phantom-fhe/include/phantom.h"
#include "phantom-fhe/include/uintmodmath.cuh"
#include "polyfhe/kernel/device_context.hpp"
#include "polyfhe/kernel/ntt-phantom.hpp"
#include "polyfhe/kernel/ntt.hpp"
#include "polyfhe/kernel/polynomial.cuh"
// Define kernel for subgraph[0], type: ElemLimb2
__global__ void Mult_1_iNTTPhase2_2(
    Params *params, int start_limb, int end_limb,
    uint64_t *edge_Init_0_6_Mult_1_0, uint64_t *edge_Init_0_7_Mult_1_1,
    uint64_t *edge_Mult_1_0_End_11_4,
    uint64_t *edge_iNTTPhase2_2_0_iNTTPhase1_3_0) {
    extern __shared__ uint64_t shared[];
    uint64_t reg[8];
    const size_t n_tower = params->N / 8;
    for (size_t tid = blockIdx.x * blockDim.x + threadIdx.x;
         tid < (end_limb - start_limb) * n_tower;
         tid += blockDim.x * gridDim.x) {
        // Load data to register
        const int twr_idx = tid / params->N + start_limb;
        uint64_t *in = edge_Init_0_6_Mult_1_0 + twr_idx * params->N +
                       blockIdx.x * blockDim.x * 8 + threadIdx.x * 8;
#pragma unroll
        for (int l = 0; l < 4; l++) {
            load_two_uint64_v2(in + 2 * l, reg[2 * l], reg[2 * l + 1]);
        }
        __syncthreads();
        size_t batch_idx = tid / n_tower;

        // Mult_1
        const size_t idx = blockIdx.x * blockDim.x * 8 + threadIdx.x * 8;
#pragma unroll
        for (int l = 0; l < 8; l++) {
            uint64_t res;
            res = xxx_multiply_and_barrett_reduce_uint64(
                edge_Init_0_6_Mult_1_0[idx + l],
                edge_Init_0_7_Mult_1_1[idx + l], params->qVec[batch_idx],
                params->modulus_const_ratio + batch_idx * 2);
            reg[l] = res;
        }
#pragma unroll
        for (int l = 0; l < 4; l++) {
            asm("st.cs.global.v2.u64 [%0], {%1, %2};"
                :
                : "l"(edge_Mult_1_0_End_11_4 + idx + 2 * l), "l"(reg[2 * l]),
                  "l"(reg[2 * l + 1]));
        }

        // iNTTPhase2_2
        d_poly_inwt_radix8_phase2(params, 0, shared, reg, tid);

        // Store data from register
        const size_t n_group = params->n2 / 8;
        const size_t idx_base =
            start_limb * params->N +
            blockIdx.x * blockDim.x * params->per_thread_ntt_size +
            (threadIdx.x / n_group) * n_group * params->per_thread_ntt_size +
            (threadIdx.x % n_group);
        uint64_t *out = edge_iNTTPhase2_2_0_iNTTPhase1_3_0;
#pragma unroll
        for (int l = 0; l < 8; l++) {
            *(out + idx_base + n_group * l) = reg[l];
        }
        __syncthreads();
    }
}

// Define kernel for subgraph[1], type: ElemLimb1
__global__ void iNTTPhase1_3_MultConst_4(
    Params *params, int start_limb, int end_limb,
    uint64_t *edge_iNTTPhase2_2_0_iNTTPhase1_3_0,
    uint64_t *edge_MultConst_4_0_BConv_23_0,
    uint64_t *partQlHatInv_mod_Ql_concat,
    uint64_t *partQlHatInv_mod_Ql_concat_shoup) {
    extern __shared__ uint64_t shared[];
    uint64_t reg[8];
    uint64_t *in = edge_iNTTPhase2_2_0_iNTTPhase1_3_0;
    for (size_t i = blockIdx.x * blockDim.x + threadIdx.x;
         i < (params->N / 8 * (end_limb - start_limb));
         i += blockDim.x * gridDim.x) {
        const size_t n_twr = params->N / 8;
        const size_t n_idx = i % n_twr;
        const size_t twr_idx = i / n_twr + start_limb;
        const size_t group = params->n1 / 8;
        const size_t pad_tid = threadIdx.x % params->pad;
        const size_t pad_idx = threadIdx.x / params->pad;
        const size_t n_init = n_twr / group * pad_idx + pad_tid +
                              params->pad * (n_idx / (group * params->pad));
        uint64_t *out;
        // iNTTPhase1_3
        d_poly_inplace_inwt_radix8_phase1(in, params, start_limb, shared, reg,
                                          i);
// MultConst_4
#pragma unroll
        for (int l = 0; l < 8; l++) {
            reg[l] = xxx_multiply_and_reduce_shoup(
                reg[l], partQlHatInv_mod_Ql_concat[twr_idx],
                partQlHatInv_mod_Ql_concat_shoup[twr_idx],
                params->qVec[twr_idx]);
        }

        // Store data from register
        const size_t idx_out = twr_idx * params->N + n_init;
        out = edge_MultConst_4_0_BConv_23_0;
#pragma unroll
        for (int l = 0; l < 8; l++) {
            *(out + idx_out + n_twr * l) = reg[l];
        }
        __syncthreads();
    }
}

// Define kernel for subgraph[22], type: Elem
__global__ void Mult_28_Mult_26_Add_27(Params *params, int start_limb,
                                       int end_limb,
                                       uint64_t *edge_Init_0_2_Mult_28_0,
                                       uint64_t *edge_Init_0_3_Mult_28_1,
                                       uint64_t *edge_Init_0_4_Mult_26_0,
                                       uint64_t *edge_Init_0_5_Mult_26_1,
                                       uint64_t *edge_Add_27_0_End_11_0) {
    for (int idx = threadIdx.x + blockIdx.x * blockDim.x;
         idx < params->N * (end_limb - start_limb);
         idx += blockDim.x * gridDim.x) {
        const int l_idx = idx / params->N + start_limb;
        const uint64_t q = params->qVec[l_idx];
        uint64_t res;
        const int n_idx = l_idx * params->N + idx % params->N;
        // Mult_28
        uint64_t res1 = xxx_multiply_and_barrett_reduce_uint64(
            edge_Init_0_2_Mult_28_0[n_idx], edge_Init_0_3_Mult_28_1[n_idx], q,
            params->modulus_const_ratio + l_idx * 2);
        // Mult_26
        uint64_t res2 = xxx_multiply_and_barrett_reduce_uint64(
            edge_Init_0_4_Mult_26_0[n_idx], edge_Init_0_5_Mult_26_1[n_idx], q,
            params->modulus_const_ratio + l_idx * 2);
        // Add_27
        res = res1 + res2;
        if (res >= q)
            res -= q;
        edge_Add_27_0_End_11_0[n_idx] = res;
    }
}

// Define kernel for subgraph[23], type: Elem
__global__ void Mult_29(Params *params, int start_limb, int end_limb,
                        uint64_t *edge_Init_0_0_Mult_29_0,
                        uint64_t *edge_Init_0_1_Mult_29_1,
                        uint64_t *edge_Mult_29_0_End_11_3) {
    for (int idx = threadIdx.x + blockIdx.x * blockDim.x;
         idx < params->N * (end_limb - start_limb);
         idx += blockDim.x * gridDim.x) {
        const int l_idx = idx / params->N + start_limb;
        const uint64_t q = params->qVec[l_idx];
        uint64_t res;
        const int n_idx = l_idx * params->N + idx % params->N;
        // Mult_29
        res = xxx_multiply_and_barrett_reduce_uint64(
            edge_Init_0_0_Mult_29_0[n_idx], edge_Init_0_1_Mult_29_1[n_idx], q,
            params->modulus_const_ratio + l_idx * 2);
        edge_Mult_29_0_End_11_3[n_idx] = res;
    }
}

void entry_kernel(Params *params_d, Params *params_h, PhantomContext &context,
                  uint64_t **relin_keys, uint64_t *in0, uint64_t *in1,
                  uint64_t *out0, uint64_t *out1, bool if_benchmark,
                  int n_divide) {
    phantom::DRNSTool *rns_tool = params_h->rns_tools[1];

    // =====================================
    // Input arguments
    // =====================================
    // Edge: Init_0 -> Mult_29
    uint64_t *edge_Init_0_0_Mult_29_0_d = in0 + 0;
    // Edge: Init_0 -> Mult_29
    uint64_t *edge_Init_0_1_Mult_29_1_d = in1 + 0;
    // Edge: Init_0 -> Mult_28
    uint64_t *edge_Init_0_2_Mult_28_0_d = in0 + 0;
    // Edge: Init_0 -> Mult_28
    uint64_t *edge_Init_0_3_Mult_28_1_d = in1 + 1966080;
    // Edge: Init_0 -> Mult_26
    uint64_t *edge_Init_0_4_Mult_26_0_d = in0 + 1966080;
    // Edge: Init_0 -> Mult_26
    uint64_t *edge_Init_0_5_Mult_26_1_d = in1 + 0;
    // Edge: Init_0 -> Mult_1
    uint64_t *edge_Init_0_6_Mult_1_0_d = in0 + 1966080;
    // Edge: Init_0 -> Mult_1
    uint64_t *edge_Init_0_7_Mult_1_1_d = in1 + 1966080;

    // =====================================
    // Output arguments
    // =====================================
    // Edge: Add_27 -> End_11
    uint64_t *edge_Add_27_0_End_11_0_d = out0 + 1966080;
    // Edge: iNTTPhase1_13 -> End_11
    uint64_t *edge_iNTTPhase1_13_0_End_11_1_d = out1 + 0;
    // Edge: iNTTPhase1_10 -> End_11
    uint64_t *edge_iNTTPhase1_10_0_End_11_2_d = out1 + 2359296;
    // Edge: Mult_29 -> End_11
    uint64_t *edge_Mult_29_0_End_11_3_d = out0 + 0;
    // Edge: Mult_1 -> End_11
    uint64_t *edge_Mult_1_0_End_11_4_d = out0 + 3932160;

    // =====================================
    // Edges
    // Define global edges for GPU
    // =====================================
    // Edge: iNTTPhase2_2 -> iNTTPhase1_3
    uint64_t *edge_iNTTPhase2_2_0_iNTTPhase1_3_0_d;
    checkCudaErrors(cudaMalloc((void **) &edge_iNTTPhase2_2_0_iNTTPhase1_3_0_d,
                               30 * params_h->N * sizeof(uint64_t)));
    // Edge: MultConst_4 -> BConv_23
    uint64_t *edge_MultConst_4_0_BConv_23_0_d;
    checkCudaErrors(cudaMalloc((void **) &edge_MultConst_4_0_BConv_23_0_d,
                               36 * params_h->N * sizeof(uint64_t)));
    // Edge: NTTPhase2_7 -> MultKeyAccum_8
    uint64_t *edge_NTTPhase2_7_0_MultKeyAccum_8_4_d;
    checkCudaErrors(cudaMalloc((void **) &edge_NTTPhase2_7_0_MultKeyAccum_8_4_d,
                               36 * params_h->N * sizeof(uint64_t)));
    // Edge: NTTPhase2_16 -> MultKeyAccum_8
    uint64_t *edge_NTTPhase2_16_0_MultKeyAccum_8_3_d;
    checkCudaErrors(
        cudaMalloc((void **) &edge_NTTPhase2_16_0_MultKeyAccum_8_3_d,
                   36 * params_h->N * sizeof(uint64_t)));
    // Edge: NTTPhase2_19 -> MultKeyAccum_8
    uint64_t *edge_NTTPhase2_19_0_MultKeyAccum_8_2_d;
    checkCudaErrors(
        cudaMalloc((void **) &edge_NTTPhase2_19_0_MultKeyAccum_8_2_d,
                   36 * params_h->N * sizeof(uint64_t)));
    // Edge: NTTPhase2_22 -> MultKeyAccum_8
    uint64_t *edge_NTTPhase2_22_0_MultKeyAccum_8_1_d;
    checkCudaErrors(
        cudaMalloc((void **) &edge_NTTPhase2_22_0_MultKeyAccum_8_1_d,
                   36 * params_h->N * sizeof(uint64_t)));
    // uint64_t *edge_NTTPhase2_22_0_MultKeyAccum_8_1_d =
    // edge_iNTTPhase1_10_0_End_11_2_d;

    // Edge:NTTPhase2_25 -> MultKeyAccum_8
    uint64_t *edge_NTTPhase2_25_0_MultKeyAccum_8_0_d;
    checkCudaErrors(
        cudaMalloc((void **) &edge_NTTPhase2_25_0_MultKeyAccum_8_0_d,
                   36 * params_h->N * sizeof(uint64_t)));
    // uint64_t *edge_NTTPhase2_25_0_MultKeyAccum_8_0_d =
    // edge_iNTTPhase1_13_0_End_11_1_d;
    uint64_t *edge_MultConst_4_1_BConv_20_0_d = edge_MultConst_4_0_BConv_23_0_d;
    uint64_t *edge_MultConst_4_2_BConv_17_0_d = edge_MultConst_4_0_BConv_23_0_d;
    uint64_t *edge_MultConst_4_3_BConv_14_0_d = edge_MultConst_4_0_BConv_23_0_d;
    uint64_t *edge_MultConst_4_4_BConv_5_0_d = edge_MultConst_4_0_BConv_23_0_d;
    uint64_t *edge_BConv_5_0_NTTPhase1_6_0_d =
        edge_NTTPhase2_7_0_MultKeyAccum_8_4_d;
    // uint64_t *edge_NTTPhase1_6_0_NTTPhase2_7_0_d =
    //     edge_NTTPhase2_7_0_MultKeyAccum_8_4_d;
    uint64_t *edge_BConv_14_0_NTTPhase1_15_0_d =
        edge_NTTPhase2_16_0_MultKeyAccum_8_3_d;
    // uint64_t *edge_NTTPhase1_15_0_NTTPhase2_16_0_d =
    //     edge_NTTPhase2_16_0_MultKeyAccum_8_3_d;
    uint64_t *edge_BConv_17_0_NTTPhase1_18_0_d =
        edge_NTTPhase2_19_0_MultKeyAccum_8_2_d;
    // uint64_t *edge_NTTPhase1_18_0_NTTPhase2_19_0_d =
    //     edge_NTTPhase2_19_0_MultKeyAccum_8_2_d;
    uint64_t *edge_BConv_20_0_NTTPhase1_21_0_d =
        edge_NTTPhase2_22_0_MultKeyAccum_8_1_d;
    // uint64_t *edge_NTTPhase1_21_0_NTTPhase2_22_0_d =
    //     edge_NTTPhase2_22_0_MultKeyAccum_8_1_d;
    uint64_t *edge_BConv_23_0_NTTPhase1_24_0_d =
        edge_NTTPhase2_25_0_MultKeyAccum_8_0_d;
    // uint64_t *edge_NTTPhase1_24_0_NTTPhase2_25_0_d =
    //     edge_NTTPhase2_25_0_MultKeyAccum_8_0_d;
    uint64_t *edge_MultKeyAccum_8_0_iNTTPhase2_12_0_d =
        edge_iNTTPhase1_13_0_End_11_1_d;
    uint64_t *edge_MultKeyAccum_8_1_iNTTPhase2_9_0_d =
        edge_iNTTPhase1_10_0_End_11_2_d;
    uint64_t *edge_iNTTPhase2_9_0_iNTTPhase1_10_0_d =
        edge_iNTTPhase1_10_0_End_11_2_d;
    uint64_t *edge_iNTTPhase2_12_0_iNTTPhase1_13_0_d =
        edge_iNTTPhase1_13_0_End_11_1_d;
    // =====================================
    std::cout << "### Warm up and Test" << std::endl;
    std::cout << "N : " << params_h->N << std::endl;
    std::cout << "L : " << params_h->L << std::endl;
    std::cout << "KL : " << params_h->KL << std::endl;
    std::cout << "dnum : " << params_h->dnum << std::endl;
    std::cout << "alpha : " << params_h->alpha << std::endl;

    const int current_limb = params_h->L;
    const int modup_limb = params_h->KL;

    // =====================================
    // Benchmark
    // =====================================
    if (if_benchmark) {
        std::cout << "### Benchmark" << std::endl;
        std::vector<double> elapsed_times;
        std::vector<double> elapsed_times_ce;

        phantom::DRNSTool *drns_tool = params_h->rns_tools[1];
        const int beta = std::ceil((params_h->L + 1) / params_h->alpha);

        uint64_t **in_list = new uint64_t *[beta];
        in_list[0] = edge_NTTPhase2_25_0_MultKeyAccum_8_0_d;
        in_list[1] = edge_NTTPhase2_22_0_MultKeyAccum_8_1_d;
        in_list[2] = edge_NTTPhase2_19_0_MultKeyAccum_8_2_d;
        in_list[3] = edge_NTTPhase2_16_0_MultKeyAccum_8_3_d;
        in_list[4] = edge_NTTPhase2_7_0_MultKeyAccum_8_4_d;
        uint64_t **d_in_list;

        checkCudaErrors(
            cudaMalloc((void **) &d_in_list, sizeof(uint64_t *) * beta));
        checkCudaErrors(cudaMemcpy(d_in_list, in_list,
                                   sizeof(uint64_t *) * beta,
                                   cudaMemcpyHostToDevice));

        uint64_t **bconv_in_list = new uint64_t *[beta];
        bconv_in_list[0] = edge_MultConst_4_0_BConv_23_0_d;
        bconv_in_list[1] = edge_MultConst_4_1_BConv_20_0_d;
        bconv_in_list[2] = edge_MultConst_4_2_BConv_17_0_d;
        bconv_in_list[3] = edge_MultConst_4_3_BConv_14_0_d;
        bconv_in_list[4] = edge_MultConst_4_4_BConv_5_0_d;
        uint64_t **d_bconv_in_list;
        checkCudaErrors(
            cudaMalloc((void **) &d_bconv_in_list, sizeof(uint64_t *) * beta));
        cudaMemcpy(d_bconv_in_list, bconv_in_list, sizeof(uint64_t *) * beta,
                   cudaMemcpyHostToDevice);

        uint64_t **bconv_out_list = new uint64_t *[beta];
        bconv_out_list[0] = edge_BConv_23_0_NTTPhase1_24_0_d;
        bconv_out_list[1] = edge_BConv_20_0_NTTPhase1_21_0_d;
        bconv_out_list[2] = edge_BConv_17_0_NTTPhase1_18_0_d;
        bconv_out_list[3] = edge_BConv_14_0_NTTPhase1_15_0_d;
        bconv_out_list[4] = edge_BConv_5_0_NTTPhase1_6_0_d;
        uint64_t **d_bconv_out_list;
        checkCudaErrors(
            cudaMalloc((void **) &d_bconv_out_list, sizeof(uint64_t *) * beta));
        cudaMemcpy(d_bconv_out_list, bconv_out_list, sizeof(uint64_t *) * beta,
                   cudaMemcpyHostToDevice);

        uint64_t **qhat_modp_list = new uint64_t *[beta];
        for (int j = 0; j < beta; j++) {
            qhat_modp_list[j] =
                drns_tool->v_base_part_Ql_to_compl_part_QlP_conv()[j]
                    .QHatModp();
        }
        uint64_t **d_qhat_modp_list;
        checkCudaErrors(
            cudaMalloc((void **) &d_qhat_modp_list, sizeof(uint64_t *) * beta));
        checkCudaErrors(cudaMemcpyAsync(d_qhat_modp_list, qhat_modp_list,
                                        sizeof(uint64_t *) * beta,
                                        cudaMemcpyHostToDevice));

        for (int i = 0; i < 10; i++) {
            // Call kernel
            // Timer start
            nvtxRangePushA("compute");
            cudaEvent_t ce_start, ce_stop;
            cudaEventCreate(&ce_start);
            cudaEventCreate(&ce_stop);
            auto start = std::chrono::high_resolution_clock::now();
            cudaEventRecord(ce_start);

            Mult_28_Mult_26_Add_27<<<4096, 256>>>(
                params_d, 0, current_limb, edge_Init_0_2_Mult_28_0_d,
                edge_Init_0_3_Mult_28_1_d, edge_Init_0_4_Mult_26_0_d,
                edge_Init_0_5_Mult_26_1_d, edge_Add_27_0_End_11_0_d);
            Mult_29<<<4096, 256, 0>>>(
                params_d, 0, current_limb, edge_Init_0_0_Mult_29_0_d,
                edge_Init_0_1_Mult_29_1_d, edge_Mult_29_0_End_11_3_d);

            Mult_1_iNTTPhase2_2<<<4096, 128,
                                  params_h->per_thread_ntt_size * 128 *
                                      sizeof(uint64_t)>>>(
                params_d, 0, current_limb, edge_Init_0_6_Mult_1_0_d,
                edge_Init_0_7_Mult_1_1_d, edge_Mult_1_0_End_11_4_d,
                edge_iNTTPhase2_2_0_iNTTPhase1_3_0_d);
            iNTTPhase1_3_MultConst_4<<<4096, (params_h->n1 / 8) * params_h->pad,
                                       (params_h->n1 + params_h->pad + 1) *
                                           params_h->pad * sizeof(uint64_t)>>>(
                params_d, 0, current_limb, edge_iNTTPhase2_2_0_iNTTPhase1_3_0_d,
                edge_MultConst_4_0_BConv_23_0_d,
                rns_tool->partQlHatInv_mod_Ql_concat(),
                rns_tool->partQlHatInv_mod_Ql_concat_shoup());

            const int limb_per = params_h->alpha;
            int n_divide_ = modup_limb / limb_per;
            for (int iter = 0; iter < n_divide_; iter++) {
                int start_li = iter * limb_per;
                int end_li = (iter + 1) * limb_per;
                BConv_general_part_allbeta<<<4096, 128>>>(
                    params_d, d_bconv_in_list, d_bconv_out_list,
                    d_qhat_modp_list, params_h->alpha, start_li, limb_per,
                    params_h->alpha, beta, params_h->ntt_tables->twiddle(),
                    params_h->ntt_tables->twiddle_shoup(),
                    params_h->ntt_tables->modulus());

                NTTP1_part_allbeta<<<4096, 128, 128 * 8 * sizeof(uint64_t)>>>(
                    params_d, start_li, end_li, 0, modup_limb,
                    params_h->ntt_tables->twiddle(),
                    params_h->ntt_tables->twiddle_shoup(),
                    params_h->ntt_tables->modulus(), d_in_list);

                NTTP2_MultKeyAccum_part<<<4096, 128,
                                          8 * 128 * sizeof(uint64_t)>>>(
                    params_d, start_li, end_li, 0, modup_limb, beta,
                    params_h->ntt_tables->twiddle(),
                    params_h->ntt_tables->twiddle_shoup(),
                    params_h->ntt_tables->modulus(), d_in_list,
                    edge_MultKeyAccum_8_0_iNTTPhase2_12_0_d,
                    edge_MultKeyAccum_8_1_iNTTPhase2_9_0_d, relin_keys);
            }

            iNTTPhase2_general<<<4096, 128,
                                 params_h->per_thread_ntt_size * 128 *
                                     sizeof(uint64_t)>>>(
                params_d, current_limb, modup_limb,
                edge_MultKeyAccum_8_1_iNTTPhase2_9_0_d,
                edge_iNTTPhase2_9_0_iNTTPhase1_10_0_d);
            iNTTPhase1_general<<<4096, (params_h->n1 / 8) * params_h->pad,
                                 (params_h->n1 + params_h->pad + 1) *
                                     params_h->pad * sizeof(uint64_t)>>>(
                params_d, current_limb, modup_limb,
                edge_iNTTPhase2_9_0_iNTTPhase1_10_0_d,
                edge_iNTTPhase1_10_0_End_11_2_d);
            iNTTPhase2_general<<<4096, 128,
                                 params_h->per_thread_ntt_size * 128 *
                                     sizeof(uint64_t)>>>(
                params_d, current_limb, modup_limb,
                edge_MultKeyAccum_8_0_iNTTPhase2_12_0_d,
                edge_iNTTPhase2_12_0_iNTTPhase1_13_0_d);
            iNTTPhase1_general<<<4096, (params_h->n1 / 8) * params_h->pad,
                                 (params_h->n1 + params_h->pad + 1) *
                                     params_h->pad * sizeof(uint64_t)>>>(
                params_d, current_limb, modup_limb,
                edge_iNTTPhase2_12_0_iNTTPhase1_13_0_d,
                edge_iNTTPhase1_13_0_End_11_1_d);
            checkCudaErrors(cudaDeviceSynchronize());

            // Timer Stop
            cudaEventRecord(ce_stop);
            cudaEventSynchronize(ce_stop);
            auto end = std::chrono::high_resolution_clock::now();

            float milliseconds = 0;
            auto elapsed_usec =
                std::chrono::duration_cast<std::chrono::microseconds>(end -
                                                                      start);
            cudaEventElapsedTime(&milliseconds, ce_start, ce_stop);
            cudaEventDestroy(ce_start);
            cudaEventDestroy(ce_stop);
            nvtxRangePop();

            if (i != 0) {
                elapsed_times.push_back(elapsed_usec.count());
                elapsed_times_ce.push_back(milliseconds);
            }
        }
        std::cout << "Average time (CudaEvent) [ms]: "
                  << std::accumulate(elapsed_times_ce.begin(),
                                     elapsed_times_ce.end(), 0.0) /
                         elapsed_times_ce.size()
                  << std::endl;
        std::cout << "Average time[us]: "
                  << std::accumulate(elapsed_times.begin(), elapsed_times.end(),
                                     0.0) /
                         elapsed_times.size()
                  << std::endl;
    }
}
