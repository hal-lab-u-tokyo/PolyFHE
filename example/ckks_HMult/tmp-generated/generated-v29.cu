// This file is generated by PolyFHE
#include <cuda.h>
#include <cuda_profiler_api.h>
#include <cuda_runtime.h>
#include <nvToolsExt.h>
#include <stdio.h>

#include <chrono>
#include <iostream>
#include <numeric>
#include <vector>

#include "phantom-fhe/include/phantom.h"
#include "phantom-fhe/include/uintmodmath.cuh"
#include "polyfhe/kernel/device_context.hpp"
#include "polyfhe/kernel/ntt-phantom.hpp"
#include "polyfhe/kernel/ntt.hpp"
#include "polyfhe/kernel/polynomial.cuh"
// Define kernel for subgraph[0], type: ElemLimb2
__global__ void Mult_1_iNTTPhase2_2(
    Params *params, int start_limb, int end_limb,
    uint64_t *edge_Init_0_6_Mult_1_0, uint64_t *edge_Init_0_7_Mult_1_1,
    uint64_t *edge_Mult_1_0_End_11_4,
    uint64_t *edge_iNTTPhase2_2_0_iNTTPhase1_3_0) {
    extern __shared__ uint64_t shared[];
    uint64_t reg[8];
    const size_t n_tower = params->N / 8;
    for (size_t tid = blockIdx.x * blockDim.x + threadIdx.x;
         tid < (end_limb - start_limb) * n_tower;
         tid += blockDim.x * gridDim.x) {
        // Load data to register
        const int twr_idx = tid / params->N + start_limb;
        uint64_t *in = edge_Init_0_6_Mult_1_0 + twr_idx * params->N;
#pragma unroll
        for (int l = 0; l < 8; l++) {
            reg[l] = *(in + blockIdx.x * blockDim.x * 8 + threadIdx.x * 8 + l);
        }
        __syncthreads();
        size_t batch_idx = tid / n_tower;

        // Mult_1
        const size_t idx = blockIdx.x * blockDim.x * 8 + threadIdx.x * 8;
#pragma unroll
        for (int l = 0; l < 8; l++) {
            uint64_t res;
            res = xxx_multiply_and_barrett_reduce_uint64(
                edge_Init_0_6_Mult_1_0[idx + l],
                edge_Init_0_7_Mult_1_1[idx + l], params->qVec[batch_idx],
                params->modulus_const_ratio + batch_idx * 2);
            reg[l] = res;
        }
#pragma unroll
        for (int l = 0; l < 4; l++) {
            asm("st.cs.global.v2.u64 [%0], {%1, %2};"
                :
                : "l"(edge_Mult_1_0_End_11_4 + idx + 2 * l), "l"(reg[2 * l]),
                  "l"(reg[2 * l + 1]));
        }

        // iNTTPhase2_2
        d_poly_inwt_radix8_phase2(params, 0, shared, reg, tid);

        // Store data from register
        const size_t n_group = params->n2 / 8;
        const size_t idx_base =
            start_limb * params->N +
            blockIdx.x * blockDim.x * params->per_thread_ntt_size +
            (threadIdx.x / n_group) * n_group * params->per_thread_ntt_size +
            (threadIdx.x % n_group);
        uint64_t *out = edge_iNTTPhase2_2_0_iNTTPhase1_3_0;
#pragma unroll
        for (int l = 0; l < 8; l++) {
            *(out + idx_base + n_group * l) = reg[l];
        }
        __syncthreads();
    }
}

// Define kernel for subgraph[1], type: ElemLimb1
__global__ void iNTTPhase1_3_MultConst_4(
    Params *params, int start_limb, int end_limb,
    uint64_t *edge_iNTTPhase2_2_0_iNTTPhase1_3_0,
    uint64_t *edge_MultConst_4_0_BConv_23_0,
    uint64_t *edge_MultConst_4_1_BConv_20_0,
    uint64_t *edge_MultConst_4_2_BConv_17_0,
    uint64_t *edge_MultConst_4_3_BConv_14_0,
    uint64_t *edge_MultConst_4_4_BConv_5_0,
    uint64_t *partQlHatInv_mod_Ql_concat,
    uint64_t *partQlHatInv_mod_Ql_concat_shoup) {
    extern __shared__ uint64_t shared[];
    uint64_t reg[8];
    uint64_t *in = edge_iNTTPhase2_2_0_iNTTPhase1_3_0;
    for (size_t i = blockIdx.x * blockDim.x + threadIdx.x;
         i < (params->N / 8 * (end_limb - start_limb));
         i += blockDim.x * gridDim.x) {
        const size_t n_twr = params->N / 8;
        const size_t n_idx = i % n_twr;
        const size_t twr_idx = i / n_twr + start_limb;
        const size_t group = params->n1 / 8;
        const size_t pad_tid = threadIdx.x % params->pad;
        const size_t pad_idx = threadIdx.x / params->pad;
        const size_t n_init = n_twr / group * pad_idx + pad_tid +
                              params->pad * (n_idx / (group * params->pad));
        uint64_t *out;
        // iNTTPhase1_3
        d_poly_inplace_inwt_radix8_phase1(in, params, start_limb, shared, reg,
                                          i);
// MultConst_4
#pragma unroll
        for (int l = 0; l < 8; l++) {
            reg[l] = xxx_multiply_and_reduce_shoup(
                reg[l], partQlHatInv_mod_Ql_concat[twr_idx],
                partQlHatInv_mod_Ql_concat_shoup[twr_idx],
                params->qVec[twr_idx]);
        }

        // Store data from register
        const size_t idx_out = twr_idx * params->N + n_init;
        out = edge_MultConst_4_0_BConv_23_0;
#pragma unroll
        for (int l = 0; l < 8; l++) {
            *(out + idx_out + n_twr * l) = reg[l];
        }
        __syncthreads();
    }
}

// Define kernel for subgraph[2], type: ElemSlot
__global__ void BConv_5(Params *params, uint64_t *edge_MultConst_4_4_BConv_5_0,
                        uint64_t *edge_BConv_5_0_NTTPhase1_6_0,
                        const uint64_t *qiHat_mod_pj, const DModulus *ibase,
                        uint64_t ibase_size, const DModulus *obase,
                        uint64_t obase_size, size_t startPartIdx,
                        size_t size_PartQl) {
    extern __shared__ uint64_t shared[];
    for (size_t i = threadIdx.x; i < obase_size * ibase_size; i += blockDim.x) {
        shared[i] = qiHat_mod_pj[i];
    }
    __syncthreads();
    uint64_t *out = edge_BConv_5_0_NTTPhase1_6_0;
    const int unroll_number = 2;
    for (size_t tid = blockIdx.x * blockDim.x + threadIdx.x;
         tid < (params->N * obase_size + unroll_number - 1) / unroll_number;
         tid += blockDim.x * gridDim.x) {
        const size_t n_idx = unroll_number * (tid / obase_size);
        const size_t l_idx = tid % obase_size;
        uint64_t res1, res2;
        // BConv_5
        BConvOpNoReg(params, &res1, &res2,
                     edge_MultConst_4_4_BConv_5_0 + params->N * startPartIdx,
                     shared, n_idx, l_idx, ibase, ibase_size, obase, obase_size,
                     startPartIdx, size_PartQl);
        const size_t l_out_idx =
            l_idx + ((l_idx >= startPartIdx) ? size_PartQl : 0);
        asm("st.cs.global.v2.u64 [%0], {%1, %2};" ::"l"(
                out + l_out_idx * params->N + n_idx),
            "l"(res1), "l"(res2));
    }
}

__global__ void BConv_general(Params *params,
                              uint64_t *edge_MultConst_4_4_BConv_5_0,
                              uint64_t *edge_BConv_5_0_NTTPhase1_6_0,
                              const uint64_t *qiHat_mod_pj, uint64_t ibase_size,
                              uint64_t obase_size, size_t startPartIdx,
                              size_t size_PartQl, const uint64_t *twiddles,
                              const uint64_t *twiddles_shoup,
                              const DModulus *modulus) {
    extern __shared__ uint64_t shared[];
    // for (size_t i = threadIdx.x; i < obase_size * ibase_size; i +=
    // blockDim.x) {
    //     shared[i] = qiHat_mod_pj[i];
    // }
    __syncthreads();
    uint64_t *out = edge_BConv_5_0_NTTPhase1_6_0;
    const int unroll_number = 2;
    for (size_t tid = blockIdx.x * blockDim.x + threadIdx.x;
         tid < (params->N * obase_size + unroll_number - 1) / unroll_number;
         tid += blockDim.x * gridDim.x) {
        const size_t n_idx = unroll_number * (tid / obase_size);
        const size_t l_idx = tid % obase_size;
        const size_t l_out_idx =
            l_idx + ((l_idx >= startPartIdx) ? size_PartQl : 0);
        uint64_t res1, res2;
        // BConv_5
        /*
        BConvOpNoReg_debug(
            params, &res1, &res2,
            edge_MultConst_4_4_BConv_5_0 + params->N * startPartIdx, shared,
            n_idx, l_idx, l_out_idx, ibase_size, startPartIdx, size_PartQl,
            twiddles, twiddles_shoup, modulus);
        */
        BConvOpNoReg_debug(
            params, &res1, &res2,
            edge_MultConst_4_4_BConv_5_0 + params->N * startPartIdx,
            qiHat_mod_pj, n_idx, l_idx, l_out_idx, ibase_size, startPartIdx,
            size_PartQl, twiddles, twiddles_shoup, modulus);
        asm("st.cs.global.v2.u64 [%0], {%1, %2};" ::"l"(
                out + l_out_idx * params->N + n_idx),
            "l"(res1), "l"(res2));
    }
}

__global__ void BConv_general_part(Params *params, uint64_t *in, uint64_t *out,
                                   const uint64_t *qiHat_mod_pj,
                                   uint64_t ibase_size, uint64_t obase_start,
                                   uint64_t obase_start_in30,
                                   uint64_t obase_size, size_t startPartIdx,
                                   size_t size_PartQl, const uint64_t *twiddles,
                                   const uint64_t *twiddles_shoup,
                                   const DModulus *modulus) {
    // extern __shared__ uint64_t shared[];
    // for (size_t i = threadIdx.x; i < obase_size * ibase_size; i +=
    // blockDim.x) {
    //     shared[i] = qiHat_mod_pj[i + obase_start_in30 * ibase_size];
    // }
    // __syncthreads();
    for (size_t tid = blockIdx.x * blockDim.x + threadIdx.x;
         tid < (params->N * obase_size + 1) / 2;
         tid += blockDim.x * gridDim.x) {
        const size_t n_idx = 2 * (tid / obase_size);
        const size_t o_idx = tid % obase_size;
        // const size_t o_idx = tid / (params->N / 2);
        // const size_t n_idx = 2 * (tid % (params->N / 2));
        const size_t l_out_idx = o_idx + obase_start;
        // BConv_5
        BConvOpNoReg_debug2(params, out, in + params->N * startPartIdx,
                            qiHat_mod_pj + obase_start_in30 * ibase_size, n_idx,
                            o_idx, l_out_idx, ibase_size, twiddles,
                            twiddles_shoup, modulus);
    }
}

// Define kernel for subgraph[3], type: ElemLimb1
__global__ void NTTPhase1_6(Params *params, int start_limb, int end_limb,
                            uint64_t *edge_BConv_5_0_NTTPhase1_6_0,
                            uint64_t *edge_NTTPhase1_6_0_NTTPhase2_7_0,
                            const uint64_t *twiddles,
                            const uint64_t *twiddles_shoup,
                            const DModulus *modulus) {
    extern __shared__ uint64_t shared[];
    uint64_t reg[8];
    uint64_t *in = edge_BConv_5_0_NTTPhase1_6_0;
    for (size_t i = blockIdx.x * blockDim.x + threadIdx.x;
         i < (params->N / 8 * (end_limb - start_limb));
         i += blockDim.x * gridDim.x) {
        const size_t n_twr = params->N / 8;
        const size_t n_idx = i % n_twr;
        const size_t twr_idx = i / n_twr + start_limb;
        const size_t group = params->n1 / 8;
        const size_t pad_tid = threadIdx.x % params->pad;
        const size_t pad_idx = threadIdx.x / params->pad;
        const size_t n_init = n_twr / group * pad_idx + pad_tid +
                              params->pad * (n_idx / (group * params->pad));
        uint64_t *out;
        // NTTPhase1_6
        const size_t exclude_end = 30;
        const size_t exclude_start = 24;
        if (twr_idx >= exclude_start && twr_idx < exclude_end) {
            continue;
        }

// Load to register
#pragma unroll
        for (int l = 0; l < 8; l++) {
            reg[l] = *(in + twr_idx * params->N + n_init + n_twr * l);
        }
        const uint64_t size_P = params->K;
        const uint64_t size_QP = params->KL;
        out = edge_NTTPhase1_6_0_NTTPhase2_7_0;
        size_t twr_idx2 = (twr_idx >= start_limb + end_limb - size_P
                               ? size_QP - (start_limb + end_limb - twr_idx)
                               : twr_idx);
        d_poly_fnwt_phase1(params, out, shared, reg, twiddles, twiddles_shoup,
                           modulus, twr_idx, twr_idx2, n_init, i);
    }
}

// Define kernel for subgraph[4], type: ElemLimb2
__global__ void NTTPhase2_7(Params *params, int start_limb, int end_limb,
                            uint64_t *edge_NTTPhase1_6_0_NTTPhase2_7_0,
                            uint64_t *edge_NTTPhase2_7_0_MultKeyAccum_8_4,
                            const uint64_t *twiddles,
                            const uint64_t *twiddles_shoup,
                            const DModulus *modulus) {
    extern __shared__ uint64_t shared[];
    uint64_t reg[8];
    const size_t n_tower = params->N / 8;
    for (size_t tid = blockIdx.x * blockDim.x + threadIdx.x;
         tid < (end_limb - start_limb) * n_tower;
         tid += blockDim.x * gridDim.x) {
        // NTTPhase2_7
        size_t twr_idx = end_limb - 1 - (tid / n_tower) + start_limb;
        const size_t exclude_end = 30;
        const size_t exclude_start = 24;
        if (twr_idx >= exclude_start && twr_idx < exclude_end) {
            continue;
        }
        uint64_t n_init;
        d_poly_fnwt_phase2(params, edge_NTTPhase1_6_0_NTTPhase2_7_0, shared,
                           reg, twiddles, twiddles_shoup, modulus, end_limb,
                           start_limb, twr_idx, &n_init, tid);
        uint64_t *out_ptr =
            edge_NTTPhase2_7_0_MultKeyAccum_8_4 + twr_idx * params->N;
#pragma unroll
        for (size_t j = 0; j < 8; j++) {
            *(out_ptr + n_init + params->n2 / 8 * j) = reg[j];
        }
        __syncthreads();
    }
}

// Define kernel for subgraph[5], type: ElemSlot
__global__ void BConv_14(Params *params,
                         uint64_t *edge_MultConst_4_3_BConv_14_0,
                         uint64_t *edge_BConv_14_0_NTTPhase1_15_0,
                         const uint64_t *qiHat_mod_pj, const DModulus *ibase,
                         uint64_t ibase_size, const DModulus *obase,
                         uint64_t obase_size, size_t startPartIdx,
                         size_t size_PartQl) {
    extern __shared__ uint64_t shared[];
    for (size_t i = threadIdx.x; i < obase_size * ibase_size; i += blockDim.x) {
        shared[i] = qiHat_mod_pj[i];
    }
    __syncthreads();
    uint64_t *out = edge_BConv_14_0_NTTPhase1_15_0;
    const int unroll_number = 2;
    for (size_t tid = blockIdx.x * blockDim.x + threadIdx.x;
         tid < (params->N * obase_size + unroll_number - 1) / unroll_number;
         tid += blockDim.x * gridDim.x) {
        const size_t n_idx = unroll_number * (tid / obase_size);
        const size_t l_idx = tid % obase_size;
        uint64_t res1, res2;
        // BConv_14
        BConvOpNoReg(params, &res1, &res2,
                     edge_MultConst_4_3_BConv_14_0 + params->N * startPartIdx,
                     shared, n_idx, l_idx, ibase, ibase_size, obase, obase_size,
                     startPartIdx, size_PartQl);
        const size_t l_out_idx =
            l_idx + ((l_idx >= startPartIdx) ? size_PartQl : 0);
        asm("st.cs.global.v2.u64 [%0], {%1, %2};" ::"l"(
                out + l_out_idx * params->N + n_idx),
            "l"(res1), "l"(res2));
    }
}

// Define kernel for subgraph[6], type: ElemLimb1
__global__ void NTTPhase1_15(Params *params, int start_limb, int end_limb,
                             uint64_t *edge_BConv_14_0_NTTPhase1_15_0,
                             uint64_t *edge_NTTPhase1_15_0_NTTPhase2_16_0,
                             const uint64_t *twiddles,
                             const uint64_t *twiddles_shoup,
                             const DModulus *modulus) {
    extern __shared__ uint64_t shared[];
    uint64_t reg[8];
    uint64_t *in = edge_BConv_14_0_NTTPhase1_15_0;
    for (size_t i = blockIdx.x * blockDim.x + threadIdx.x;
         i < (params->N / 8 * (end_limb - start_limb));
         i += blockDim.x * gridDim.x) {
        const size_t n_twr = params->N / 8;
        const size_t n_idx = i % n_twr;
        const size_t twr_idx = i / n_twr + start_limb;
        const size_t group = params->n1 / 8;
        const size_t pad_tid = threadIdx.x % params->pad;
        const size_t pad_idx = threadIdx.x / params->pad;
        const size_t n_init = n_twr / group * pad_idx + pad_tid +
                              params->pad * (n_idx / (group * params->pad));
        uint64_t *out;
        // NTTPhase1_15
        const size_t exclude_end = 24;
        const size_t exclude_start = 18;
        if (twr_idx >= exclude_start && twr_idx < exclude_end) {
            continue;
        }

// Load to register
#pragma unroll
        for (int l = 0; l < 8; l++) {
            reg[l] = *(in + twr_idx * params->N + n_init + n_twr * l);
        }
        const uint64_t size_P = params->K;
        const uint64_t size_QP = params->KL;
        out = edge_NTTPhase1_15_0_NTTPhase2_16_0;
        size_t twr_idx2 = (twr_idx >= start_limb + end_limb - size_P
                               ? size_QP - (start_limb + end_limb - twr_idx)
                               : twr_idx);
        d_poly_fnwt_phase1(params, out, shared, reg, twiddles, twiddles_shoup,
                           modulus, twr_idx, twr_idx2, n_init, i);
    }
}

// Define kernel for subgraph[7], type: ElemLimb2
__global__ void NTTPhase2_16(Params *params, int start_limb, int end_limb,
                             uint64_t *edge_NTTPhase1_15_0_NTTPhase2_16_0,
                             uint64_t *edge_NTTPhase2_16_0_MultKeyAccum_8_3,
                             const uint64_t *twiddles,
                             const uint64_t *twiddles_shoup,
                             const DModulus *modulus) {
    extern __shared__ uint64_t shared[];
    uint64_t reg[8];
    const size_t n_tower = params->N / 8;
    for (size_t tid = blockIdx.x * blockDim.x + threadIdx.x;
         tid < (end_limb - start_limb) * n_tower;
         tid += blockDim.x * gridDim.x) {
        // NTTPhase2_16
        size_t twr_idx = end_limb - 1 - (tid / n_tower) + start_limb;
        const size_t exclude_end = 24;
        const size_t exclude_start = 18;
        if (twr_idx >= exclude_start && twr_idx < exclude_end) {
            continue;
        }
        uint64_t n_init;
        d_poly_fnwt_phase2(params, edge_NTTPhase1_15_0_NTTPhase2_16_0, shared,
                           reg, twiddles, twiddles_shoup, modulus, end_limb,
                           start_limb, twr_idx, &n_init, tid);
        uint64_t *out_ptr =
            edge_NTTPhase2_16_0_MultKeyAccum_8_3 + twr_idx * params->N;
#pragma unroll
        for (size_t j = 0; j < 8; j++) {
            *(out_ptr + n_init + params->n2 / 8 * j) = reg[j];
        }
        __syncthreads();
    }
}

// Define kernel for subgraph[8], type: ElemSlot
__global__ void BConv_17(Params *params,
                         uint64_t *edge_MultConst_4_2_BConv_17_0,
                         uint64_t *edge_BConv_17_0_NTTPhase1_18_0,
                         const uint64_t *qiHat_mod_pj, const DModulus *ibase,
                         uint64_t ibase_size, const DModulus *obase,
                         uint64_t obase_size, size_t startPartIdx,
                         size_t size_PartQl) {
    extern __shared__ uint64_t shared[];
    for (size_t i = threadIdx.x; i < obase_size * ibase_size; i += blockDim.x) {
        shared[i] = qiHat_mod_pj[i];
    }
    __syncthreads();
    uint64_t *out = edge_BConv_17_0_NTTPhase1_18_0;
    const int unroll_number = 2;
    for (size_t tid = blockIdx.x * blockDim.x + threadIdx.x;
         tid < (params->N * obase_size + unroll_number - 1) / unroll_number;
         tid += blockDim.x * gridDim.x) {
        const size_t n_idx = unroll_number * (tid / obase_size);
        const size_t l_idx = tid % obase_size;
        uint64_t res1, res2;
        // BConv_17
        BConvOpNoReg(params, &res1, &res2,
                     edge_MultConst_4_2_BConv_17_0 + params->N * startPartIdx,
                     shared, n_idx, l_idx, ibase, ibase_size, obase, obase_size,
                     startPartIdx, size_PartQl);
        const size_t l_out_idx =
            l_idx + ((l_idx >= startPartIdx) ? size_PartQl : 0);
        asm("st.cs.global.v2.u64 [%0], {%1, %2};" ::"l"(
                out + l_out_idx * params->N + n_idx),
            "l"(res1), "l"(res2));
    }
}

// Define kernel for subgraph[9], type: ElemLimb1
__global__ void NTTPhase1_18(Params *params, int start_limb, int end_limb,
                             uint64_t *edge_BConv_17_0_NTTPhase1_18_0,
                             uint64_t *edge_NTTPhase1_18_0_NTTPhase2_19_0,
                             const uint64_t *twiddles,
                             const uint64_t *twiddles_shoup,
                             const DModulus *modulus) {
    extern __shared__ uint64_t shared[];
    uint64_t reg[8];
    uint64_t *in = edge_BConv_17_0_NTTPhase1_18_0;
    for (size_t i = blockIdx.x * blockDim.x + threadIdx.x;
         i < (params->N / 8 * (end_limb - start_limb));
         i += blockDim.x * gridDim.x) {
        const size_t n_twr = params->N / 8;
        const size_t n_idx = i % n_twr;
        const size_t twr_idx = i / n_twr + start_limb;
        const size_t group = params->n1 / 8;
        const size_t pad_tid = threadIdx.x % params->pad;
        const size_t pad_idx = threadIdx.x / params->pad;
        const size_t n_init = n_twr / group * pad_idx + pad_tid +
                              params->pad * (n_idx / (group * params->pad));
        uint64_t *out;
        // NTTPhase1_18
        const size_t exclude_end = 18;
        const size_t exclude_start = 12;
        if (twr_idx >= exclude_start && twr_idx < exclude_end) {
            continue;
        }

// Load to register
#pragma unroll
        for (int l = 0; l < 8; l++) {
            reg[l] = *(in + twr_idx * params->N + n_init + n_twr * l);
        }
        const uint64_t size_P = params->K;
        const uint64_t size_QP = params->KL;
        out = edge_NTTPhase1_18_0_NTTPhase2_19_0;
        size_t twr_idx2 = (twr_idx >= start_limb + end_limb - size_P
                               ? size_QP - (start_limb + end_limb - twr_idx)
                               : twr_idx);
        d_poly_fnwt_phase1(params, out, shared, reg, twiddles, twiddles_shoup,
                           modulus, twr_idx, twr_idx2, n_init, i);
    }
}

// Define kernel for subgraph[10], type: ElemLimb2
__global__ void NTTPhase2_19(Params *params, int start_limb, int end_limb,
                             uint64_t *edge_NTTPhase1_18_0_NTTPhase2_19_0,
                             uint64_t *edge_NTTPhase2_19_0_MultKeyAccum_8_2,
                             const uint64_t *twiddles,
                             const uint64_t *twiddles_shoup,
                             const DModulus *modulus) {
    extern __shared__ uint64_t shared[];
    uint64_t reg[8];
    const size_t n_tower = params->N / 8;
    for (size_t tid = blockIdx.x * blockDim.x + threadIdx.x;
         tid < (end_limb - start_limb) * n_tower;
         tid += blockDim.x * gridDim.x) {
        // NTTPhase2_19
        size_t twr_idx = end_limb - 1 - (tid / n_tower) + start_limb;
        const size_t exclude_end = 18;
        const size_t exclude_start = 12;
        if (twr_idx >= exclude_start && twr_idx < exclude_end) {
            continue;
        }
        uint64_t n_init;
        d_poly_fnwt_phase2(params, edge_NTTPhase1_18_0_NTTPhase2_19_0, shared,
                           reg, twiddles, twiddles_shoup, modulus, end_limb,
                           start_limb, twr_idx, &n_init, tid);
        uint64_t *out_ptr =
            edge_NTTPhase2_19_0_MultKeyAccum_8_2 + twr_idx * params->N;
#pragma unroll
        for (size_t j = 0; j < 8; j++) {
            *(out_ptr + n_init + params->n2 / 8 * j) = reg[j];
        }
        __syncthreads();
    }
}

// Define kernel for subgraph[11], type: ElemSlot
__global__ void BConv_20(Params *params,
                         uint64_t *edge_MultConst_4_1_BConv_20_0,
                         uint64_t *edge_BConv_20_0_NTTPhase1_21_0,
                         const uint64_t *qiHat_mod_pj, const DModulus *ibase,
                         uint64_t ibase_size, const DModulus *obase,
                         uint64_t obase_size, size_t startPartIdx,
                         size_t size_PartQl) {
    extern __shared__ uint64_t shared[];
    for (size_t i = threadIdx.x; i < obase_size * ibase_size; i += blockDim.x) {
        shared[i] = qiHat_mod_pj[i];
    }
    __syncthreads();
    uint64_t *out = edge_BConv_20_0_NTTPhase1_21_0;
    const int unroll_number = 2;
    for (size_t tid = blockIdx.x * blockDim.x + threadIdx.x;
         tid < (params->N * obase_size + unroll_number - 1) / unroll_number;
         tid += blockDim.x * gridDim.x) {
        const size_t n_idx = unroll_number * (tid / obase_size);
        const size_t l_idx = tid % obase_size;
        uint64_t res1, res2;
        // BConv_20
        BConvOpNoReg(params, &res1, &res2,
                     edge_MultConst_4_1_BConv_20_0 + params->N * startPartIdx,
                     shared, n_idx, l_idx, ibase, ibase_size, obase, obase_size,
                     startPartIdx, size_PartQl);
        const size_t l_out_idx =
            l_idx + ((l_idx >= startPartIdx) ? size_PartQl : 0);
        asm("st.cs.global.v2.u64 [%0], {%1, %2};" ::"l"(
                out + l_out_idx * params->N + n_idx),
            "l"(res1), "l"(res2));
    }
}

// Define kernel for subgraph[12], type: ElemLimb1
__global__ void NTTPhase1_21(Params *params, int start_limb, int end_limb,
                             uint64_t *edge_BConv_20_0_NTTPhase1_21_0,
                             uint64_t *edge_NTTPhase1_21_0_NTTPhase2_22_0,
                             const uint64_t *twiddles,
                             const uint64_t *twiddles_shoup,
                             const DModulus *modulus) {
    extern __shared__ uint64_t shared[];
    uint64_t reg[8];
    uint64_t *in = edge_BConv_20_0_NTTPhase1_21_0;
    for (size_t i = blockIdx.x * blockDim.x + threadIdx.x;
         i < (params->N / 8 * (end_limb - start_limb));
         i += blockDim.x * gridDim.x) {
        const size_t n_twr = params->N / 8;
        const size_t n_idx = i % n_twr;
        const size_t twr_idx = i / n_twr + start_limb;
        const size_t group = params->n1 / 8;
        const size_t pad_tid = threadIdx.x % params->pad;
        const size_t pad_idx = threadIdx.x / params->pad;
        const size_t n_init = n_twr / group * pad_idx + pad_tid +
                              params->pad * (n_idx / (group * params->pad));
        uint64_t *out;
        // NTTPhase1_21
        const size_t exclude_end = 12;
        const size_t exclude_start = 6;
        if (twr_idx >= exclude_start && twr_idx < exclude_end) {
            continue;
        }

// Load to register
#pragma unroll
        for (int l = 0; l < 8; l++) {
            reg[l] = *(in + twr_idx * params->N + n_init + n_twr * l);
        }
        const uint64_t size_P = params->K;
        const uint64_t size_QP = params->KL;
        out = edge_NTTPhase1_21_0_NTTPhase2_22_0;
        size_t twr_idx2 = (twr_idx >= start_limb + end_limb - size_P
                               ? size_QP - (start_limb + end_limb - twr_idx)
                               : twr_idx);
        d_poly_fnwt_phase1(params, out, shared, reg, twiddles, twiddles_shoup,
                           modulus, twr_idx, twr_idx2, n_init, i);
    }
}

// Define kernel for subgraph[13], type: ElemLimb2
__global__ void NTTPhase2_22(Params *params, int start_limb, int end_limb,
                             uint64_t *edge_NTTPhase1_21_0_NTTPhase2_22_0,
                             uint64_t *edge_NTTPhase2_22_0_MultKeyAccum_8_1,
                             const uint64_t *twiddles,
                             const uint64_t *twiddles_shoup,
                             const DModulus *modulus) {
    extern __shared__ uint64_t shared[];
    uint64_t reg[8];
    const size_t n_tower = params->N / 8;
    for (size_t tid = blockIdx.x * blockDim.x + threadIdx.x;
         tid < (end_limb - start_limb) * n_tower;
         tid += blockDim.x * gridDim.x) {
        // NTTPhase2_22
        size_t twr_idx = tid / n_tower + start_limb;
        const size_t exclude_end = 12;
        const size_t exclude_start = 6;
        if (twr_idx >= exclude_start && twr_idx < exclude_end) {
            continue;
        }
        uint64_t n_init;
        d_poly_fnwt_phase2(params, edge_NTTPhase1_21_0_NTTPhase2_22_0, shared,
                           reg, twiddles, twiddles_shoup, modulus, end_limb,
                           start_limb, twr_idx, &n_init, tid);
        uint64_t *out_ptr =
            edge_NTTPhase2_22_0_MultKeyAccum_8_1 + twr_idx * params->N;
#pragma unroll
        for (size_t j = 0; j < 8; j++) {
            *(out_ptr + n_init + params->n2 / 8 * j) = reg[j];
        }
        __syncthreads();
    }
}

// Define kernel for subgraph[14], type: ElemSlot
__global__ void BConv_23(Params *params,
                         uint64_t *edge_MultConst_4_0_BConv_23_0,
                         uint64_t *edge_BConv_23_0_NTTPhase1_24_0,
                         const uint64_t *qiHat_mod_pj, const DModulus *ibase,
                         uint64_t ibase_size, const DModulus *obase,
                         uint64_t obase_size, size_t startPartIdx,
                         size_t size_PartQl) {
    extern __shared__ uint64_t shared[];
    for (size_t i = threadIdx.x; i < obase_size * ibase_size; i += blockDim.x) {
        shared[i] = qiHat_mod_pj[i];
    }
    __syncthreads();
    uint64_t *out = edge_BConv_23_0_NTTPhase1_24_0;
    const int unroll_number = 2;
    for (size_t tid = blockIdx.x * blockDim.x + threadIdx.x;
         tid < (params->N * obase_size + unroll_number - 1) / unroll_number;
         tid += blockDim.x * gridDim.x) {
        const size_t n_idx = unroll_number * (tid / obase_size);
        const size_t l_idx = tid % obase_size;
        uint64_t res1, res2;
        // BConv_23
        BConvOpNoReg(params, &res1, &res2,
                     edge_MultConst_4_0_BConv_23_0 + params->N * startPartIdx,
                     shared, n_idx, l_idx, ibase, ibase_size, obase, obase_size,
                     startPartIdx, size_PartQl);
        const size_t l_out_idx =
            l_idx + ((l_idx >= startPartIdx) ? size_PartQl : 0);
        asm("st.cs.global.v2.u64 [%0], {%1, %2};" ::"l"(
                out + l_out_idx * params->N + n_idx),
            "l"(res1), "l"(res2));
    }
}

// Define kernel for subgraph[15], type: ElemLimb1
__global__ void NTTPhase1_24(Params *params, int start_limb, int end_limb,
                             uint64_t *edge_BConv_23_0_NTTPhase1_24_0,
                             uint64_t *edge_NTTPhase1_24_0_NTTPhase2_25_0,
                             const uint64_t *twiddles,
                             const uint64_t *twiddles_shoup,
                             const DModulus *modulus) {
    extern __shared__ uint64_t shared[];
    uint64_t reg[8];
    uint64_t *in = edge_BConv_23_0_NTTPhase1_24_0;
    for (size_t i = blockIdx.x * blockDim.x + threadIdx.x;
         i < (params->N / 8 * (end_limb - start_limb));
         i += blockDim.x * gridDim.x) {
        const size_t n_twr = params->N / 8;
        const size_t n_idx = i % n_twr;
        const size_t twr_idx = i / n_twr + start_limb;
        const size_t group = params->n1 / 8;
        const size_t pad_tid = threadIdx.x % params->pad;
        const size_t pad_idx = threadIdx.x / params->pad;
        const size_t n_init = n_twr / group * pad_idx + pad_tid +
                              params->pad * (n_idx / (group * params->pad));
        uint64_t *out;
        // NTTPhase1_24
        const size_t exclude_end = 6;
        if (twr_idx < exclude_end) {
            continue;
        }

// Load to register
#pragma unroll
        for (int l = 0; l < 8; l++) {
            reg[l] = *(in + twr_idx * params->N + n_init + n_twr * l);
        }
        const uint64_t size_P = params->K;
        const uint64_t size_QP = params->KL;
        out = edge_NTTPhase1_24_0_NTTPhase2_25_0;
        size_t twr_idx2 = (twr_idx >= start_limb + end_limb - size_P
                               ? size_QP - (start_limb + end_limb - twr_idx)
                               : twr_idx);
        d_poly_fnwt_phase1(params, out, shared, reg, twiddles, twiddles_shoup,
                           modulus, twr_idx, twr_idx2, n_init, i);
    }
}

// Define kernel for subgraph[16], type: ElemLimb2
__global__ void NTTPhase2_25(Params *params, int start_limb, int end_limb,
                             int start_limb_original, int end_limb_original,
                             uint64_t *edge_NTTPhase1_24_0_NTTPhase2_25_0,
                             uint64_t *edge_NTTPhase2_25_0_MultKeyAccum_8_0,
                             const uint64_t *twiddles,
                             const uint64_t *twiddles_shoup,
                             const DModulus *modulus) {
    extern __shared__ uint64_t shared[];
    uint64_t reg[8];
    const size_t n_tower = params->N / 8;
    for (size_t tid = blockIdx.x * blockDim.x + threadIdx.x;
         tid < (end_limb - start_limb) * n_tower;
         tid += blockDim.x * gridDim.x) {
        // NTTPhase2_25
        size_t twr_idx = tid / n_tower + start_limb;
        size_t twr_idx2 =
            (twr_idx >= start_limb_original + end_limb_original - params->K
                 ? params->KL -
                       (start_limb_original + end_limb_original - twr_idx)
                 : twr_idx);
        const size_t exclude_end = 6;
        if (twr_idx < exclude_end) {
            continue;
        }
        uint64_t n_init;
        d_poly_fnwt_phase2_debug(params, edge_NTTPhase1_24_0_NTTPhase2_25_0,
                                 shared, reg, twiddles, twiddles_shoup, modulus,
                                 end_limb, start_limb, twr_idx, twr_idx2,
                                 &n_init, tid);
        uint64_t *out_ptr =
            edge_NTTPhase2_25_0_MultKeyAccum_8_0 + twr_idx * params->N;
#pragma unroll
        for (size_t j = 0; j < 8; j++) {
            *(out_ptr + n_init + params->n2 / 8 * j) = reg[j];
        }
        __syncthreads();
    }
}

__global__ void NTTPhase1_general(
    Params *params, int start_limb, int end_limb, int start_limb_original,
    int end_limb_original, int exclude_start, int exclude_end,
    uint64_t *edge_BConv_23_0_NTTPhase1_24_0,
    uint64_t *edge_NTTPhase1_24_0_NTTPhase2_25_0, const uint64_t *twiddles,
    const uint64_t *twiddles_shoup, const DModulus *modulus) {
    extern __shared__ uint64_t shared[];
    uint64_t reg[8];
    uint64_t *in = edge_BConv_23_0_NTTPhase1_24_0;
    for (size_t i = blockIdx.x * blockDim.x + threadIdx.x;
         i < (params->N / 8 * (end_limb - start_limb));
         i += blockDim.x * gridDim.x) {
        const size_t n_twr = params->N / 8;
        const size_t n_idx = i % n_twr;
        const size_t twr_idx = i / n_twr + start_limb;
        const size_t group = params->n1 / 8;
        const size_t pad_tid = threadIdx.x % params->pad;
        const size_t pad_idx = threadIdx.x / params->pad;
        const size_t n_init = n_twr / group * pad_idx + pad_tid +
                              params->pad * (n_idx / (group * params->pad));
        uint64_t *out;
        // NTTPhase1_24
        if (twr_idx < exclude_end && twr_idx >= exclude_start) {
            continue;
        }

// Load to register
#pragma unroll
        for (int l = 0; l < 8; l++) {
            reg[l] = *(in + twr_idx * params->N + n_init + n_twr * l);
        }
        const uint64_t size_P = params->K;
        const uint64_t size_QP = params->KL;
        out = edge_NTTPhase1_24_0_NTTPhase2_25_0;
        size_t twr_idx2 =
            (twr_idx >= start_limb_original + end_limb_original - size_P
                 ? size_QP - (start_limb_original + end_limb_original - twr_idx)
                 : twr_idx);
        d_poly_fnwt_phase1(params, out, shared, reg, twiddles, twiddles_shoup,
                           modulus, twr_idx, twr_idx2, n_init, i);
    }
}

__global__ void NTTPhase1_general_per(Params *params, int start_limb,
                                      int end_limb, int start_limb_original,
                                      int end_limb_original, uint64_t *in,
                                      uint64_t *out, const uint64_t *twiddles,
                                      const uint64_t *twiddles_shoup,
                                      const DModulus *modulus) {
    extern __shared__ uint64_t shared[];
    uint64_t reg[8];
    for (size_t i = blockIdx.x * blockDim.x + threadIdx.x;
         i < (params->N / 8 * (end_limb - start_limb));
         i += blockDim.x * gridDim.x) {
        const size_t n_twr = params->N / 8;
        const size_t n_idx = i % n_twr;
        const size_t twr_idx = i / n_twr + start_limb;
        const size_t group = params->n1 / 8;
        const size_t pad_tid = threadIdx.x % params->pad;
        const size_t pad_idx = threadIdx.x / params->pad;
        const size_t n_init = n_twr / group * pad_idx + pad_tid +
                              params->pad * (n_idx / (group * params->pad));
        // NTTPhase1_24
// Load to register
#pragma unroll
        for (int l = 0; l < 8; l++) {
            reg[l] = *(in + twr_idx * params->N + n_init + n_twr * l);
        }
        size_t twr_idx2 =
            (twr_idx >= start_limb_original + end_limb_original - params->K
                 ? params->KL -
                       (start_limb_original + end_limb_original - twr_idx)
                 : twr_idx);
        d_poly_fnwt_phase1(params, out, shared, reg, twiddles, twiddles_shoup,
                           modulus, twr_idx, twr_idx2, n_init, i);
    }
}

__global__ void NTTPhase2_general(
    Params *params, int start_limb, int end_limb, int start_limb_original,
    int end_limb_original, int exclude_start, int exclude_end,
    uint64_t *edge_NTTPhase1_24_0_NTTPhase2_25_0,
    uint64_t *edge_NTTPhase2_25_0_MultKeyAccum_8_0, const uint64_t *twiddles,
    const uint64_t *twiddles_shoup, const DModulus *modulus) {
    extern __shared__ uint64_t shared[];
    uint64_t reg[8];
    const size_t n_tower = params->N / 8;
    for (size_t tid = blockIdx.x * blockDim.x + threadIdx.x;
         tid < (params->N / 8 * (end_limb - start_limb));
         tid += blockDim.x * gridDim.x) {
        // NTTPhase2_25
        const uint64_t size_P = params->K;
        const uint64_t size_QP = params->KL;

        const size_t twr_idx = tid / n_tower + start_limb;
        size_t twr_idx2 =
            (twr_idx >= start_limb_original + end_limb_original - size_P
                 ? size_QP - (start_limb_original + end_limb_original - twr_idx)
                 : twr_idx);

        if (twr_idx < exclude_end && twr_idx >= exclude_start) {
            continue;
        }
        uint64_t n_init;
        d_poly_fnwt_phase2_debug(params, edge_NTTPhase1_24_0_NTTPhase2_25_0,
                                 shared, reg, twiddles, twiddles_shoup, modulus,
                                 end_limb, start_limb, twr_idx, twr_idx2,
                                 &n_init, tid);
        uint64_t *out_ptr =
            edge_NTTPhase2_25_0_MultKeyAccum_8_0 + twr_idx * params->N;
#pragma unroll
        for (size_t j = 0; j < 8; j++) {
            *(out_ptr + n_init + params->n2 / 8 * j) = reg[j];
        }
        __syncthreads();
    }
}

__global__ void NTTPhase2_general_part(Params *params, int start_limb,
                                       int end_limb, int start_limb_original,
                                       int end_limb_original, uint64_t *in,
                                       uint64_t *out, const uint64_t *twiddles,
                                       const uint64_t *twiddles_shoup,
                                       const DModulus *modulus) {
    extern __shared__ uint64_t shared[];
    uint64_t reg[8];
    const size_t n_tower = params->N / 8;
    for (size_t tid = blockIdx.x * blockDim.x + threadIdx.x;
         tid < (params->N / 8 * (end_limb - start_limb));
         tid += blockDim.x * gridDim.x) {
        // NTTPhase2_25
        const uint64_t size_P = params->K;
        const uint64_t size_QP = params->KL;

        const size_t twr_idx = tid / n_tower + start_limb;
        size_t twr_idx2 =
            (twr_idx >= start_limb_original + end_limb_original - size_P
                 ? size_QP - (start_limb_original + end_limb_original - twr_idx)
                 : twr_idx);

        uint64_t n_init;
        d_poly_fnwt_phase2_debug(params, in, shared, reg, twiddles,
                                 twiddles_shoup, modulus, end_limb, start_limb,
                                 twr_idx, twr_idx2, &n_init, tid);
        uint64_t *out_ptr = out + twr_idx * params->N;
#pragma unroll
        for (size_t j = 0; j < 8; j++) {
            *(out_ptr + n_init + params->n2 / 8 * j) = reg[j];
        }
        __syncthreads();
    }
}

__global__ void NTTPhase12(Params *params, int _start_limb, int _end_limb,
                           int start_limb_original, int end_limb_original,
                           int exclude_start, int exclude_end, uint64_t *in,
                           uint64_t *p1_p2, uint64_t *out,
                           const uint64_t *twiddles,
                           const uint64_t *twiddles_shoup,
                           const DModulus *modulus, int n_divide) {
    extern __shared__ uint64_t shared[];
    uint64_t reg[8];
    const size_t n_tower = params->N / 8;
    int li_per_divide = 36 / n_divide;
    for (int iter = 0; iter < n_divide; iter++) {
        int start_limb = iter * li_per_divide;
        int end_limb = (iter + 1) * li_per_divide;

        // NTTPhase1
        for (size_t tid = blockIdx.x * blockDim.x + threadIdx.x;
             tid < (params->N / 8 * (end_limb - start_limb));
             tid += blockDim.x * gridDim.x) {
            const uint64_t size_P = params->K;
            const uint64_t size_QP = params->KL;

            const size_t twr_idx = tid / n_tower + start_limb;
            if (twr_idx < exclude_end && twr_idx >= exclude_start) {
                continue;
            }

            size_t twr_idx2 =
                (twr_idx >= start_limb_original + end_limb_original - size_P
                     ? size_QP -
                           (start_limb_original + end_limb_original - twr_idx)
                     : twr_idx);
            const size_t group = params->n1 / 8;
            const size_t pad_tid = threadIdx.x % params->pad;
            const size_t pad_idx = threadIdx.x / params->pad;
            const size_t n_idx = tid % n_tower;
            size_t n_init = n_tower / group * pad_idx + pad_tid +
                            params->pad * (n_idx / (group * params->pad));
#pragma unroll
            for (int l = 0; l < 8; l++) {
                reg[l] = *(in + twr_idx * params->N + n_init + n_tower * l);
            }
            d_poly_fnwt_phase1(params, p1_p2, shared, reg, twiddles,
                               twiddles_shoup, modulus, twr_idx, twr_idx2,
                               n_init, tid);
        }
        /*

        // NTTPhase2
        for (size_t tid = blockIdx.x * blockDim.x + threadIdx.x;
             tid < (params->N / 8 * (end_limb - start_limb));
             tid += blockDim.x * gridDim.x) {
            const size_t twr_idx = tid / n_tower + start_limb;
            if (twr_idx < exclude_end && twr_idx >= exclude_start) {
                continue;
            }

            size_t twr_idx2 =
                (twr_idx >= start_limb_original + end_limb_original - params->K
                     ? params->KL -
                           (start_limb_original + end_limb_original - twr_idx)
                     : twr_idx);
            // NTTPhase2
            uint64_t n_init;
            d_poly_fnwt_phase2_debug(
                params, p1_p2, shared, reg, twiddles, twiddles_shoup, modulus,
                end_limb, start_limb, twr_idx, twr_idx2, &n_init, tid);
            uint64_t *out_ptr = out + twr_idx * params->N;
#pragma unroll
            for (size_t j = 0; j < 8; j++) {
                *(out_ptr + n_init + params->n2 / 8 * j) = reg[j];
            }
            __syncthreads();
        }
         */
    }
}

__global__ void NTTPhase11(Params *params, int start_limb, int end_limb,
                           int start_limb_original, int end_limb_original,
                           int exclude_start, int exclude_end,
                           uint64_t *a_p1_in, uint64_t *a_p1_out,
                           uint64_t *a_p2_out, int b_exclude_start,
                           int b_exclude_end, uint64_t *b_p1_in,
                           uint64_t *b_p1_out, const uint64_t *twiddles,
                           const uint64_t *twiddles_shoup,
                           const DModulus *modulus) {
    extern __shared__ uint64_t shared[];
    uint64_t reg[8];
    const uint64_t size_P = params->K;
    const uint64_t size_QP = params->KL;
    const size_t n_twr = params->N / 8;
    const size_t group = params->n1 / 8;

    for (size_t i = blockIdx.x * blockDim.x + threadIdx.x;
         i < (params->N / 8 * (end_limb - start_limb));
         i += blockDim.x * gridDim.x) {
        const size_t n_idx = i % n_twr;
        const size_t twr_idx = i / n_twr + start_limb;
        const size_t pad_tid = threadIdx.x % params->pad;
        const size_t pad_idx = threadIdx.x / params->pad;
        size_t n_init = n_twr / group * pad_idx + pad_tid +
                        params->pad * (n_idx / (group * params->pad));

        uint64_t *out;
        size_t twr_idx2 =
            (twr_idx >= start_limb_original + end_limb_original - size_P
                 ? size_QP - (start_limb_original + end_limb_original - twr_idx)
                 : twr_idx);

        // NTTPhase1 of a
        if (twr_idx >= exclude_end || twr_idx < exclude_start) {
#pragma unroll
            for (int l = 0; l < 8; l++) {
                reg[l] = *(a_p1_in + twr_idx * params->N + n_init + n_twr * l);
            }
            out = a_p1_out;
            d_poly_fnwt_phase1(params, out, shared, reg, twiddles,
                               twiddles_shoup, modulus, twr_idx, twr_idx2,
                               n_init, i);
        }
        // NTTPhase1 of b
        if (twr_idx >= b_exclude_end || twr_idx < b_exclude_start) {
#pragma unroll
            for (int l = 0; l < 8; l++) {
                reg[l] = *(b_p1_in + twr_idx * params->N + n_init + n_twr * l);
            }
            out = b_p1_out;
            d_poly_fnwt_phase1(params, out, shared, reg, twiddles,
                               twiddles_shoup, modulus, twr_idx, twr_idx2,
                               n_init, i);
            __syncthreads();
        }
    }
}

__global__ void NTTPhase22(Params *params, int start_limb, int end_limb,
                           int start_limb_original, int end_limb_original,
                           int exclude_start, int exclude_end, uint64_t *a_in,
                           uint64_t *a_out, int b_exclude_start,
                           int b_exclude_end, uint64_t *b_in, uint64_t *b_out,
                           const uint64_t *twiddles,
                           const uint64_t *twiddles_shoup,
                           const DModulus *modulus) {
    extern __shared__ uint64_t shared[];
    uint64_t reg[8];
    const size_t n_tower = params->N / 8;
    for (size_t tid = blockIdx.x * blockDim.x + threadIdx.x;
         tid < (params->N / 8 * (end_limb - start_limb));
         tid += blockDim.x * gridDim.x) {
        const uint64_t size_P = params->K;
        const uint64_t size_QP = params->KL;

        const size_t twr_idx = tid / n_tower + start_limb;
        size_t twr_idx2 =
            (twr_idx >= start_limb_original + end_limb_original - size_P
                 ? size_QP - (start_limb_original + end_limb_original - twr_idx)
                 : twr_idx);

        // NTTPhase2 of a
        if (twr_idx >= exclude_end || twr_idx < exclude_start) {
            uint64_t n_init;
            d_poly_fnwt_phase2_debug(
                params, a_in, shared, reg, twiddles, twiddles_shoup, modulus,
                end_limb, start_limb, twr_idx, twr_idx2, &n_init, tid);
            uint64_t *out_ptr = a_out + twr_idx * params->N;
#pragma unroll
            for (size_t j = 0; j < 8; j++) {
                *(out_ptr + n_init + params->n2 / 8 * j) = reg[j];
            }
            __syncthreads();
        }

        // NTTPhase2 of b
        if (twr_idx >= b_exclude_end || twr_idx < b_exclude_start) {
            uint64_t n_init;
            d_poly_fnwt_phase2_debug(
                params, b_in, shared, reg, twiddles, twiddles_shoup, modulus,
                end_limb, start_limb, twr_idx, twr_idx2, &n_init, tid);
            uint64_t *out_ptr = b_out + twr_idx * params->N;
#pragma unroll
            for (size_t j = 0; j < 8; j++) {
                *(out_ptr + n_init + params->n2 / 8 * j) = reg[j];
            }
            __syncthreads();
        }
    }
}

// Define kernel for subgraph[17], type: Elem
__global__ void NTTP2_MultKeyAccum(
    Params *params, int start_limb, int end_limb, int start_limb_original,
    int end_limb_original, uint64_t *edge_NTTPhase1_24_0_NTTPhase2_25_0,
    const uint64_t *twiddles, const uint64_t *twiddles_shoup,
    const DModulus *modulus, uint64_t **in_list,
    uint64_t *edge_MultKeyAccum_8_0_iNTTPhase2_12_0,
    uint64_t *edge_MultKeyAccum_8_1_iNTTPhase2_9_0, uint64_t **relin_keys) {
    extern __shared__ uint64_t shared[];
    uint64_t reg[8];

    for (size_t tid = blockIdx.x * blockDim.x + threadIdx.x;
         tid < (params->N / 8 * (end_limb - start_limb));
         tid += blockDim.x * gridDim.x) {
        const size_t l_idx = tid / (params->N / 8) + start_limb;
        size_t n_idx = tid % (params->N / 8);
        size_t m_idx = n_idx / (params->n2 / 8);
        size_t t_idx = n_idx % (params->n2 / 8);
        const size_t n_init = m_idx * params->n2 + t_idx;
        // NTTPhase2
        for (int beta_idx = 0; beta_idx < 5; beta_idx++) {
            size_t twr_idx2 =
                (l_idx >= start_limb_original + end_limb_original - params->K
                     ? params->KL -
                           (start_limb_original + end_limb_original - l_idx)
                     : l_idx);
            if (l_idx >= (beta_idx + 1) * 6 || l_idx < beta_idx * 6) {
                d_poly_fnwt_phase2_debug2(params, in_list[beta_idx], shared,
                                          reg, twiddles, twiddles_shoup,
                                          modulus, end_limb, start_limb, l_idx,
                                          twr_idx2, n_init, tid);
                uint64_t *out_ptr =
                    in_list[beta_idx] + l_idx * params->N + n_init;

#pragma unroll
                for (size_t j = 0; j < 8; j++) {
                    *(out_ptr + params->n2 / 8 * j) = reg[j];
                }
            }
        }
        __syncthreads();
        {
            size_t idx = l_idx * params->N + n_init;
#pragma unroll
            for (size_t j = 0; j < 8; j++) {
                MulKeyAccumOp_opt(params, edge_MultKeyAccum_8_0_iNTTPhase2_12_0,
                                  edge_MultKeyAccum_8_1_iNTTPhase2_9_0, in_list,
                                  relin_keys, 5, idx, l_idx, shared, reg, j);
                idx += params->n2 / 8;
            }
        }
    }
}

/*
__global__ void NTTP2_MultKeyAccum(
    Params *params, int start_limb, int end_limb, int start_limb_original,
    int end_limb_original, uint64_t *edge_NTTPhase1_24_0_NTTPhase2_25_0,
    const uint64_t *twiddles, const uint64_t *twiddles_shoup,
    const DModulus *modulus, uint64_t **in_list,
    uint64_t *edge_MultKeyAccum_8_0_iNTTPhase2_12_0,
    uint64_t *edge_MultKeyAccum_8_1_iNTTPhase2_9_0, uint64_t **relin_keys) {
    extern __shared__ uint64_t shared[];
    uint64_t reg[8];

    for (size_t tid = blockIdx.x * blockDim.x + threadIdx.x;
         tid < (params->N / 8 * (end_limb - start_limb));
         tid += blockDim.x * gridDim.x) {
        const size_t l_idx = tid / (params->N / 8) + start_limb;
        size_t n_idx = tid % (params->N / 8);
        size_t m_idx = n_idx / (params->n2 / 8);
        size_t t_idx = n_idx % (params->n2 / 8);
        const size_t n_init = m_idx * params->n2 + t_idx;
        // NTTPhase2
        for (int beta_idx = 0; beta_idx < 5; beta_idx++) {
            size_t twr_idx2 =
                (l_idx >= start_limb_original + end_limb_original - params->K
                     ? params->KL -
                           (start_limb_original + end_limb_original - l_idx)
                     : l_idx);
            if (l_idx >= (beta_idx + 1) * 6 || l_idx < beta_idx * 6) {
                d_poly_fnwt_phase2_debug2(params, in_list[beta_idx], shared,
                                          reg, twiddles, twiddles_shoup,
                                          modulus, end_limb, start_limb, l_idx,
                                          twr_idx2, n_init, tid);
                uint64_t *out_ptr =
                    in_list[beta_idx] + l_idx * params->N + n_init;

#pragma unroll
                for (size_t j = 0; j < 8; j++) {
                    *(out_ptr + params->n2 / 8 * j) = reg[j];
                }
            }
        }
    }
    __syncthreads();
    for (size_t tid = blockIdx.x * blockDim.x + threadIdx.x;
         tid < (params->N * (end_limb - start_limb));
         tid += blockDim.x * gridDim.x) {
        const size_t l_idx = tid / params->N + start_limb;
        MulKeyAccumOp_opt2(params, edge_MultKeyAccum_8_0_iNTTPhase2_12_0,
                           edge_MultKeyAccum_8_1_iNTTPhase2_9_0, in_list,
                           relin_keys, 5, tid, l_idx);
    }
}
*/

__global__ void NTTP1_part_allbeta(Params *params, int start_limb, int end_limb,
                                   int start_limb_original,
                                   int end_limb_original,
                                   const uint64_t *twiddles,
                                   const uint64_t *twiddles_shoup,
                                   const DModulus *modulus,
                                   uint64_t **in_list) {
    extern __shared__ uint64_t shared[];
    uint64_t reg[8];
    for (size_t i = blockIdx.x * blockDim.x + threadIdx.x;
         i < (params->N / 8 * (end_limb - start_limb));
         i += blockDim.x * gridDim.x) {
        const size_t n_twr = params->N / 8;
        const size_t n_idx = i % n_twr;
        const size_t twr_idx = i / n_twr + start_limb;
        const size_t group = params->n1 / 8;
        const size_t pad_tid = threadIdx.x % params->pad;
        const size_t pad_idx = threadIdx.x / params->pad;
        const size_t n_init = n_twr / group * pad_idx + pad_tid +
                              params->pad * (n_idx / (group * params->pad));
        size_t twr_idx2 =
            (twr_idx >= start_limb_original + end_limb_original - params->K
                 ? params->KL -
                       (start_limb_original + end_limb_original - twr_idx)
                 : twr_idx);

        for (int beta_idx = 0; beta_idx < 5; beta_idx++) {
            if (twr_idx >= (beta_idx + 1) * 6 || twr_idx < beta_idx * 6) {
#pragma unroll
                for (int l = 0; l < 8; l++) {
                    reg[l] = *(in_list[beta_idx] + twr_idx * params->N +
                               n_init + n_twr * l);
                }
                d_poly_fnwt_phase1(params, in_list[beta_idx], shared, reg,
                                   twiddles, twiddles_shoup, modulus, twr_idx,
                                   twr_idx2, n_init, i);
            }
        }
    }
}

__global__ void NTTP2_part_allbeta(Params *params, int start_limb, int end_limb,
                                   int start_limb_original,
                                   int end_limb_original,
                                   const uint64_t *twiddles,
                                   const uint64_t *twiddles_shoup,
                                   const DModulus *modulus,
                                   uint64_t **in_list) {
    extern __shared__ uint64_t shared[];
    uint64_t reg[8];

    for (size_t tid = blockIdx.x * blockDim.x + threadIdx.x;
         tid < (params->N / 8 * (end_limb - start_limb));
         tid += blockDim.x * gridDim.x) {
        const size_t l_idx = tid / (params->N / 8) + start_limb;
        size_t n_idx = tid % (params->N / 8);
        size_t m_idx = n_idx / (params->n2 / 8);
        size_t t_idx = n_idx % (params->n2 / 8);
        const size_t n_init = m_idx * params->n2 + t_idx;
        // NTTPhase2
        for (int beta_idx = 0; beta_idx < 5; beta_idx++) {
            size_t twr_idx2 =
                (l_idx >= start_limb_original + end_limb_original - params->K
                     ? params->KL -
                           (start_limb_original + end_limb_original - l_idx)
                     : l_idx);
            if (l_idx >= (beta_idx + 1) * 6 || l_idx < beta_idx * 6) {
                d_poly_fnwt_phase2_debug2(params, in_list[beta_idx], shared,
                                          reg, twiddles, twiddles_shoup,
                                          modulus, end_limb, start_limb, l_idx,
                                          twr_idx2, n_init, tid);
                uint64_t *out_ptr =
                    in_list[beta_idx] + l_idx * params->N + n_init;

#pragma unroll
                for (size_t j = 0; j < 8; j++) {
                    *(out_ptr + params->n2 / 8 * j) = reg[j];
                }
            }
        }
    }
}

__global__ void MultKeyAccum_part(
    Params *params, int start_limb, int end_limb, uint64_t **in_list,
    uint64_t *edge_MultKeyAccum_8_0_iNTTPhase2_12_0,
    uint64_t *edge_MultKeyAccum_8_1_iNTTPhase2_9_0, uint64_t **relin_keys) {
    for (size_t tid = blockIdx.x * blockDim.x + threadIdx.x;
         tid < (params->N * (end_limb - start_limb));
         tid += blockDim.x * gridDim.x) {
        const size_t l_idx = tid / params->N + start_limb;
        MulKeyAccumOp_opt2(params, edge_MultKeyAccum_8_0_iNTTPhase2_12_0,
                           edge_MultKeyAccum_8_1_iNTTPhase2_9_0, in_list,
                           relin_keys, 5, tid + params->N * start_limb, l_idx);
    }
}

__global__ void MultKeyAccum_8(Params *params, int start_limb, int end_limb,
                               uint64_t *edge_NTTPhase2_25_0_MultKeyAccum_8_0,
                               uint64_t *edge_NTTPhase2_22_0_MultKeyAccum_8_1,
                               uint64_t *edge_NTTPhase2_19_0_MultKeyAccum_8_2,
                               uint64_t *edge_NTTPhase2_16_0_MultKeyAccum_8_3,
                               uint64_t *edge_NTTPhase2_7_0_MultKeyAccum_8_4,
                               uint64_t *edge_MultKeyAccum_8_0_iNTTPhase2_12_0,
                               uint64_t *edge_MultKeyAccum_8_1_iNTTPhase2_9_0,
                               uint64_t **relin_keys) {
    for (int idx = threadIdx.x + blockIdx.x * blockDim.x;
         idx < params->N * (end_limb - start_limb);
         idx += blockDim.x * gridDim.x) {
        const int l_idx = idx / params->N + start_limb;
        // MultKeyAccum_8
        uint64_t *in_list[5] = {edge_NTTPhase2_25_0_MultKeyAccum_8_0,
                                edge_NTTPhase2_22_0_MultKeyAccum_8_1,
                                edge_NTTPhase2_19_0_MultKeyAccum_8_2,
                                edge_NTTPhase2_16_0_MultKeyAccum_8_3,
                                edge_NTTPhase2_7_0_MultKeyAccum_8_4};
        MulKeyAccumOp(params, edge_MultKeyAccum_8_0_iNTTPhase2_12_0,
                      edge_MultKeyAccum_8_1_iNTTPhase2_9_0, in_list, relin_keys,
                      5, idx, l_idx);
    }
}

// Define kernel for subgraph[18], type: ElemLimb2
__global__ void iNTTPhase2_9(Params *params, int start_limb, int end_limb,
                             uint64_t *edge_MultKeyAccum_8_1_iNTTPhase2_9_0,
                             uint64_t *edge_iNTTPhase2_9_0_iNTTPhase1_10_0) {
    extern __shared__ uint64_t shared[];
    uint64_t reg[8];
    const size_t n_tower = params->N / 8;
    for (size_t tid = blockIdx.x * blockDim.x + threadIdx.x;
         tid < (end_limb - start_limb) * n_tower;
         tid += blockDim.x * gridDim.x) {
        // Load data to register
        const int twr_idx = tid / params->N + start_limb;
        uint64_t *in =
            edge_MultKeyAccum_8_1_iNTTPhase2_9_0 + twr_idx * params->N;
#pragma unroll
        for (int l = 0; l < 8; l++) {
            reg[l] = *(in + blockIdx.x * blockDim.x * 8 + threadIdx.x * 8 + l);
        }
        __syncthreads();

        // iNTTPhase2_9
        d_poly_inwt_radix8_phase2(params, 30, shared, reg, tid);

        // Store data from register
        const size_t n_group = params->n2 / 8;
        const size_t idx_base =
            start_limb * params->N +
            blockIdx.x * blockDim.x * params->per_thread_ntt_size +
            (threadIdx.x / n_group) * n_group * params->per_thread_ntt_size +
            (threadIdx.x % n_group);
        uint64_t *out = edge_iNTTPhase2_9_0_iNTTPhase1_10_0;
#pragma unroll
        for (int l = 0; l < 8; l++) {
            *(out + idx_base + n_group * l) = reg[l];
        }
        __syncthreads();
    }
}

// Define kernel for subgraph[19], type: ElemLimb1
__global__ void iNTTPhase1_10(Params *params, int start_limb, int end_limb,
                              uint64_t *edge_iNTTPhase2_9_0_iNTTPhase1_10_0,
                              uint64_t *edge_iNTTPhase1_10_0_End_11_2) {
    extern __shared__ uint64_t shared[];
    uint64_t reg[8];
    uint64_t *in = edge_iNTTPhase2_9_0_iNTTPhase1_10_0;
    for (size_t i = blockIdx.x * blockDim.x + threadIdx.x;
         i < (params->N / 8 * (end_limb - start_limb));
         i += blockDim.x * gridDim.x) {
        const size_t n_twr = params->N / 8;
        const size_t n_idx = i % n_twr;
        const size_t twr_idx = i / n_twr + start_limb;
        const size_t group = params->n1 / 8;
        const size_t pad_tid = threadIdx.x % params->pad;
        const size_t pad_idx = threadIdx.x / params->pad;
        const size_t n_init = n_twr / group * pad_idx + pad_tid +
                              params->pad * (n_idx / (group * params->pad));
        uint64_t *out;
        // iNTTPhase1_10
        d_poly_inplace_inwt_radix8_phase1(in, params, start_limb, shared, reg,
                                          i);

        // Store data from register
        const size_t idx_out = twr_idx * params->N + n_init;
        out = edge_iNTTPhase1_10_0_End_11_2;
#pragma unroll
        for (int l = 0; l < 8; l++) {
            *(out + idx_out + n_twr * l) = reg[l];
        }
        __syncthreads();
    }
}

// Define kernel for subgraph[20], type: ElemLimb2
__global__ void iNTTPhase2_12(Params *params, int start_limb, int end_limb,
                              uint64_t *edge_MultKeyAccum_8_0_iNTTPhase2_12_0,
                              uint64_t *edge_iNTTPhase2_12_0_iNTTPhase1_13_0) {
    extern __shared__ uint64_t shared[];
    uint64_t reg[8];
    const size_t n_tower = params->N / 8;
    for (size_t tid = blockIdx.x * blockDim.x + threadIdx.x;
         tid < (end_limb - start_limb) * n_tower;
         tid += blockDim.x * gridDim.x) {
        // Load data to register
        const int twr_idx = tid / params->N + start_limb;
        uint64_t *in =
            edge_MultKeyAccum_8_0_iNTTPhase2_12_0 + twr_idx * params->N;
#pragma unroll
        for (int l = 0; l < 8; l++) {
            reg[l] = *(in + blockIdx.x * blockDim.x * 8 + threadIdx.x * 8 + l);
        }
        __syncthreads();

        // iNTTPhase2_12
        d_poly_inwt_radix8_phase2(params, 30, shared, reg, tid);

        // Store data from register
        const size_t n_group = params->n2 / 8;
        const size_t idx_base =
            start_limb * params->N +
            blockIdx.x * blockDim.x * params->per_thread_ntt_size +
            (threadIdx.x / n_group) * n_group * params->per_thread_ntt_size +
            (threadIdx.x % n_group);
        uint64_t *out = edge_iNTTPhase2_12_0_iNTTPhase1_13_0;
#pragma unroll
        for (int l = 0; l < 8; l++) {
            *(out + idx_base + n_group * l) = reg[l];
        }
        __syncthreads();
    }
}

// Define kernel for subgraph[21], type: ElemLimb1
__global__ void iNTTPhase1_13(Params *params, int start_limb, int end_limb,
                              uint64_t *edge_iNTTPhase2_12_0_iNTTPhase1_13_0,
                              uint64_t *edge_iNTTPhase1_13_0_End_11_1) {
    extern __shared__ uint64_t shared[];
    uint64_t reg[8];
    uint64_t *in = edge_iNTTPhase2_12_0_iNTTPhase1_13_0;
    for (size_t i = blockIdx.x * blockDim.x + threadIdx.x;
         i < (params->N / 8 * (end_limb - start_limb));
         i += blockDim.x * gridDim.x) {
        const size_t n_twr = params->N / 8;
        const size_t n_idx = i % n_twr;
        const size_t twr_idx = i / n_twr + start_limb;
        const size_t group = params->n1 / 8;
        const size_t pad_tid = threadIdx.x % params->pad;
        const size_t pad_idx = threadIdx.x / params->pad;
        const size_t n_init = n_twr / group * pad_idx + pad_tid +
                              params->pad * (n_idx / (group * params->pad));
        uint64_t *out;
        // iNTTPhase1_13
        d_poly_inplace_inwt_radix8_phase1(in, params, start_limb, shared, reg,
                                          i);

        // Store data from register
        const size_t idx_out = twr_idx * params->N + n_init;
        out = edge_iNTTPhase1_13_0_End_11_1;
#pragma unroll
        for (int l = 0; l < 8; l++) {
            *(out + idx_out + n_twr * l) = reg[l];
        }
        __syncthreads();
    }
}

// Define kernel for subgraph[22], type: Elem
__global__ void Mult_28_Mult_26_Add_27(Params *params, int start_limb,
                                       int end_limb,
                                       uint64_t *edge_Init_0_2_Mult_28_0,
                                       uint64_t *edge_Init_0_3_Mult_28_1,
                                       uint64_t *edge_Init_0_4_Mult_26_0,
                                       uint64_t *edge_Init_0_5_Mult_26_1,
                                       uint64_t *edge_Add_27_0_End_11_0) {
    for (int idx = threadIdx.x + blockIdx.x * blockDim.x;
         idx < params->N * (end_limb - start_limb);
         idx += blockDim.x * gridDim.x) {
        const int l_idx = idx / params->N + start_limb;
        const uint64_t q = params->qVec[l_idx];
        uint64_t res;
        const int n_idx = l_idx * params->N + idx % params->N;
        // Mult_28
        uint64_t res1 = xxx_multiply_and_barrett_reduce_uint64(
            edge_Init_0_2_Mult_28_0[n_idx], edge_Init_0_3_Mult_28_1[n_idx], q,
            params->modulus_const_ratio + l_idx * 2);
        // Mult_26
        uint64_t res2 = xxx_multiply_and_barrett_reduce_uint64(
            edge_Init_0_4_Mult_26_0[n_idx], edge_Init_0_5_Mult_26_1[n_idx], q,
            params->modulus_const_ratio + l_idx * 2);
        // Add_27
        res = res1 + res2;
        if (res >= q)
            res -= q;
        edge_Add_27_0_End_11_0[n_idx] = res;
    }
}

// Define kernel for subgraph[23], type: Elem
__global__ void Mult_29(Params *params, int start_limb, int end_limb,
                        uint64_t *edge_Init_0_0_Mult_29_0,
                        uint64_t *edge_Init_0_1_Mult_29_1,
                        uint64_t *edge_Mult_29_0_End_11_3) {
    for (int idx = threadIdx.x + blockIdx.x * blockDim.x;
         idx < params->N * (end_limb - start_limb);
         idx += blockDim.x * gridDim.x) {
        const int l_idx = idx / params->N + start_limb;
        const uint64_t q = params->qVec[l_idx];
        uint64_t res;
        const int n_idx = l_idx * params->N + idx % params->N;
        // Mult_29
        res = xxx_multiply_and_barrett_reduce_uint64(
            edge_Init_0_0_Mult_29_0[n_idx], edge_Init_0_1_Mult_29_1[n_idx], q,
            params->modulus_const_ratio + l_idx * 2);
        edge_Mult_29_0_End_11_3[n_idx] = res;
    }
}

void entry_kernel(Params *params_d, Params *params_h, PhantomContext &context,
                  uint64_t **relin_keys, uint64_t *in0, uint64_t *in1,
                  uint64_t *out0, uint64_t *out1, bool if_benchmark,
                  int n_divide) {
    phantom::DRNSTool *rns_tool = params_h->rns_tools[1];

    // =====================================
    // Input arguments
    // =====================================
    // Edge: Init_0 -> Mult_29
    uint64_t *edge_Init_0_0_Mult_29_0_d = in0 + 0;
    // Edge: Init_0 -> Mult_29
    uint64_t *edge_Init_0_1_Mult_29_1_d = in1 + 0;
    // Edge: Init_0 -> Mult_28
    uint64_t *edge_Init_0_2_Mult_28_0_d = in0 + 0;
    // Edge: Init_0 -> Mult_28
    uint64_t *edge_Init_0_3_Mult_28_1_d = in1 + 1966080;
    // Edge: Init_0 -> Mult_26
    uint64_t *edge_Init_0_4_Mult_26_0_d = in0 + 1966080;
    // Edge: Init_0 -> Mult_26
    uint64_t *edge_Init_0_5_Mult_26_1_d = in1 + 0;
    // Edge: Init_0 -> Mult_1
    uint64_t *edge_Init_0_6_Mult_1_0_d = in0 + 1966080;
    // Edge: Init_0 -> Mult_1
    uint64_t *edge_Init_0_7_Mult_1_1_d = in1 + 1966080;

    // =====================================
    // Output arguments
    // =====================================
    // Edge: Add_27 -> End_11
    uint64_t *edge_Add_27_0_End_11_0_d = out0 + 1966080;
    // Edge: iNTTPhase1_13 -> End_11
    uint64_t *edge_iNTTPhase1_13_0_End_11_1_d = out1 + 0;
    // Edge: iNTTPhase1_10 -> End_11
    uint64_t *edge_iNTTPhase1_10_0_End_11_2_d = out1 + 2359296;
    // Edge: Mult_29 -> End_11
    uint64_t *edge_Mult_29_0_End_11_3_d = out0 + 0;
    // Edge: Mult_1 -> End_11
    uint64_t *edge_Mult_1_0_End_11_4_d = out0 + 3932160;

    // =====================================
    // Edges
    // Define global edges for GPU
    // =====================================
    // Edge: iNTTPhase2_2 -> iNTTPhase1_3
    uint64_t *edge_iNTTPhase2_2_0_iNTTPhase1_3_0_d;
    checkCudaErrors(cudaMalloc((void **) &edge_iNTTPhase2_2_0_iNTTPhase1_3_0_d,
                               30 * params_h->N * sizeof(uint64_t)));
    // Edge: MultConst_4 -> BConv_23
    uint64_t *edge_MultConst_4_0_BConv_23_0_d;
    checkCudaErrors(cudaMalloc((void **) &edge_MultConst_4_0_BConv_23_0_d,
                               36 * params_h->N * sizeof(uint64_t)));
    // Edge: NTTPhase2_7 -> MultKeyAccum_8
    uint64_t *edge_NTTPhase2_7_0_MultKeyAccum_8_4_d;
    checkCudaErrors(cudaMalloc((void **) &edge_NTTPhase2_7_0_MultKeyAccum_8_4_d,
                               36 * params_h->N * sizeof(uint64_t)));
    // Edge: NTTPhase2_16 -> MultKeyAccum_8
    uint64_t *edge_NTTPhase2_16_0_MultKeyAccum_8_3_d;
    checkCudaErrors(
        cudaMalloc((void **) &edge_NTTPhase2_16_0_MultKeyAccum_8_3_d,
                   36 * params_h->N * sizeof(uint64_t)));
    // Edge: NTTPhase2_19 -> MultKeyAccum_8
    uint64_t *edge_NTTPhase2_19_0_MultKeyAccum_8_2_d;
    checkCudaErrors(
        cudaMalloc((void **) &edge_NTTPhase2_19_0_MultKeyAccum_8_2_d,
                   36 * params_h->N * sizeof(uint64_t)));
    // Edge: NTTPhase2_22 -> MultKeyAccum_8
    uint64_t *edge_NTTPhase2_22_0_MultKeyAccum_8_1_d;
    checkCudaErrors(
        cudaMalloc((void **) &edge_NTTPhase2_22_0_MultKeyAccum_8_1_d,
                   36 * params_h->N * sizeof(uint64_t)));
    // uint64_t *edge_NTTPhase2_22_0_MultKeyAccum_8_1_d =
    // edge_iNTTPhase1_10_0_End_11_2_d;

    // Edge:NTTPhase2_25 -> MultKeyAccum_8
    uint64_t *edge_NTTPhase2_25_0_MultKeyAccum_8_0_d;
    checkCudaErrors(
        cudaMalloc((void **) &edge_NTTPhase2_25_0_MultKeyAccum_8_0_d,
                   36 * params_h->N * sizeof(uint64_t)));
    // uint64_t *edge_NTTPhase2_25_0_MultKeyAccum_8_0_d =
    // edge_iNTTPhase1_13_0_End_11_1_d;
    uint64_t *edge_MultConst_4_1_BConv_20_0_d = edge_MultConst_4_0_BConv_23_0_d;
    uint64_t *edge_MultConst_4_2_BConv_17_0_d = edge_MultConst_4_0_BConv_23_0_d;
    uint64_t *edge_MultConst_4_3_BConv_14_0_d = edge_MultConst_4_0_BConv_23_0_d;
    uint64_t *edge_MultConst_4_4_BConv_5_0_d = edge_MultConst_4_0_BConv_23_0_d;
    uint64_t *edge_BConv_5_0_NTTPhase1_6_0_d =
        edge_NTTPhase2_7_0_MultKeyAccum_8_4_d;
    uint64_t *edge_NTTPhase1_6_0_NTTPhase2_7_0_d =
        edge_NTTPhase2_7_0_MultKeyAccum_8_4_d;
    uint64_t *edge_BConv_14_0_NTTPhase1_15_0_d =
        edge_NTTPhase2_16_0_MultKeyAccum_8_3_d;
    uint64_t *edge_NTTPhase1_15_0_NTTPhase2_16_0_d =
        edge_NTTPhase2_16_0_MultKeyAccum_8_3_d;
    uint64_t *edge_BConv_17_0_NTTPhase1_18_0_d =
        edge_NTTPhase2_19_0_MultKeyAccum_8_2_d;
    uint64_t *edge_NTTPhase1_18_0_NTTPhase2_19_0_d =
        edge_NTTPhase2_19_0_MultKeyAccum_8_2_d;
    uint64_t *edge_BConv_20_0_NTTPhase1_21_0_d =
        edge_NTTPhase2_22_0_MultKeyAccum_8_1_d;
    uint64_t *edge_NTTPhase1_21_0_NTTPhase2_22_0_d =
        edge_NTTPhase2_22_0_MultKeyAccum_8_1_d;
    uint64_t *edge_BConv_23_0_NTTPhase1_24_0_d =
        edge_NTTPhase2_25_0_MultKeyAccum_8_0_d;
    uint64_t *edge_NTTPhase1_24_0_NTTPhase2_25_0_d =
        edge_NTTPhase2_25_0_MultKeyAccum_8_0_d;
    uint64_t *edge_MultKeyAccum_8_0_iNTTPhase2_12_0_d =
        edge_iNTTPhase1_13_0_End_11_1_d;
    uint64_t *edge_MultKeyAccum_8_1_iNTTPhase2_9_0_d =
        edge_iNTTPhase1_10_0_End_11_2_d;
    uint64_t *edge_iNTTPhase2_9_0_iNTTPhase1_10_0_d =
        edge_iNTTPhase1_10_0_End_11_2_d;
    uint64_t *edge_iNTTPhase2_12_0_iNTTPhase1_13_0_d =
        edge_iNTTPhase1_13_0_End_11_1_d;
    // =====================================
    std::cout << "### Warm up and Test" << std::endl;
    std::cout << "N : " << params_h->N << std::endl;
    std::cout << "L : " << params_h->L << std::endl;
    std::cout << "dnum : " << params_h->dnum << std::endl;
    std::cout << "alpha : " << params_h->alpha << std::endl;

    // =====================================
    // Warm up
    // =====================================
    {
        // Call kernel
        // Timer start
        auto start = std::chrono::high_resolution_clock::now();
        phantom::DRNSTool *drns_tool = params_h->rns_tools[1];
        const int beta = std::ceil((params_h->L + 1) / params_h->alpha);
        Mult_1_iNTTPhase2_2<<<4096, 128,
                              params_h->per_thread_ntt_size * 128 *
                                  sizeof(uint64_t)>>>(
            params_d, 0, 30, edge_Init_0_6_Mult_1_0_d, edge_Init_0_7_Mult_1_1_d,
            edge_Mult_1_0_End_11_4_d, edge_iNTTPhase2_2_0_iNTTPhase1_3_0_d);
        checkCudaErrors(cudaDeviceSynchronize());
        iNTTPhase1_3_MultConst_4<<<4096, (params_h->n1 / 8) * params_h->pad,
                                   (params_h->n1 + params_h->pad + 1) *
                                       params_h->pad * sizeof(uint64_t)>>>(
            params_d, 0, 30, edge_iNTTPhase2_2_0_iNTTPhase1_3_0_d,
            edge_MultConst_4_0_BConv_23_0_d, edge_MultConst_4_1_BConv_20_0_d,
            edge_MultConst_4_2_BConv_17_0_d, edge_MultConst_4_3_BConv_14_0_d,
            edge_MultConst_4_4_BConv_5_0_d,
            rns_tool->partQlHatInv_mod_Ql_concat(),
            rns_tool->partQlHatInv_mod_Ql_concat_shoup());
        {
            const size_t beta_idx = 4;
            const size_t startPartIdx = params_h->alpha * beta_idx;
            const size_t size_PartQl =
                (beta_idx == beta - 1)
                    ? (params_h->L - params_h->alpha * (beta - 1))
                    : params_h->alpha;
            auto &bconv_pre =
                drns_tool->v_base_part_Ql_to_compl_part_QlP_conv()[beta_idx];
            auto &ibase = bconv_pre.ibase();
            auto &obase = bconv_pre.obase();
            constexpr int unroll_factor = 2;
            BConv_5<<<params_h->N * obase.size() / 128 / unroll_factor, 128,
                      obase.size() * ibase.size() * sizeof(uint64_t)>>>(
                params_d, edge_MultConst_4_4_BConv_5_0_d,
                edge_BConv_5_0_NTTPhase1_6_0_d, bconv_pre.QHatModp(),
                bconv_pre.ibase().base(), bconv_pre.ibase().size(),
                bconv_pre.obase().base(), bconv_pre.obase().size(),
                startPartIdx, size_PartQl);
        }
        NTTPhase1_6<<<4096, (params_h->n1 / 8) * params_h->pad,
                      (params_h->n1 + params_h->pad + 1) * params_h->pad *
                          sizeof(uint64_t)>>>(
            params_d, 0, 36, edge_BConv_5_0_NTTPhase1_6_0_d,
            edge_NTTPhase1_6_0_NTTPhase2_7_0_d, params_h->ntt_tables->twiddle(),
            params_h->ntt_tables->twiddle_shoup(),
            params_h->ntt_tables->modulus());
        checkCudaErrors(cudaDeviceSynchronize());
        NTTPhase2_7<<<4096, 128,
                      params_h->per_thread_ntt_size * 128 * sizeof(uint64_t)>>>(
            params_d, 0, 36, edge_NTTPhase1_6_0_NTTPhase2_7_0_d,
            edge_NTTPhase2_7_0_MultKeyAccum_8_4_d,
            params_h->ntt_tables->twiddle(),
            params_h->ntt_tables->twiddle_shoup(),
            params_h->ntt_tables->modulus());
        {
            const size_t beta_idx = 3;
            const size_t startPartIdx = params_h->alpha * beta_idx;
            const size_t size_PartQl =
                (beta_idx == beta - 1)
                    ? (params_h->L - params_h->alpha * (beta - 1))
                    : params_h->alpha;
            auto &bconv_pre =
                drns_tool->v_base_part_Ql_to_compl_part_QlP_conv()[beta_idx];
            auto &ibase = bconv_pre.ibase();
            auto &obase = bconv_pre.obase();
            constexpr int unroll_factor = 2;
            BConv_14<<<params_h->N * obase.size() / 128 / unroll_factor, 128,
                       obase.size() * ibase.size() * sizeof(uint64_t)>>>(
                params_d, edge_MultConst_4_3_BConv_14_0_d,
                edge_BConv_14_0_NTTPhase1_15_0_d, bconv_pre.QHatModp(),
                bconv_pre.ibase().base(), bconv_pre.ibase().size(),
                bconv_pre.obase().base(), bconv_pre.obase().size(),
                startPartIdx, size_PartQl);
        }
        NTTPhase1_15<<<4096, (params_h->n1 / 8) * params_h->pad,
                       (params_h->n1 + params_h->pad + 1) * params_h->pad *
                           sizeof(uint64_t)>>>(
            params_d, 0, 36, edge_BConv_14_0_NTTPhase1_15_0_d,
            edge_NTTPhase1_15_0_NTTPhase2_16_0_d,
            params_h->ntt_tables->twiddle(),
            params_h->ntt_tables->twiddle_shoup(),
            params_h->ntt_tables->modulus());
        checkCudaErrors(cudaDeviceSynchronize());
        NTTPhase2_16<<<4096, 128,
                       params_h->per_thread_ntt_size * 128 *
                           sizeof(uint64_t)>>>(
            params_d, 0, 36, edge_NTTPhase1_15_0_NTTPhase2_16_0_d,
            edge_NTTPhase2_16_0_MultKeyAccum_8_3_d,
            params_h->ntt_tables->twiddle(),
            params_h->ntt_tables->twiddle_shoup(),
            params_h->ntt_tables->modulus());
        {
            const size_t beta_idx = 2;
            const size_t startPartIdx = params_h->alpha * beta_idx;
            const size_t size_PartQl =
                (beta_idx == beta - 1)
                    ? (params_h->L - params_h->alpha * (beta - 1))
                    : params_h->alpha;
            auto &bconv_pre =
                drns_tool->v_base_part_Ql_to_compl_part_QlP_conv()[beta_idx];
            auto &ibase = bconv_pre.ibase();
            auto &obase = bconv_pre.obase();
            constexpr int unroll_factor = 2;
            BConv_17<<<params_h->N * obase.size() / 128 / unroll_factor, 128,
                       obase.size() * ibase.size() * sizeof(uint64_t)>>>(
                params_d, edge_MultConst_4_2_BConv_17_0_d,
                edge_BConv_17_0_NTTPhase1_18_0_d, bconv_pre.QHatModp(),
                bconv_pre.ibase().base(), bconv_pre.ibase().size(),
                bconv_pre.obase().base(), bconv_pre.obase().size(),
                startPartIdx, size_PartQl);
        }
        NTTPhase1_18<<<4096, (params_h->n1 / 8) * params_h->pad,
                       (params_h->n1 + params_h->pad + 1) * params_h->pad *
                           sizeof(uint64_t)>>>(
            params_d, 0, 36, edge_BConv_17_0_NTTPhase1_18_0_d,
            edge_NTTPhase1_18_0_NTTPhase2_19_0_d,
            params_h->ntt_tables->twiddle(),
            params_h->ntt_tables->twiddle_shoup(),
            params_h->ntt_tables->modulus());
        checkCudaErrors(cudaDeviceSynchronize());
        NTTPhase2_19<<<4096, 128,
                       params_h->per_thread_ntt_size * 128 *
                           sizeof(uint64_t)>>>(
            params_d, 0, 36, edge_NTTPhase1_18_0_NTTPhase2_19_0_d,
            edge_NTTPhase2_19_0_MultKeyAccum_8_2_d,
            params_h->ntt_tables->twiddle(),
            params_h->ntt_tables->twiddle_shoup(),
            params_h->ntt_tables->modulus());
        {
            const size_t beta_idx = 1;
            const size_t startPartIdx = params_h->alpha * beta_idx;
            const size_t size_PartQl =
                (beta_idx == beta - 1)
                    ? (params_h->L - params_h->alpha * (beta - 1))
                    : params_h->alpha;
            auto &bconv_pre =
                drns_tool->v_base_part_Ql_to_compl_part_QlP_conv()[beta_idx];
            auto &ibase = bconv_pre.ibase();
            auto &obase = bconv_pre.obase();
            constexpr int unroll_factor = 2;
            BConv_20<<<params_h->N * obase.size() / 128 / unroll_factor, 128,
                       obase.size() * ibase.size() * sizeof(uint64_t)>>>(
                params_d, edge_MultConst_4_1_BConv_20_0_d,
                edge_BConv_20_0_NTTPhase1_21_0_d, bconv_pre.QHatModp(),
                bconv_pre.ibase().base(), bconv_pre.ibase().size(),
                bconv_pre.obase().base(), bconv_pre.obase().size(),
                startPartIdx, size_PartQl);
        }
        NTTPhase1_21<<<4096, (params_h->n1 / 8) * params_h->pad,
                       (params_h->n1 + params_h->pad + 1) * params_h->pad *
                           sizeof(uint64_t)>>>(
            params_d, 0, 36, edge_BConv_20_0_NTTPhase1_21_0_d,
            edge_NTTPhase1_21_0_NTTPhase2_22_0_d,
            params_h->ntt_tables->twiddle(),
            params_h->ntt_tables->twiddle_shoup(),
            params_h->ntt_tables->modulus());
        checkCudaErrors(cudaDeviceSynchronize());
        NTTPhase2_22<<<4096, 128,
                       params_h->per_thread_ntt_size * 128 *
                           sizeof(uint64_t)>>>(
            params_d, 0, 36, edge_NTTPhase1_21_0_NTTPhase2_22_0_d,
            edge_NTTPhase2_22_0_MultKeyAccum_8_1_d,
            params_h->ntt_tables->twiddle(),
            params_h->ntt_tables->twiddle_shoup(),
            params_h->ntt_tables->modulus());
        {
            const size_t beta_idx = 0;
            const size_t startPartIdx = params_h->alpha * beta_idx;
            const size_t size_PartQl =
                (beta_idx == beta - 1)
                    ? (params_h->L - params_h->alpha * (beta - 1))
                    : params_h->alpha;
            auto &bconv_pre =
                drns_tool->v_base_part_Ql_to_compl_part_QlP_conv()[beta_idx];
            auto &ibase = bconv_pre.ibase();
            auto &obase = bconv_pre.obase();
            constexpr int unroll_factor = 2;
            BConv_23<<<params_h->N * obase.size() / 128 / unroll_factor, 128,
                       obase.size() * ibase.size() * sizeof(uint64_t)>>>(
                params_d, edge_MultConst_4_0_BConv_23_0_d,
                edge_BConv_23_0_NTTPhase1_24_0_d, bconv_pre.QHatModp(),
                bconv_pre.ibase().base(), bconv_pre.ibase().size(),
                bconv_pre.obase().base(), bconv_pre.obase().size(),
                startPartIdx, size_PartQl);
        }
        NTTPhase1_24<<<4096, (params_h->n1 / 8) * params_h->pad,
                       (params_h->n1 + params_h->pad + 1) * params_h->pad *
                           sizeof(uint64_t)>>>(
            params_d, 0, 36, edge_BConv_23_0_NTTPhase1_24_0_d,
            edge_NTTPhase1_24_0_NTTPhase2_25_0_d,
            params_h->ntt_tables->twiddle(),
            params_h->ntt_tables->twiddle_shoup(),
            params_h->ntt_tables->modulus());
        checkCudaErrors(cudaDeviceSynchronize());
        NTTPhase2_25<<<4096, 128,
                       params_h->per_thread_ntt_size * 128 *
                           sizeof(uint64_t)>>>(
            params_d, 0, 36, 0, 36, edge_NTTPhase1_24_0_NTTPhase2_25_0_d,
            edge_NTTPhase2_25_0_MultKeyAccum_8_0_d,
            params_h->ntt_tables->twiddle(),
            params_h->ntt_tables->twiddle_shoup(),
            params_h->ntt_tables->modulus());
        MultKeyAccum_8<<<4096, 256, 0>>>(
            params_d, 0, 36, edge_NTTPhase2_25_0_MultKeyAccum_8_0_d,
            edge_NTTPhase2_22_0_MultKeyAccum_8_1_d,
            edge_NTTPhase2_19_0_MultKeyAccum_8_2_d,
            edge_NTTPhase2_16_0_MultKeyAccum_8_3_d,
            edge_NTTPhase2_7_0_MultKeyAccum_8_4_d,
            edge_MultKeyAccum_8_0_iNTTPhase2_12_0_d,
            edge_MultKeyAccum_8_1_iNTTPhase2_9_0_d, relin_keys);
        iNTTPhase2_9<<<4096, 128,
                       params_h->per_thread_ntt_size * 128 *
                           sizeof(uint64_t)>>>(
            params_d, 30, 36, edge_MultKeyAccum_8_1_iNTTPhase2_9_0_d,
            edge_iNTTPhase2_9_0_iNTTPhase1_10_0_d);
        checkCudaErrors(cudaDeviceSynchronize());
        iNTTPhase1_10<<<4096, (params_h->n1 / 8) * params_h->pad,
                        (params_h->n1 + params_h->pad + 1) * params_h->pad *
                            sizeof(uint64_t)>>>(
            params_d, 30, 36, edge_iNTTPhase2_9_0_iNTTPhase1_10_0_d,
            edge_iNTTPhase1_10_0_End_11_2_d);
        checkCudaErrors(cudaDeviceSynchronize());
        iNTTPhase2_12<<<4096, 128,
                        params_h->per_thread_ntt_size * 128 *
                            sizeof(uint64_t)>>>(
            params_d, 30, 36, edge_MultKeyAccum_8_0_iNTTPhase2_12_0_d,
            edge_iNTTPhase2_12_0_iNTTPhase1_13_0_d);
        checkCudaErrors(cudaDeviceSynchronize());
        iNTTPhase1_13<<<4096, (params_h->n1 / 8) * params_h->pad,
                        (params_h->n1 + params_h->pad + 1) * params_h->pad *
                            sizeof(uint64_t)>>>(
            params_d, 30, 36, edge_iNTTPhase2_12_0_iNTTPhase1_13_0_d,
            edge_iNTTPhase1_13_0_End_11_1_d);
        Mult_28_Mult_26_Add_27<<<4096, 256>>>(
            params_d, 0, 30, edge_Init_0_2_Mult_28_0_d,
            edge_Init_0_3_Mult_28_1_d, edge_Init_0_4_Mult_26_0_d,
            edge_Init_0_5_Mult_26_1_d, edge_Add_27_0_End_11_0_d);
        Mult_29<<<4096, 256, 0>>>(params_d, 0, 30, edge_Init_0_0_Mult_29_0_d,
                                  edge_Init_0_1_Mult_29_1_d,
                                  edge_Mult_29_0_End_11_3_d);
        checkCudaErrors(cudaDeviceSynchronize());
        // Timer Stop
        auto end = std::chrono::high_resolution_clock::now();
    }

    // =====================================
    // Benchmark
    // =====================================
    if (if_benchmark) {
        std::cout << "### Benchmark" << std::endl;
        std::vector<double> elapsed_times;
        std::vector<double> elapsed_times_ce;

        phantom::DRNSTool *drns_tool = params_h->rns_tools[1];
        const int beta = std::ceil((params_h->L + 1) / params_h->alpha);

        uint64_t **in_list = new uint64_t *[5];
        in_list[0] = edge_NTTPhase2_25_0_MultKeyAccum_8_0_d;
        in_list[1] = edge_NTTPhase2_22_0_MultKeyAccum_8_1_d;
        in_list[2] = edge_NTTPhase2_19_0_MultKeyAccum_8_2_d;
        in_list[3] = edge_NTTPhase2_16_0_MultKeyAccum_8_3_d;
        in_list[4] = edge_NTTPhase2_7_0_MultKeyAccum_8_4_d;
        uint64_t **d_in_list;

        checkCudaErrors(
            cudaMalloc((void **) &d_in_list, sizeof(uint64_t *) * beta));
        checkCudaErrors(cudaMemcpy(d_in_list, in_list,
                                   sizeof(uint64_t *) * beta,
                                   cudaMemcpyHostToDevice));

        uint64_t **bconv_in_list = new uint64_t *[beta];
        bconv_in_list[0] = edge_MultConst_4_0_BConv_23_0_d;
        bconv_in_list[1] = edge_MultConst_4_1_BConv_20_0_d;
        bconv_in_list[2] = edge_MultConst_4_2_BConv_17_0_d;
        bconv_in_list[3] = edge_MultConst_4_3_BConv_14_0_d;
        bconv_in_list[4] = edge_MultConst_4_4_BConv_5_0_d;
        uint64_t **d_bconv_in_list;
        checkCudaErrors(
            cudaMalloc((void **) &d_bconv_in_list, sizeof(uint64_t *) * beta));
        cudaMemcpy(d_bconv_in_list, bconv_in_list, sizeof(uint64_t *) * beta,
                   cudaMemcpyHostToDevice);

        uint64_t **bconv_out_list = new uint64_t *[beta];
        bconv_out_list[0] = edge_BConv_23_0_NTTPhase1_24_0_d;
        bconv_out_list[1] = edge_BConv_20_0_NTTPhase1_21_0_d;
        bconv_out_list[2] = edge_BConv_17_0_NTTPhase1_18_0_d;
        bconv_out_list[3] = edge_BConv_14_0_NTTPhase1_15_0_d;
        bconv_out_list[4] = edge_BConv_5_0_NTTPhase1_6_0_d;
        uint64_t **d_bconv_out_list;
        checkCudaErrors(
            cudaMalloc((void **) &d_bconv_out_list, sizeof(uint64_t *) * beta));
        cudaMemcpy(d_bconv_out_list, bconv_out_list, sizeof(uint64_t *) * beta,
                   cudaMemcpyHostToDevice);

        uint64_t **qhat_modp_list = new uint64_t *[beta];
        for (int j = 0; j < beta; j++) {
            qhat_modp_list[j] =
                drns_tool->v_base_part_Ql_to_compl_part_QlP_conv()[j]
                    .QHatModp();
        }
        uint64_t **d_qhat_modp_list;
        checkCudaErrors(
            cudaMalloc((void **) &d_qhat_modp_list, sizeof(uint64_t *) * beta));
        checkCudaErrors(cudaMemcpyAsync(d_qhat_modp_list, qhat_modp_list,
                                        sizeof(uint64_t *) * beta,
                                        cudaMemcpyHostToDevice));

        for (int i = 0; i < 10; i++) {
            // Call kernel
            // Timer start
            nvtxRangePushA("compute");
            cudaEvent_t ce_start, ce_stop;
            cudaEventCreate(&ce_start);
            cudaEventCreate(&ce_stop);
            auto start = std::chrono::high_resolution_clock::now();
            cudaEventRecord(ce_start);

            Mult_28_Mult_26_Add_27<<<4096, 256>>>(
                params_d, 0, 30, edge_Init_0_2_Mult_28_0_d,
                edge_Init_0_3_Mult_28_1_d, edge_Init_0_4_Mult_26_0_d,
                edge_Init_0_5_Mult_26_1_d, edge_Add_27_0_End_11_0_d);
            Mult_29<<<4096, 256, 0>>>(
                params_d, 0, 30, edge_Init_0_0_Mult_29_0_d,
                edge_Init_0_1_Mult_29_1_d, edge_Mult_29_0_End_11_3_d);

            Mult_1_iNTTPhase2_2<<<4096, 128,
                                  params_h->per_thread_ntt_size * 128 *
                                      sizeof(uint64_t)>>>(
                params_d, 0, 30, edge_Init_0_6_Mult_1_0_d,
                edge_Init_0_7_Mult_1_1_d, edge_Mult_1_0_End_11_4_d,
                edge_iNTTPhase2_2_0_iNTTPhase1_3_0_d);
            checkCudaErrors(cudaDeviceSynchronize());
            iNTTPhase1_3_MultConst_4<<<4096, (params_h->n1 / 8) * params_h->pad,
                                       (params_h->n1 + params_h->pad + 1) *
                                           params_h->pad * sizeof(uint64_t)>>>(
                params_d, 0, 30, edge_iNTTPhase2_2_0_iNTTPhase1_3_0_d,
                edge_MultConst_4_0_BConv_23_0_d,
                edge_MultConst_4_1_BConv_20_0_d,
                edge_MultConst_4_2_BConv_17_0_d,
                edge_MultConst_4_3_BConv_14_0_d, edge_MultConst_4_4_BConv_5_0_d,
                rns_tool->partQlHatInv_mod_Ql_concat(),
                rns_tool->partQlHatInv_mod_Ql_concat_shoup());
            bool if_accum_opt = n_divide > 0;
            if (if_accum_opt) {
                const int limb_per = 6;
                int n_divide_ = 36 / limb_per;
                for (int iter = 0; iter < n_divide_; iter++) {
                    int start_li = iter * limb_per;
                    int end_li = (iter + 1) * limb_per;
                    for (int beta_idx = 0; beta_idx < beta; beta_idx++) {
                        const size_t startPartIdx = params_h->alpha * beta_idx;
                        const size_t size_PartQl =
                            (beta_idx == beta - 1)
                                ? (params_h->L - params_h->alpha * (beta - 1))
                                : params_h->alpha;

                        if (start_li != startPartIdx) {
                            auto &bconv_pre =
                                drns_tool
                                    ->v_base_part_Ql_to_compl_part_QlP_conv()
                                        [beta_idx];
                            constexpr int unroll_factor = 2;
                            int larger_than_startPartIdx =
                                start_li >= startPartIdx;

                            BConv_general_part<<<params_h->N * limb_per / 128 /
                                                     unroll_factor,
                                                 128>>>(
                                params_d, bconv_in_list[beta_idx],
                                bconv_out_list[beta_idx],
                                // bconv_pre.QHatModp(),
                                qhat_modp_list[beta_idx], 6, start_li,
                                (iter - larger_than_startPartIdx) * limb_per,
                                limb_per, startPartIdx, size_PartQl,
                                params_h->ntt_tables->twiddle(),
                                params_h->ntt_tables->twiddle_shoup(),
                                params_h->ntt_tables->modulus());
                        }
                    }

                    NTTP1_part_allbeta<<<4096, 128,
                                         128 * 8 * sizeof(uint64_t)>>>(
                        params_d, start_li, end_li, 0, 36,
                        params_h->ntt_tables->twiddle(),
                        params_h->ntt_tables->twiddle_shoup(),
                        params_h->ntt_tables->modulus(), d_in_list);

                    NTTP2_MultKeyAccum<<<4096, 128,
                                         8 * 128 * sizeof(uint64_t)>>>(
                        params_d, start_li, end_li, 0, 36,
                        edge_NTTPhase1_24_0_NTTPhase2_25_0_d,
                        params_h->ntt_tables->twiddle(),
                        params_h->ntt_tables->twiddle_shoup(),
                        params_h->ntt_tables->modulus(), d_in_list,
                        edge_MultKeyAccum_8_0_iNTTPhase2_12_0_d,
                        edge_MultKeyAccum_8_1_iNTTPhase2_9_0_d, relin_keys);
                }

                // checkCudaErrors(cudaDeviceSynchronize());
            } else {
                {
                    const size_t beta_idx = 4;
                    const size_t startPartIdx = params_h->alpha * beta_idx;
                    const size_t size_PartQl =
                        (beta_idx == beta - 1)
                            ? (params_h->L - params_h->alpha * (beta - 1))
                            : params_h->alpha;
                    auto &bconv_pre =
                        drns_tool
                            ->v_base_part_Ql_to_compl_part_QlP_conv()[beta_idx];
                    auto &ibase = bconv_pre.ibase();
                    auto &obase = bconv_pre.obase();
                    constexpr int unroll_factor = 2;
                    BConv_5<<<params_h->N * obase.size() / 128 / unroll_factor,
                              128,
                              obase.size() * ibase.size() * sizeof(uint64_t)>>>(
                        params_d, edge_MultConst_4_4_BConv_5_0_d,
                        edge_BConv_5_0_NTTPhase1_6_0_d, bconv_pre.QHatModp(),
                        bconv_pre.ibase().base(), bconv_pre.ibase().size(),
                        bconv_pre.obase().base(), bconv_pre.obase().size(),
                        startPartIdx, size_PartQl);
                }
                NTTPhase1_6<<<4096, (params_h->n1 / 8) * params_h->pad,
                              (params_h->n1 + params_h->pad + 1) *
                                  params_h->pad * sizeof(uint64_t)>>>(
                    params_d, 0, 36, edge_BConv_5_0_NTTPhase1_6_0_d,
                    edge_NTTPhase1_6_0_NTTPhase2_7_0_d,
                    params_h->ntt_tables->twiddle(),
                    params_h->ntt_tables->twiddle_shoup(),
                    params_h->ntt_tables->modulus());
                checkCudaErrors(cudaDeviceSynchronize());
                NTTPhase2_7<<<4096, 128,
                              params_h->per_thread_ntt_size * 128 *
                                  sizeof(uint64_t)>>>(
                    params_d, 0, 36, edge_NTTPhase1_6_0_NTTPhase2_7_0_d,
                    edge_NTTPhase2_7_0_MultKeyAccum_8_4_d,
                    params_h->ntt_tables->twiddle(),
                    params_h->ntt_tables->twiddle_shoup(),
                    params_h->ntt_tables->modulus());
                {
                    const size_t beta_idx = 3;
                    const size_t startPartIdx = params_h->alpha * beta_idx;
                    const size_t size_PartQl =
                        (beta_idx == beta - 1)
                            ? (params_h->L - params_h->alpha * (beta - 1))
                            : params_h->alpha;
                    auto &bconv_pre =
                        drns_tool
                            ->v_base_part_Ql_to_compl_part_QlP_conv()[beta_idx];
                    auto &ibase = bconv_pre.ibase();
                    auto &obase = bconv_pre.obase();
                    constexpr int unroll_factor = 2;
                    BConv_14<<<
                        params_h->N * obase.size() / 128 / unroll_factor, 128,
                        obase.size() * ibase.size() * sizeof(uint64_t)>>>(
                        params_d, edge_MultConst_4_3_BConv_14_0_d,
                        edge_BConv_14_0_NTTPhase1_15_0_d, bconv_pre.QHatModp(),
                        bconv_pre.ibase().base(), bconv_pre.ibase().size(),
                        bconv_pre.obase().base(), bconv_pre.obase().size(),
                        startPartIdx, size_PartQl);
                }
                NTTPhase1_15<<<4096, (params_h->n1 / 8) * params_h->pad,
                               (params_h->n1 + params_h->pad + 1) *
                                   params_h->pad * sizeof(uint64_t)>>>(
                    params_d, 0, 36, edge_BConv_14_0_NTTPhase1_15_0_d,
                    edge_NTTPhase1_15_0_NTTPhase2_16_0_d,
                    params_h->ntt_tables->twiddle(),
                    params_h->ntt_tables->twiddle_shoup(),
                    params_h->ntt_tables->modulus());
                checkCudaErrors(cudaDeviceSynchronize());
                NTTPhase2_16<<<4096, 128,
                               params_h->per_thread_ntt_size * 128 *
                                   sizeof(uint64_t)>>>(
                    params_d, 0, 36, edge_NTTPhase1_15_0_NTTPhase2_16_0_d,
                    edge_NTTPhase2_16_0_MultKeyAccum_8_3_d,
                    params_h->ntt_tables->twiddle(),
                    params_h->ntt_tables->twiddle_shoup(),
                    params_h->ntt_tables->modulus());
                {
                    const size_t beta_idx = 2;
                    const size_t startPartIdx = params_h->alpha * beta_idx;
                    const size_t size_PartQl =
                        (beta_idx == beta - 1)
                            ? (params_h->L - params_h->alpha * (beta - 1))
                            : params_h->alpha;
                    auto &bconv_pre =
                        drns_tool
                            ->v_base_part_Ql_to_compl_part_QlP_conv()[beta_idx];
                    auto &ibase = bconv_pre.ibase();
                    auto &obase = bconv_pre.obase();
                    constexpr int unroll_factor = 2;
                    BConv_17<<<
                        params_h->N * obase.size() / 128 / unroll_factor, 128,
                        obase.size() * ibase.size() * sizeof(uint64_t)>>>(
                        params_d, edge_MultConst_4_2_BConv_17_0_d,
                        edge_BConv_17_0_NTTPhase1_18_0_d, bconv_pre.QHatModp(),
                        bconv_pre.ibase().base(), bconv_pre.ibase().size(),
                        bconv_pre.obase().base(), bconv_pre.obase().size(),
                        startPartIdx, size_PartQl);
                }
                NTTPhase1_18<<<4096, (params_h->n1 / 8) * params_h->pad,
                               (params_h->n1 + params_h->pad + 1) *
                                   params_h->pad * sizeof(uint64_t)>>>(
                    params_d, 0, 36, edge_BConv_17_0_NTTPhase1_18_0_d,
                    edge_NTTPhase1_18_0_NTTPhase2_19_0_d,
                    params_h->ntt_tables->twiddle(),
                    params_h->ntt_tables->twiddle_shoup(),
                    params_h->ntt_tables->modulus());
                checkCudaErrors(cudaDeviceSynchronize());
                NTTPhase2_19<<<4096, 128,
                               params_h->per_thread_ntt_size * 128 *
                                   sizeof(uint64_t)>>>(
                    params_d, 0, 36, edge_NTTPhase1_18_0_NTTPhase2_19_0_d,
                    edge_NTTPhase2_19_0_MultKeyAccum_8_2_d,
                    params_h->ntt_tables->twiddle(),
                    params_h->ntt_tables->twiddle_shoup(),
                    params_h->ntt_tables->modulus());
                {
                    const size_t beta_idx = 1;
                    const size_t startPartIdx = params_h->alpha * beta_idx;
                    const size_t size_PartQl =
                        (beta_idx == beta - 1)
                            ? (params_h->L - params_h->alpha * (beta - 1))
                            : params_h->alpha;
                    auto &bconv_pre =
                        drns_tool
                            ->v_base_part_Ql_to_compl_part_QlP_conv()[beta_idx];
                    auto &ibase = bconv_pre.ibase();
                    auto &obase = bconv_pre.obase();
                    constexpr int unroll_factor = 2;
                    BConv_20<<<
                        params_h->N * obase.size() / 128 / unroll_factor, 128,
                        obase.size() * ibase.size() * sizeof(uint64_t)>>>(
                        params_d, edge_MultConst_4_1_BConv_20_0_d,
                        edge_BConv_20_0_NTTPhase1_21_0_d, bconv_pre.QHatModp(),
                        bconv_pre.ibase().base(), bconv_pre.ibase().size(),
                        bconv_pre.obase().base(), bconv_pre.obase().size(),
                        startPartIdx, size_PartQl);
                }
                NTTPhase1_general<<<4096, (params_h->n1 / 8) * params_h->pad,
                                    (params_h->n1 + params_h->pad + 1) *
                                        params_h->pad * sizeof(uint64_t)>>>(
                    params_d, 0, 36, 0, 36, 6, 12,
                    edge_BConv_20_0_NTTPhase1_21_0_d,
                    edge_NTTPhase1_21_0_NTTPhase2_22_0_d,
                    params_h->ntt_tables->twiddle(),
                    params_h->ntt_tables->twiddle_shoup(),
                    params_h->ntt_tables->modulus());
                checkCudaErrors(cudaDeviceSynchronize());
                NTTPhase2_general<<<4096, 128,
                                    params_h->per_thread_ntt_size * 128 *
                                        sizeof(uint64_t)>>>(
                    params_d, 0, 36, 0, 36, 6, 12,
                    edge_NTTPhase1_21_0_NTTPhase2_22_0_d,
                    edge_NTTPhase2_22_0_MultKeyAccum_8_1_d,
                    params_h->ntt_tables->twiddle(),
                    params_h->ntt_tables->twiddle_shoup(),
                    params_h->ntt_tables->modulus());

                {
                    const size_t beta_idx = 0;
                    const size_t startPartIdx = params_h->alpha * beta_idx;
                    const size_t size_PartQl =
                        (beta_idx == beta - 1)
                            ? (params_h->L - params_h->alpha * (beta - 1))
                            : params_h->alpha;
                    auto &bconv_pre =
                        drns_tool
                            ->v_base_part_Ql_to_compl_part_QlP_conv()[beta_idx];
                    auto &ibase = bconv_pre.ibase();
                    auto &obase = bconv_pre.obase();
                    constexpr int unroll_factor = 2;
                    BConv_23<<<
                        params_h->N * obase.size() / 128 / unroll_factor, 128,
                        obase.size() * ibase.size() * sizeof(uint64_t)>>>(
                        params_d, edge_MultConst_4_0_BConv_23_0_d,
                        edge_BConv_23_0_NTTPhase1_24_0_d, bconv_pre.QHatModp(),
                        bconv_pre.ibase().base(), bconv_pre.ibase().size(),
                        bconv_pre.obase().base(), bconv_pre.obase().size(),
                        startPartIdx, size_PartQl);
                }
                NTTPhase1_general<<<4096, 128,
                                    (params_h->n1 + params_h->pad + 1) *
                                        params_h->pad * sizeof(uint64_t)>>>(
                    params_d, 0, 36, 0, 36, 0, 6,
                    edge_BConv_23_0_NTTPhase1_24_0_d,
                    edge_NTTPhase1_24_0_NTTPhase2_25_0_d,
                    params_h->ntt_tables->twiddle(),
                    params_h->ntt_tables->twiddle_shoup(),
                    params_h->ntt_tables->modulus());

                NTTPhase2_general<<<4096, 128,
                                    params_h->per_thread_ntt_size * 128 *
                                        sizeof(uint64_t)>>>(
                    params_d, 0, 36, 0, 36, 0, 6,
                    edge_NTTPhase1_24_0_NTTPhase2_25_0_d,
                    edge_NTTPhase2_25_0_MultKeyAccum_8_0_d,
                    params_h->ntt_tables->twiddle(),
                    params_h->ntt_tables->twiddle_shoup(),
                    params_h->ntt_tables->modulus());

                MultKeyAccum_8<<<4096, 128>>>(
                    params_d, 0, 36, edge_NTTPhase2_25_0_MultKeyAccum_8_0_d,
                    edge_NTTPhase2_22_0_MultKeyAccum_8_1_d,
                    edge_NTTPhase2_19_0_MultKeyAccum_8_2_d,
                    edge_NTTPhase2_16_0_MultKeyAccum_8_3_d,
                    edge_NTTPhase2_7_0_MultKeyAccum_8_4_d,
                    edge_MultKeyAccum_8_0_iNTTPhase2_12_0_d,
                    edge_MultKeyAccum_8_1_iNTTPhase2_9_0_d, relin_keys);
                checkCudaErrors(cudaDeviceSynchronize());
            }

            iNTTPhase2_9<<<4096, 128,
                           params_h->per_thread_ntt_size * 128 *
                               sizeof(uint64_t)>>>(
                params_d, 30, 36, edge_MultKeyAccum_8_1_iNTTPhase2_9_0_d,
                edge_iNTTPhase2_9_0_iNTTPhase1_10_0_d);
            // checkCudaErrors(cudaDeviceSynchronize());
            iNTTPhase1_10<<<4096, (params_h->n1 / 8) * params_h->pad,
                            (params_h->n1 + params_h->pad + 1) * params_h->pad *
                                sizeof(uint64_t)>>>(
                params_d, 30, 36, edge_iNTTPhase2_9_0_iNTTPhase1_10_0_d,
                edge_iNTTPhase1_10_0_End_11_2_d);
            // checkCudaErrors(cudaDeviceSynchronize());
            iNTTPhase2_12<<<4096, 128,
                            params_h->per_thread_ntt_size * 128 *
                                sizeof(uint64_t)>>>(
                params_d, 30, 36, edge_MultKeyAccum_8_0_iNTTPhase2_12_0_d,
                edge_iNTTPhase2_12_0_iNTTPhase1_13_0_d);
            // checkCudaErrors(cudaDeviceSynchronize());
            iNTTPhase1_13<<<4096, (params_h->n1 / 8) * params_h->pad,
                            (params_h->n1 + params_h->pad + 1) * params_h->pad *
                                sizeof(uint64_t)>>>(
                params_d, 30, 36, edge_iNTTPhase2_12_0_iNTTPhase1_13_0_d,
                edge_iNTTPhase1_13_0_End_11_1_d);
            checkCudaErrors(cudaDeviceSynchronize());

            // Timer Stop
            cudaEventRecord(ce_stop);
            cudaEventSynchronize(ce_stop);
            auto end = std::chrono::high_resolution_clock::now();

            float milliseconds = 0;
            auto elapsed_usec =
                std::chrono::duration_cast<std::chrono::microseconds>(end -
                                                                      start);
            cudaEventElapsedTime(&milliseconds, ce_start, ce_stop);
            cudaEventDestroy(ce_start);
            cudaEventDestroy(ce_stop);
            nvtxRangePop();

            if (i != 0) {
                elapsed_times.push_back(elapsed_usec.count());
                elapsed_times_ce.push_back(milliseconds);
            }
        }
        std::cout << "Average time (CudaEvent) [ms]: "
                  << std::accumulate(elapsed_times_ce.begin(),
                                     elapsed_times_ce.end(), 0.0) /
                         elapsed_times_ce.size()
                  << std::endl;
        std::cout << "Average time[us]: "
                  << std::accumulate(elapsed_times.begin(), elapsed_times.end(),
                                     0.0) /
                         elapsed_times.size()
                  << std::endl;
    }
}
