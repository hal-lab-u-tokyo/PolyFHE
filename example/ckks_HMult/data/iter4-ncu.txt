Compiling example code...
nvcc -std=c++17 -O3 -I. -I../../thirdparty -I../../thirdparty/phantom-fhe/include -I../../ -Xlinker -rpath -Xlinker ../../build/thirdparty/phantom-fhe/lib -L../../build/thirdparty/phantom-fhe/lib -lPhantom -lpthread -lnvToolsExt -o build/example.out build/generated.cu example.cu 
ncu --nvtx --nvtx-include "compute/" ./build/example.out
==PROF== Connected to process 41596 (/opt/mount/PolyFHE/example/ckks_HMult/build/example.out)
/
| Encryption parameters :
|   scheme: CKKS
|   poly_modulus_degree: 65536
|   coeff_modulus size: 1580 (60 + 40 + 40 + 40 + 40 + 40 + 40 + 40 + 40 + 40 + 40 + 40 + 40 + 40 + 40 + 40 + 40 + 40 + 40 + 40 + 40 + 40 + 40 + 40 + 40 + 40 + 40 + 40 + 40 + 40 + 60 + 60 + 60 + 60 + 60 + 60) bits

1152921504589938689 ,  1099460640769 ,  1099460902913 ,  1099461820417 ,  1099463786497 ,  1099465359361 ,  1099467194369 ,  1099468505089 ,  1099468767233 ,  1099469684737 ,  1099479121921 ,  1099480956929 ,  1099482923009 ,  1099484495873 ,  1099484889089 ,  1099486855169 ,  1099488428033 ,  1099489607681 ,  1099490000897 ,  1099498258433 ,  1099499175937 ,  1099499569153 ,  1099500617729 ,  1099502714881 ,  1099503370241 ,  1099503894529 ,  1099504549889 ,  1099506515969 ,  1099507695617 ,  1099510054913 ,  1152921504592429057 ,  1152921504592822273 ,  1152921504595968001 ,  1152921504597016577 ,  1152921504598720513 ,  1152921504606584833 ,  

\
Input vector 1: length = 32768

    [ 0.1416183 + i * 0.4965996, 0.8039969 + i * 0.2411065, 0.0067704 + i * 0.9177580, ..., 0.7622014 + i * 0.0630792, 0.7892954 + i * 0.4921201, 0.6376603 + i * 0.9398093 ]

Input vector 2: length = 32768

    [ 0.4186610 + i * 0.4934903, 0.9734065 + i * 0.9239546, 0.5539651 + i * 0.0461687, ..., 0.0729490 + i * 0.9032878, 0.7347246 + i * 0.5525427, 0.9823806 + i * 0.4039763 ]

x_plain.chain_index(): 1
x_plain.chain_index(): 1
x_cipher.chain_index(): 1
coeff_mod_size: 30
beta: 5
### Warm up and Test
N : 65536
L : 30
dnum : 5
alpha : 6
### Benchmark
==PROF== Profiling "NTTPhase2_general" - 0: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 1: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 2: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 3: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 4: 0%....50%....100% - 8 passes
==PROF== Profiling "MultKeyAccum_8" - 5: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 6: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 7: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 8: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 9: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 10: 0%....50%....100% - 8 passes
==PROF== Profiling "MultKeyAccum_8" - 11: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 12: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 13: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 14: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 15: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 16: 0%....50%....100% - 8 passes
==PROF== Profiling "MultKeyAccum_8" - 17: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 18: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 19: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 20: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 21: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 22: 0%....50%....100% - 8 passes
==PROF== Profiling "MultKeyAccum_8" - 23: 0%....50%....100% - 8 passes
Elapsed time: 4676740us
==PROF== Profiling "NTTPhase2_general" - 24: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 25: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 26: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 27: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 28: 0%....50%....100% - 8 passes
==PROF== Profiling "MultKeyAccum_8" - 29: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 30: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 31: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 32: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 33: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 34: 0%....50%....100% - 8 passes
==PROF== Profiling "MultKeyAccum_8" - 35: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 36: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 37: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 38: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 39: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 40: 0%....50%....100% - 8 passes
==PROF== Profiling "MultKeyAccum_8" - 41: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 42: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 43: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 44: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 45: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 46: 0%....50%....100% - 8 passes
==PROF== Profiling "MultKeyAccum_8" - 47: 0%....50%....100% - 8 passes
Elapsed time: 4315800us
==PROF== Profiling "NTTPhase2_general" - 48: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 49: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 50: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 51: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 52: 0%....50%....100% - 8 passes
==PROF== Profiling "MultKeyAccum_8" - 53: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 54: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 55: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 56: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 57: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 58: 0%....50%....100% - 8 passes
==PROF== Profiling "MultKeyAccum_8" - 59: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 60: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 61: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 62: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 63: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 64: 0%....50%....100% - 8 passes
==PROF== Profiling "MultKeyAccum_8" - 65: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 66: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 67: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 68: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 69: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 70: 0%....50%....100% - 8 passes
==PROF== Profiling "MultKeyAccum_8" - 71: 0%....50%....100% - 8 passes
Elapsed time: 4323276us
==PROF== Profiling "NTTPhase2_general" - 72: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 73: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 74: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 75: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 76: 0%....50%....100% - 8 passes
==PROF== Profiling "MultKeyAccum_8" - 77: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 78: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 79: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 80: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 81: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 82: 0%....50%....100% - 8 passes
==PROF== Profiling "MultKeyAccum_8" - 83: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 84: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 85: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 86: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 87: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 88: 0%....50%....100% - 8 passes
==PROF== Profiling "MultKeyAccum_8" - 89: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 90: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 91: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 92: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 93: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 94: 0%....50%....100% - 8 passes
==PROF== Profiling "MultKeyAccum_8" - 95: 0%....50%....100% - 8 passes
Elapsed time: 4321880us
==PROF== Profiling "NTTPhase2_general" - 96: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 97: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 98: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 99: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 100: 0%....50%....100% - 8 passes
==PROF== Profiling "MultKeyAccum_8" - 101: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 102: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 103: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 104: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 105: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 106: 0%....50%....100% - 8 passes
==PROF== Profiling "MultKeyAccum_8" - 107: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 108: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 109: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 110: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 111: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 112: 0%....50%....100% - 8 passes
==PROF== Profiling "MultKeyAccum_8" - 113: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 114: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 115: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 116: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 117: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 118: 0%....50%....100% - 8 passes
==PROF== Profiling "MultKeyAccum_8" - 119: 0%....50%....100% - 8 passes
Elapsed time: 4350327us
==PROF== Profiling "NTTPhase2_general" - 120: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 121: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 122: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 123: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 124: 0%....50%....100% - 8 passes
==PROF== Profiling "MultKeyAccum_8" - 125: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 126: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 127: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 128: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 129: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 130: 0%....50%....100% - 8 passes
==PROF== Profiling "MultKeyAccum_8" - 131: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 132: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 133: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 134: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 135: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 136: 0%....50%....100% - 8 passes
==PROF== Profiling "MultKeyAccum_8" - 137: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 138: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 139: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 140: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 141: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 142: 0%....50%....100% - 8 passes
==PROF== Profiling "MultKeyAccum_8" - 143: 0%....50%....100% - 8 passes
Elapsed time: 4321688us
==PROF== Profiling "NTTPhase2_general" - 144: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 145: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 146: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 147: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 148: 0%....50%....100% - 8 passes
==PROF== Profiling "MultKeyAccum_8" - 149: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 150: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 151: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 152: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 153: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 154: 0%....50%....100% - 8 passes
==PROF== Profiling "MultKeyAccum_8" - 155: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 156: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 157: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 158: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 159: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 160: 0%....50%....100% - 8 passes
==PROF== Profiling "MultKeyAccum_8" - 161: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 162: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 163: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 164: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 165: 0%....50%....100% - 8 passes
==PROF== Profiling "NTTPhase2_general" - 166: 0%....50%....100% - 8 passes
==PROF== Profiling "MultKeyAccum_8" - 167: 0%....50%....100% - 8 passes
Elapsed time: 4293946us
Average time[us]: 4.32115e+06
xy_cipher.chain_index(): 1
idx: 0
  OK
idx: 1
  OK
idx: 2
  OK
Modup result
params_h.KL: 36
poly_degree: 65536
beta_idx: 0
beta_idx: 1
  OK
Average elapsed time (Phantom): 5167.17 us
==PROF== Disconnected from process 41596
[41596] example.out@127.0.0.1
  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.22
    SM Frequency            cycle/nsecond         2.22
    Elapsed Cycles                  cycle        44536
    Memory Throughput                   %        71.98
    DRAM Throughput                     %        71.98
    Duration                      usecond        20.03
    L1/TEX Cache Throughput             %        33.06
    L2 Cache Throughput                 %        31.26
    SM Active Cycles                cycle     34232.67
    Compute (SM) Throughput             %        18.42
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        36.78
    Achieved Active Warps Per SM           warp        17.65
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.44%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (36.8%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       147408
    Total DRAM Elapsed Cycles        cycle      2457600
    Average L1 Active Cycles         cycle     34232.67
    Total L1 Elapsed Cycles          cycle      5539686
    Average L2 Active Cycles         cycle     32694.89
    Total L2 Elapsed Cycles          cycle      1411668
    Average SM Active Cycles         cycle     34232.67
    Total SM Elapsed Cycles          cycle      5539686
    Average SMSP Active Cycles       cycle     34274.01
    Total SMSP Elapsed Cycles        cycle     22158744
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 14.38%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 18.17% above the average, while the minimum instance value is 30.96% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.37%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 15.62% above the average, while the minimum instance value is 31.13% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 14.38%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 18.17% above the average, while the minimum instance value is 30.96% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.22
    SM Frequency            cycle/nsecond         2.19
    Elapsed Cycles                  cycle        42787
    Memory Throughput                   %        73.98
    DRAM Throughput                     %        73.98
    Duration                      usecond        19.49
    L1/TEX Cache Throughput             %        33.43
    L2 Cache Throughput                 %        32.35
    SM Active Cycles                cycle     33870.30
    Compute (SM) Throughput             %        18.17
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        37.25
    Achieved Active Warps Per SM           warp        17.88
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.5%                                                                                     
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (37.3%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    147413.33
    Total DRAM Elapsed Cycles        cycle      2391040
    Average L1 Active Cycles         cycle     33870.30
    Total L1 Elapsed Cycles          cycle      5613338
    Average L2 Active Cycles         cycle        32724
    Total L2 Elapsed Cycles          cycle      1364112
    Average SM Active Cycles         cycle     33870.30
    Total SM Elapsed Cycles          cycle      5613338
    Average SMSP Active Cycles       cycle     34870.39
    Total SMSP Elapsed Cycles        cycle     22453352
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 12.15%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 15.73% above the average, while the minimum instance value is 30.30% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.07%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 15.18% above the average, while the minimum instance value is 29.63% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.15%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 15.73% above the average, while the minimum instance value is 30.30% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.23
    SM Frequency            cycle/nsecond         2.19
    Elapsed Cycles                  cycle        43034
    Memory Throughput                   %        73.57
    DRAM Throughput                     %        73.57
    Duration                      usecond        19.58
    L1/TEX Cache Throughput             %        33.22
    L2 Cache Throughput                 %        32.17
    SM Active Cycles                cycle     34105.56
    Compute (SM) Throughput             %        18.48
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        37.34
    Achieved Active Warps Per SM           warp        17.92
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.33%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (37.3%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    147413.33
    Total DRAM Elapsed Cycles        cycle      2404352
    Average L1 Active Cycles         cycle     34105.56
    Total L1 Elapsed Cycles          cycle      5521134
    Average L2 Active Cycles         cycle        32351
    Total L2 Elapsed Cycles          cycle      1371348
    Average SM Active Cycles         cycle     34105.56
    Total SM Elapsed Cycles          cycle      5521134
    Average SMSP Active Cycles       cycle     34048.09
    Total SMSP Elapsed Cycles        cycle     22084536
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.92%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 15.07% above the average, while the minimum instance value is 33.27% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.24%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 15.51% above the average, while the minimum instance value is 31.52% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.92%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 15.07% above the average, while the minimum instance value is 33.27% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.19
    SM Frequency            cycle/nsecond         2.18
    Elapsed Cycles                  cycle        31797
    Memory Throughput                   %        66.45
    DRAM Throughput                     %        66.45
    Duration                      usecond        14.53
    L1/TEX Cache Throughput             %        28.30
    L2 Cache Throughput                 %        29.07
    SM Active Cycles                cycle     26958.68
    Compute (SM) Throughput             %        17.33
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        25.29
    Achieved Active Warps Per SM           warp        12.14
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 49.43%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (25.3%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     98386.67
    Total DRAM Elapsed Cycles        cycle      1776640
    Average L1 Active Cycles         cycle     26958.68
    Total L1 Elapsed Cycles          cycle      4037896
    Average L2 Active Cycles         cycle     22990.44
    Total L2 Elapsed Cycles          cycle      1012068
    Average SM Active Cycles         cycle     26958.68
    Total SM Elapsed Cycles          cycle      4037896
    Average SMSP Active Cycles       cycle     27313.08
    Total SMSP Elapsed Cycles        cycle     16151584
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.408%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 7.50% above the average, while the minimum instance value is 24.89% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.408%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 7.50% above the average, while the minimum instance value is 24.89% below the       
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.16
    SM Frequency            cycle/nsecond         2.19
    Elapsed Cycles                  cycle        22863
    Memory Throughput                   %        46.72
    DRAM Throughput                     %        46.72
    Duration                      usecond        10.40
    L1/TEX Cache Throughput             %        34.41
    L2 Cache Throughput                 %        20.25
    SM Active Cycles                cycle     11685.20
    Compute (SM) Throughput             %        14.08
    ----------------------- ------------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        25.89
    Achieved Active Warps Per SM           warp        12.43
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 48.22%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (25.9%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     49354.67
    Total DRAM Elapsed Cycles        cycle      1267712
    Average L1 Active Cycles         cycle     11685.20
    Total L1 Elapsed Cycles          cycle      2845718
    Average L2 Active Cycles         cycle     14362.94
    Total L2 Elapsed Cycles          cycle       726264
    Average SM Active Cycles         cycle     11685.20
    Total SM Elapsed Cycles          cycle      2845718
    Average SMSP Active Cycles       cycle     11323.90
    Total SMSP Elapsed Cycles        cycle     11382872
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 22.05%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 41.95% above the average, while the minimum instance value is 68.49% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 21.48%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 42.16% above the average, while the minimum instance value is 72.41% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 22.05%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 41.95% above the average, while the minimum instance value is 68.49% below the      
          average.                                                                                                      

  MultKeyAccum_8(Params *, int, int, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long **) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.23
    SM Frequency            cycle/nsecond         2.23
    Elapsed Cycles                  cycle       180657
    Memory Throughput                   %        92.31
    DRAM Throughput                     %        92.31
    Duration                      usecond        80.90
    L1/TEX Cache Throughput             %         9.91
    L2 Cache Throughput                 %        38.68
    SM Active Cycles                cycle    174585.84
    Compute (SM) Throughput             %        10.15
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                2.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           12
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        86.71
    Achieved Active Warps Per SM           warp        41.62
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 13.29%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (86.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    764257.33
    Total DRAM Elapsed Cycles        cycle      9934848
    Average L1 Active Cycles         cycle    174585.84
    Total L1 Elapsed Cycles          cycle     22118750
    Average L2 Active Cycles         cycle    146692.42
    Total L2 Elapsed Cycles          cycle      5718492
    Average SM Active Cycles         cycle    174585.84
    Total SM Elapsed Cycles          cycle     22118750
    Average SMSP Active Cycles       cycle    166018.80
    Total SMSP Elapsed Cycles        cycle     88475000
    -------------------------- ----------- ------------

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.21
    SM Frequency            cycle/nsecond         2.22
    Elapsed Cycles                  cycle        42877
    Memory Throughput                   %        74.85
    DRAM Throughput                     %        74.85
    Duration                      usecond        19.30
    L1/TEX Cache Throughput             %        32.97
    L2 Cache Throughput                 %        32.46
    SM Active Cycles                cycle     34346.27
    Compute (SM) Throughput             %        18.43
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        36.47
    Achieved Active Warps Per SM           warp        17.51
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.05%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (36.5%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    147414.67
    Total DRAM Elapsed Cycles        cycle      2363392
    Average L1 Active Cycles         cycle     34346.27
    Total L1 Elapsed Cycles          cycle      5534556
    Average L2 Active Cycles         cycle     32736.50
    Total L2 Elapsed Cycles          cycle      1359360
    Average SM Active Cycles         cycle     34346.27
    Total SM Elapsed Cycles          cycle      5534556
    Average SMSP Active Cycles       cycle     34458.05
    Total SMSP Elapsed Cycles        cycle     22138224
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.41%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 14.36% above the average, while the minimum instance value is 26.96% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 13.02%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 16.34% above the average, while the minimum instance value is 31.73% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.41%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 14.36% above the average, while the minimum instance value is 26.96% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.20
    SM Frequency            cycle/nsecond         2.18
    Elapsed Cycles                  cycle        43518
    Memory Throughput                   %        72.86
    DRAM Throughput                     %        72.86
    Duration                      usecond        19.84
    L1/TEX Cache Throughput             %        33.21
    L2 Cache Throughput                 %        31.82
    SM Active Cycles                cycle     34116.82
    Compute (SM) Throughput             %        18.23
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        36.99
    Achieved Active Warps Per SM           warp        17.76
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.02%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (37.0%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    147406.67
    Total DRAM Elapsed Cycles        cycle      2427904
    Average L1 Active Cycles         cycle     34116.82
    Total L1 Elapsed Cycles          cycle      5596030
    Average L2 Active Cycles         cycle     32685.92
    Total L2 Elapsed Cycles          cycle      1386756
    Average SM Active Cycles         cycle     34116.82
    Total SM Elapsed Cycles          cycle      5596030
    Average SMSP Active Cycles       cycle     34318.50
    Total SMSP Elapsed Cycles        cycle     22384120
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 12.62%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 16.17% above the average, while the minimum instance value is 28.65% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.46%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 15.87% above the average, while the minimum instance value is 31.25% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.62%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 16.17% above the average, while the minimum instance value is 28.65% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.17
    SM Frequency            cycle/nsecond         2.19
    Elapsed Cycles                  cycle        22908
    Memory Throughput                   %        46.53
    DRAM Throughput                     %        46.53
    Duration                      usecond        10.43
    L1/TEX Cache Throughput             %        34.37
    L2 Cache Throughput                 %        20.20
    SM Active Cycles                cycle     11723.87
    Compute (SM) Throughput             %        13.76
    ----------------------- ------------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        26.20
    Achieved Active Warps Per SM           warp        12.58
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 47.6%                                                                                     
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (26.2%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        49356
    Total DRAM Elapsed Cycles        cycle      1272832
    Average L1 Active Cycles         cycle     11723.87
    Total L1 Elapsed Cycles          cycle      2912598
    Average L2 Active Cycles         cycle     14299.75
    Total L2 Elapsed Cycles          cycle       727956
    Average SM Active Cycles         cycle     11723.87
    Total SM Elapsed Cycles          cycle      2912598
    Average SMSP Active Cycles       cycle     11326.91
    Total SMSP Elapsed Cycles        cycle     11650392
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 21.57%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 41.87% above the average, while the minimum instance value is 66.69% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.97%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 42.12% above the average, while the minimum instance value is 70.71% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 21.57%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 41.87% above the average, while the minimum instance value is 66.69% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.22
    SM Frequency            cycle/nsecond         2.20
    Elapsed Cycles                  cycle        31924
    Memory Throughput                   %        66.53
    DRAM Throughput                     %        66.53
    Duration                      usecond        14.46
    L1/TEX Cache Throughput             %        28.37
    L2 Cache Throughput                 %        28.99
    SM Active Cycles                cycle     26882.09
    Compute (SM) Throughput             %        16.97
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        25.08
    Achieved Active Warps Per SM           warp        12.04
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 49.83%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (25.1%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     98382.67
    Total DRAM Elapsed Cycles        cycle      1774592
    Average L1 Active Cycles         cycle     26882.09
    Total L1 Elapsed Cycles          cycle      4123684
    Average L2 Active Cycles         cycle     22684.31
    Total L2 Elapsed Cycles          cycle      1014768
    Average SM Active Cycles         cycle     26882.09
    Total SM Elapsed Cycles          cycle      4123684
    Average SMSP Active Cycles       cycle     26624.03
    Total SMSP Elapsed Cycles        cycle     16494736
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.915%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 7.09% above the average, while the minimum instance value is 20.52% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.709%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 9.33% above the average, while the minimum instance value is 27.30% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.915%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 7.09% above the average, while the minimum instance value is 20.52% below the       
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.22
    SM Frequency            cycle/nsecond         2.21
    Elapsed Cycles                  cycle        43438
    Memory Throughput                   %        73.66
    DRAM Throughput                     %        73.66
    Duration                      usecond        19.58
    L1/TEX Cache Throughput             %        32.83
    L2 Cache Throughput                 %        32.01
    SM Active Cycles                cycle     34504.60
    Compute (SM) Throughput             %        18.44
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        36.53
    Achieved Active Warps Per SM           warp        17.53
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.94%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (36.5%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    147406.67
    Total DRAM Elapsed Cycles        cycle      2401280
    Average L1 Active Cycles         cycle     34504.60
    Total L1 Elapsed Cycles          cycle      5531818
    Average L2 Active Cycles         cycle     32622.81
    Total L2 Elapsed Cycles          cycle      1378584
    Average SM Active Cycles         cycle     34504.60
    Total SM Elapsed Cycles          cycle      5531818
    Average SMSP Active Cycles       cycle     34320.75
    Total SMSP Elapsed Cycles        cycle     22127272
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 12.34%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 15.46% above the average, while the minimum instance value is 32.05% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.39%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 15.61% above the average, while the minimum instance value is 32.30% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.34%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 15.46% above the average, while the minimum instance value is 32.05% below the      
          average.                                                                                                      

  MultKeyAccum_8(Params *, int, int, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long **) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.23
    SM Frequency            cycle/nsecond         2.23
    Elapsed Cycles                  cycle       181481
    Memory Throughput                   %        92.05
    DRAM Throughput                     %        92.05
    Duration                      usecond        81.25
    L1/TEX Cache Throughput             %         9.88
    L2 Cache Throughput                 %        38.50
    SM Active Cycles                cycle    175092.72
    Compute (SM) Throughput             %        10.16
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                2.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           12
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        87.00
    Achieved Active Warps Per SM           warp        41.76
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 13%                                                                                       
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (87.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       765368
    Total DRAM Elapsed Cycles        cycle      9977856
    Average L1 Active Cycles         cycle    175092.72
    Total L1 Elapsed Cycles          cycle     22099632
    Average L2 Active Cycles         cycle    147204.22
    Total L2 Elapsed Cycles          cycle      5744700
    Average SM Active Cycles         cycle    175092.72
    Total SM Elapsed Cycles          cycle     22099632
    Average SMSP Active Cycles       cycle    166358.76
    Total SMSP Elapsed Cycles        cycle     88398528
    -------------------------- ----------- ------------

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.20
    SM Frequency            cycle/nsecond         2.18
    Elapsed Cycles                  cycle        31812
    Memory Throughput                   %        66.57
    DRAM Throughput                     %        66.57
    Duration                      usecond        14.50
    L1/TEX Cache Throughput             %        28.10
    L2 Cache Throughput                 %        29.04
    SM Active Cycles                cycle     27158.66
    Compute (SM) Throughput             %        16.88
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        24.89
    Achieved Active Warps Per SM           warp        11.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50.22%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (24.9%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     98389.33
    Total DRAM Elapsed Cycles        cycle      1773568
    Average L1 Active Cycles         cycle     27158.66
    Total L1 Elapsed Cycles          cycle      4147458
    Average L2 Active Cycles         cycle     22884.08
    Total L2 Elapsed Cycles          cycle      1013040
    Average SM Active Cycles         cycle     27158.66
    Total SM Elapsed Cycles          cycle      4147458
    Average SMSP Active Cycles       cycle     26883.94
    Total SMSP Elapsed Cycles        cycle     16589832
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.376%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 6.41% above the average, while the minimum instance value is 24.25% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.389%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 7.70% above the average, while the minimum instance value is 19.12% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.376%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 6.41% above the average, while the minimum instance value is 24.25% below the       
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.18
    SM Frequency            cycle/nsecond         2.19
    Elapsed Cycles                  cycle        22791
    Memory Throughput                   %        46.76
    DRAM Throughput                     %        46.76
    Duration                      usecond        10.37
    L1/TEX Cache Throughput             %        33.98
    L2 Cache Throughput                 %        20.31
    SM Active Cycles                cycle     11841.68
    Compute (SM) Throughput             %        13.83
    ----------------------- ------------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        25.52
    Achieved Active Warps Per SM           warp        12.25
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 48.97%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (25.5%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     49361.33
    Total DRAM Elapsed Cycles        cycle      1266688
    Average L1 Active Cycles         cycle     11841.68
    Total L1 Elapsed Cycles          cycle      2898202
    Average L2 Active Cycles         cycle     14295.47
    Total L2 Elapsed Cycles          cycle       724320
    Average SM Active Cycles         cycle     11841.68
    Total SM Elapsed Cycles          cycle      2898202
    Average SMSP Active Cycles       cycle     11237.66
    Total SMSP Elapsed Cycles        cycle     11592808
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 21.41%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 40.94% above the average, while the minimum instance value is 68.27% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.92%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 42.16% above the average, while the minimum instance value is 72.17% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 21.41%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 40.94% above the average, while the minimum instance value is 68.27% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.23
    SM Frequency            cycle/nsecond         2.21
    Elapsed Cycles                  cycle        43792
    Memory Throughput                   %        73.10
    DRAM Throughput                     %        73.10
    Duration                      usecond        19.71
    L1/TEX Cache Throughput             %        32.87
    L2 Cache Throughput                 %        31.76
    SM Active Cycles                cycle     34446.01
    Compute (SM) Throughput             %        18.71
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        36.14
    Achieved Active Warps Per SM           warp        17.35
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.72%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (36.1%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       147408
    Total DRAM Elapsed Cycles        cycle      2419712
    Average L1 Active Cycles         cycle     34446.01
    Total L1 Elapsed Cycles          cycle      5453938
    Average L2 Active Cycles         cycle     32605.97
    Total L2 Elapsed Cycles          cycle      1389348
    Average SM Active Cycles         cycle     34446.01
    Total SM Elapsed Cycles          cycle      5453938
    Average SMSP Active Cycles       cycle     34120.27
    Total SMSP Elapsed Cycles        cycle     21815752
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 12.97%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 16.05% above the average, while the minimum instance value is 30.76% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.53%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 15.65% above the average, while the minimum instance value is 30.40% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.97%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 16.05% above the average, while the minimum instance value is 30.76% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.21
    SM Frequency            cycle/nsecond         2.21
    Elapsed Cycles                  cycle        43537
    Memory Throughput                   %        73.60
    DRAM Throughput                     %        73.60
    Duration                      usecond        19.62
    L1/TEX Cache Throughput             %        33.19
    L2 Cache Throughput                 %        31.95
    SM Active Cycles                cycle     34106.66
    Compute (SM) Throughput             %        18.51
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        36.70
    Achieved Active Warps Per SM           warp        17.62
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.59%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (36.7%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    147406.67
    Total DRAM Elapsed Cycles        cycle      2403328
    Average L1 Active Cycles         cycle     34106.66
    Total L1 Elapsed Cycles          cycle      5510472
    Average L2 Active Cycles         cycle     32535.47
    Total L2 Elapsed Cycles          cycle      1380816
    Average SM Active Cycles         cycle     34106.66
    Total SM Elapsed Cycles          cycle      5510472
    Average SMSP Active Cycles       cycle     34650.48
    Total SMSP Elapsed Cycles        cycle     22041888
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 13.06%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 16.49% above the average, while the minimum instance value is 29.93% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.52%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 14.31% above the average, while the minimum instance value is 27.49% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 13.06%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 16.49% above the average, while the minimum instance value is 29.93% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.21
    SM Frequency            cycle/nsecond         2.22
    Elapsed Cycles                  cycle        43794
    Memory Throughput                   %        73.23
    DRAM Throughput                     %        73.23
    Duration                      usecond        19.71
    L1/TEX Cache Throughput             %        32.91
    L2 Cache Throughput                 %        31.77
    SM Active Cycles                cycle     34408.07
    Compute (SM) Throughput             %        18.64
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        36.47
    Achieved Active Warps Per SM           warp        17.50
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.06%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (36.5%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       147404
    Total DRAM Elapsed Cycles        cycle      2415616
    Average L1 Active Cycles         cycle     34408.07
    Total L1 Elapsed Cycles          cycle      5473372
    Average L2 Active Cycles         cycle     32260.64
    Total L2 Elapsed Cycles          cycle      1388772
    Average SM Active Cycles         cycle     34408.07
    Total SM Elapsed Cycles          cycle      5473372
    Average SMSP Active Cycles       cycle     33632.83
    Total SMSP Elapsed Cycles        cycle     21893488
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 13.12%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 16.30% above the average, while the minimum instance value is 29.48% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 13.13%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 16.70% above the average, while the minimum instance value is 33.50% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 13.12%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 16.30% above the average, while the minimum instance value is 29.48% below the      
          average.                                                                                                      

  MultKeyAccum_8(Params *, int, int, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long **) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.24
    SM Frequency            cycle/nsecond         2.23
    Elapsed Cycles                  cycle       180325
    Memory Throughput                   %        92.62
    DRAM Throughput                     %        92.62
    Duration                      usecond        80.74
    L1/TEX Cache Throughput             %         9.87
    L2 Cache Throughput                 %        38.75
    SM Active Cycles                cycle    175249.72
    Compute (SM) Throughput             %        10.15
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                2.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           12
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        86.78
    Achieved Active Warps Per SM           warp        41.65
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 13.22%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (86.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    765341.33
    Total DRAM Elapsed Cycles        cycle      9916416
    Average L1 Active Cycles         cycle    175249.72
    Total L1 Elapsed Cycles          cycle     22122706
    Average L2 Active Cycles         cycle    146948.72
    Total L2 Elapsed Cycles          cycle      5707404
    Average SM Active Cycles         cycle    175249.72
    Total SM Elapsed Cycles          cycle     22122706
    Average SMSP Active Cycles       cycle    165984.04
    Total SMSP Elapsed Cycles        cycle     88490824
    -------------------------- ----------- ------------

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.21
    SM Frequency            cycle/nsecond         2.18
    Elapsed Cycles                  cycle        32063
    Memory Throughput                   %        65.77
    DRAM Throughput                     %        65.77
    Duration                      usecond        14.66
    L1/TEX Cache Throughput             %        28.45
    L2 Cache Throughput                 %        28.76
    SM Active Cycles                cycle     26819.95
    Compute (SM) Throughput             %        16.87
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        25.34
    Achieved Active Warps Per SM           warp        12.16
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 49.33%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (25.3%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     98381.33
    Total DRAM Elapsed Cycles        cycle      1795072
    Average L1 Active Cycles         cycle     26819.95
    Total L1 Elapsed Cycles          cycle      4148020
    Average L2 Active Cycles         cycle     23001.25
    Total L2 Elapsed Cycles          cycle      1022976
    Average SM Active Cycles         cycle     26819.95
    Total SM Elapsed Cycles          cycle      4148020
    Average SMSP Active Cycles       cycle     26900.09
    Total SMSP Elapsed Cycles        cycle     16592080
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.613%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 7.99% above the average, while the minimum instance value is 14.67% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.154%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 7.41% above the average, while the minimum instance value is 15.69% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.613%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 7.99% above the average, while the minimum instance value is 14.67% below the       
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.20
    SM Frequency            cycle/nsecond         2.18
    Elapsed Cycles                  cycle        43081
    Memory Throughput                   %        73.44
    DRAM Throughput                     %        73.44
    Duration                      usecond        19.68
    L1/TEX Cache Throughput             %        33.12
    L2 Cache Throughput                 %        32.12
    SM Active Cycles                cycle     34182.35
    Compute (SM) Throughput             %        18.37
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        36.22
    Achieved Active Warps Per SM           warp        17.39
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.56%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (36.2%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    147406.67
    Total DRAM Elapsed Cycles        cycle      2408448
    Average L1 Active Cycles         cycle     34182.35
    Total L1 Elapsed Cycles          cycle      5552916
    Average L2 Active Cycles         cycle     32632.28
    Total L2 Elapsed Cycles          cycle      1373832
    Average SM Active Cycles         cycle     34182.35
    Total SM Elapsed Cycles          cycle      5552916
    Average SMSP Active Cycles       cycle     34406.55
    Total SMSP Elapsed Cycles        cycle     22211664
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.72%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 14.87% above the average, while the minimum instance value is 30.12% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.86%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 14.96% above the average, while the minimum instance value is 30.25% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.72%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 14.87% above the average, while the minimum instance value is 30.12% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.21
    SM Frequency            cycle/nsecond         2.20
    Elapsed Cycles                  cycle        43173
    Memory Throughput                   %        73.73
    DRAM Throughput                     %        73.73
    Duration                      usecond        19.58
    L1/TEX Cache Throughput             %        32.97
    L2 Cache Throughput                 %        32.12
    SM Active Cycles                cycle     34349.04
    Compute (SM) Throughput             %        18.30
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        36.44
    Achieved Active Warps Per SM           warp        17.49
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.13%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (36.4%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       147408
    Total DRAM Elapsed Cycles        cycle      2399232
    Average L1 Active Cycles         cycle     34349.04
    Total L1 Elapsed Cycles          cycle      5576040
    Average L2 Active Cycles         cycle     32519.56
    Total L2 Elapsed Cycles          cycle      1373652
    Average SM Active Cycles         cycle     34349.04
    Total SM Elapsed Cycles          cycle      5576040
    Average SMSP Active Cycles       cycle     34296.34
    Total SMSP Elapsed Cycles        cycle     22304160
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.86%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 15.04% above the average, while the minimum instance value is 28.43% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.73%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 16.17% above the average, while the minimum instance value is 28.51% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.86%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 15.04% above the average, while the minimum instance value is 28.43% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.21
    SM Frequency            cycle/nsecond         2.21
    Elapsed Cycles                  cycle        43844
    Memory Throughput                   %        73.01
    DRAM Throughput                     %        73.01
    Duration                      usecond        19.78
    L1/TEX Cache Throughput             %        32.64
    L2 Cache Throughput                 %        31.71
    SM Active Cycles                cycle     34664.39
    Compute (SM) Throughput             %        18.60
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        36.48
    Achieved Active Warps Per SM           warp        17.51
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.05%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (36.5%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    147406.67
    Total DRAM Elapsed Cycles        cycle      2422784
    Average L1 Active Cycles         cycle     34664.39
    Total L1 Elapsed Cycles          cycle      5484192
    Average L2 Active Cycles         cycle     32496.58
    Total L2 Elapsed Cycles          cycle      1391472
    Average SM Active Cycles         cycle     34664.39
    Total SM Elapsed Cycles          cycle      5484192
    Average SMSP Active Cycles       cycle     34369.53
    Total SMSP Elapsed Cycles        cycle     21936768
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 12.31%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 15.22% above the average, while the minimum instance value is 33.00% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.24%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 15.26% above the average, while the minimum instance value is 27.85% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.31%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 15.22% above the average, while the minimum instance value is 33.00% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.22
    SM Frequency            cycle/nsecond         2.21
    Elapsed Cycles                  cycle        43412
    Memory Throughput                   %        73.66
    DRAM Throughput                     %        73.66
    Duration                      usecond        19.58
    L1/TEX Cache Throughput             %        32.57
    L2 Cache Throughput                 %        32.01
    SM Active Cycles                cycle     34762.22
    Compute (SM) Throughput             %        18.73
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        36.25
    Achieved Active Warps Per SM           warp        17.40
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.5%                                                                                     
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (36.2%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    147406.67
    Total DRAM Elapsed Cycles        cycle      2401280
    Average L1 Active Cycles         cycle     34762.22
    Total L1 Elapsed Cycles          cycle      5447958
    Average L2 Active Cycles         cycle     32415.33
    Total L2 Elapsed Cycles          cycle      1378260
    Average SM Active Cycles         cycle     34762.22
    Total SM Elapsed Cycles          cycle      5447958
    Average SMSP Active Cycles       cycle     34448.72
    Total SMSP Elapsed Cycles        cycle     21791832
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.39%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 13.94% above the average, while the minimum instance value is 29.54% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.35%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 15.26% above the average, while the minimum instance value is 33.00% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.39%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 13.94% above the average, while the minimum instance value is 29.54% below the      
          average.                                                                                                      

  MultKeyAccum_8(Params *, int, int, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long **) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.24
    SM Frequency            cycle/nsecond         2.23
    Elapsed Cycles                  cycle       180431
    Memory Throughput                   %        92.61
    DRAM Throughput                     %        92.61
    Duration                      usecond        80.86
    L1/TEX Cache Throughput             %        10.00
    L2 Cache Throughput                 %        38.72
    SM Active Cycles                cycle    172986.45
    Compute (SM) Throughput             %        10.14
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                2.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           12
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.26
    Achieved Active Warps Per SM           warp        42.36
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.74%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    766597.33
    Total DRAM Elapsed Cycles        cycle      9932800
    Average L1 Active Cycles         cycle    172986.45
    Total L1 Elapsed Cycles          cycle     22138724
    Average L2 Active Cycles         cycle    146865.97
    Total L2 Elapsed Cycles          cycle      5712948
    Average SM Active Cycles         cycle    172986.45
    Total SM Elapsed Cycles          cycle     22138724
    Average SMSP Active Cycles       cycle    165247.68
    Total SMSP Elapsed Cycles        cycle     88554896
    -------------------------- ----------- ------------

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.23
    SM Frequency            cycle/nsecond         2.19
    Elapsed Cycles                  cycle        43407
    Memory Throughput                   %        73.01
    DRAM Throughput                     %        73.01
    Duration                      usecond        19.74
    L1/TEX Cache Throughput             %        32.72
    L2 Cache Throughput                 %        31.91
    SM Active Cycles                cycle     34631.24
    Compute (SM) Throughput             %        18.47
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        36.18
    Achieved Active Warps Per SM           warp        17.37
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.63%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (36.2%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    147414.67
    Total DRAM Elapsed Cycles        cycle      2422784
    Average L1 Active Cycles         cycle     34631.24
    Total L1 Elapsed Cycles          cycle      5524296
    Average L2 Active Cycles         cycle     32630.03
    Total L2 Elapsed Cycles          cycle      1382868
    Average SM Active Cycles         cycle     34631.24
    Total SM Elapsed Cycles          cycle      5524296
    Average SMSP Active Cycles       cycle     34474.17
    Total SMSP Elapsed Cycles        cycle     22097184
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.68%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 14.55% above the average, while the minimum instance value is 22.92% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.69%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 15.88% above the average, while the minimum instance value is 29.42% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.68%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 14.55% above the average, while the minimum instance value is 22.92% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.21
    SM Frequency            cycle/nsecond         2.20
    Elapsed Cycles                  cycle        43094
    Memory Throughput                   %        73.95
    DRAM Throughput                     %        73.95
    Duration                      usecond        19.52
    L1/TEX Cache Throughput             %        32.74
    L2 Cache Throughput                 %        32.20
    SM Active Cycles                cycle     34592.24
    Compute (SM) Throughput             %        18.52
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        35.98
    Achieved Active Warps Per SM           warp        17.27
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.04%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (36.0%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    147406.67
    Total DRAM Elapsed Cycles        cycle      2392064
    Average L1 Active Cycles         cycle     34592.24
    Total L1 Elapsed Cycles          cycle      5508070
    Average L2 Active Cycles         cycle     32444.06
    Total L2 Elapsed Cycles          cycle      1370376
    Average SM Active Cycles         cycle     34592.24
    Total SM Elapsed Cycles          cycle      5508070
    Average SMSP Active Cycles       cycle     34284.04
    Total SMSP Elapsed Cycles        cycle     22032280
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.47%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 14.27% above the average, while the minimum instance value is 31.37% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.76%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 14.75% above the average, while the minimum instance value is 23.68% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.47%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 14.27% above the average, while the minimum instance value is 31.37% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.23
    SM Frequency            cycle/nsecond         2.21
    Elapsed Cycles                  cycle        43420
    Memory Throughput                   %        73.69
    DRAM Throughput                     %        73.69
    Duration                      usecond        19.55
    L1/TEX Cache Throughput             %        32.70
    L2 Cache Throughput                 %        32.03
    SM Active Cycles                cycle     34621.45
    Compute (SM) Throughput             %        18.35
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        36.61
    Achieved Active Warps Per SM           warp        17.57
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.78%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (36.6%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    147405.33
    Total DRAM Elapsed Cycles        cycle      2400256
    Average L1 Active Cycles         cycle     34621.45
    Total L1 Elapsed Cycles          cycle      5558566
    Average L2 Active Cycles         cycle     32473.28
    Total L2 Elapsed Cycles          cycle      1377540
    Average SM Active Cycles         cycle     34621.45
    Total SM Elapsed Cycles          cycle      5558566
    Average SMSP Active Cycles       cycle     34564.74
    Total SMSP Elapsed Cycles        cycle     22234264
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.73%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 14.71% above the average, while the minimum instance value is 27.66% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.2%                                                                                           
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 14.08% above the average, while the minimum instance value is 30.47% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.73%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 14.71% above the average, while the minimum instance value is 27.66% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.20
    SM Frequency            cycle/nsecond         2.21
    Elapsed Cycles                  cycle        32681
    Memory Throughput                   %        65.36
    DRAM Throughput                     %        65.36
    Duration                      usecond        14.75
    L1/TEX Cache Throughput             %        28.02
    L2 Cache Throughput                 %        28.36
    SM Active Cycles                cycle     27227.96
    Compute (SM) Throughput             %        17.21
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        25.28
    Achieved Active Warps Per SM           warp        12.14
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 49.44%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (25.3%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        98388
    Total DRAM Elapsed Cycles        cycle      1806336
    Average L1 Active Cycles         cycle     27227.96
    Total L1 Elapsed Cycles          cycle      4066380
    Average L2 Active Cycles         cycle     22832.97
    Total L2 Elapsed Cycles          cycle      1037232
    Average SM Active Cycles         cycle     27227.96
    Total SM Elapsed Cycles          cycle      4066380
    Average SMSP Active Cycles       cycle     27118.15
    Total SMSP Elapsed Cycles        cycle     16265520
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.466%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 8.71% above the average, while the minimum instance value is 15.46% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.921%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 9.28% above the average, while the minimum instance value is 18.70% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.466%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 8.71% above the average, while the minimum instance value is 15.46% below the       
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.19
    SM Frequency            cycle/nsecond         2.19
    Elapsed Cycles                  cycle        22667
    Memory Throughput                   %        46.99
    DRAM Throughput                     %        46.99
    Duration                      usecond        10.30
    L1/TEX Cache Throughput             %        34.20
    L2 Cache Throughput                 %        20.42
    SM Active Cycles                cycle     11767.73
    Compute (SM) Throughput             %        14.09
    ----------------------- ------------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        26.23
    Achieved Active Warps Per SM           warp        12.59
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 47.55%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (26.2%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     49357.33
    Total DRAM Elapsed Cycles        cycle      1260544
    Average L1 Active Cycles         cycle     11767.73
    Total L1 Elapsed Cycles          cycle      2845312
    Average L2 Active Cycles         cycle     14395.61
    Total L2 Elapsed Cycles          cycle       720144
    Average SM Active Cycles         cycle     11767.73
    Total SM Elapsed Cycles          cycle      2845312
    Average SMSP Active Cycles       cycle     11345.54
    Total SMSP Elapsed Cycles        cycle     11381248
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 21.61%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 40.82% above the average, while the minimum instance value is 68.49% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 21.84%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 42.79% above the average, while the minimum instance value is 72.43% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 21.61%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 40.82% above the average, while the minimum instance value is 68.49% below the      
          average.                                                                                                      

  MultKeyAccum_8(Params *, int, int, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long **) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.23
    SM Frequency            cycle/nsecond         2.23
    Elapsed Cycles                  cycle       180707
    Memory Throughput                   %        92.20
    DRAM Throughput                     %        92.20
    Duration                      usecond        80.99
    L1/TEX Cache Throughput             %         9.90
    L2 Cache Throughput                 %        38.65
    SM Active Cycles                cycle    174695.85
    Compute (SM) Throughput             %        10.19
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                2.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           12
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        87.03
    Achieved Active Warps Per SM           warp        41.77
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 12.97%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (87.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       764300
    Total DRAM Elapsed Cycles        cycle      9947136
    Average L1 Active Cycles         cycle    174695.85
    Total L1 Elapsed Cycles          cycle     22024196
    Average L2 Active Cycles         cycle    146880.56
    Total L2 Elapsed Cycles          cycle      5722308
    Average SM Active Cycles         cycle    174695.85
    Total SM Elapsed Cycles          cycle     22024196
    Average SMSP Active Cycles       cycle    165695.20
    Total SMSP Elapsed Cycles        cycle     88096784
    -------------------------- ----------- ------------

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.21
    SM Frequency            cycle/nsecond         2.20
    Elapsed Cycles                  cycle        43352
    Memory Throughput                   %        73.48
    DRAM Throughput                     %        73.48
    Duration                      usecond        19.65
    L1/TEX Cache Throughput             %        33.13
    L2 Cache Throughput                 %        32.03
    SM Active Cycles                cycle     34210.09
    Compute (SM) Throughput             %        18.50
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        36.73
    Achieved Active Warps Per SM           warp        17.63
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.55%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (36.7%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    147405.33
    Total DRAM Elapsed Cycles        cycle      2407424
    Average L1 Active Cycles         cycle     34210.09
    Total L1 Elapsed Cycles          cycle      5513980
    Average L2 Active Cycles         cycle     32531.97
    Total L2 Elapsed Cycles          cycle      1377468
    Average SM Active Cycles         cycle     34210.09
    Total SM Elapsed Cycles          cycle      5513980
    Average SMSP Active Cycles       cycle     34155.96
    Total SMSP Elapsed Cycles        cycle     22055920
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 12.47%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 15.70% above the average, while the minimum instance value is 30.47% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.66%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 14.70% above the average, while the minimum instance value is 30.29% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.47%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 15.70% above the average, while the minimum instance value is 30.47% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.22
    SM Frequency            cycle/nsecond         2.21
    Elapsed Cycles                  cycle        42993
    Memory Throughput                   %        74.39
    DRAM Throughput                     %        74.39
    Duration                      usecond        19.39
    L1/TEX Cache Throughput             %        32.93
    L2 Cache Throughput                 %        32.34
    SM Active Cycles                cycle     34374.74
    Compute (SM) Throughput             %        18.50
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        36.70
    Achieved Active Warps Per SM           warp        17.62
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.6%                                                                                     
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (36.7%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    147405.33
    Total DRAM Elapsed Cycles        cycle      2377728
    Average L1 Active Cycles         cycle     34374.74
    Total L1 Elapsed Cycles          cycle      5515256
    Average L2 Active Cycles         cycle     32711.97
    Total L2 Elapsed Cycles          cycle      1364328
    Average SM Active Cycles         cycle     34374.74
    Total SM Elapsed Cycles          cycle      5515256
    Average SMSP Active Cycles       cycle     34234.13
    Total SMSP Elapsed Cycles        cycle     22061024
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.5%                                                                                           
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 14.42% above the average, while the minimum instance value is 28.52% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.25%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 15.42% above the average, while the minimum instance value is 30.97% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.5%                                                                                           
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 14.42% above the average, while the minimum instance value is 28.52% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.19
    SM Frequency            cycle/nsecond         2.22
    Elapsed Cycles                  cycle        22906
    Memory Throughput                   %        47.02
    DRAM Throughput                     %        47.02
    Duration                      usecond        10.30
    L1/TEX Cache Throughput             %        33.54
    L2 Cache Throughput                 %        20.29
    SM Active Cycles                cycle     11994.66
    Compute (SM) Throughput             %        13.92
    ----------------------- ------------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        25.40
    Achieved Active Warps Per SM           warp        12.19
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 49.2%                                                                                     
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (25.4%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        49356
    Total DRAM Elapsed Cycles        cycle      1259520
    Average L1 Active Cycles         cycle     11994.66
    Total L1 Elapsed Cycles          cycle      2879056
    Average L2 Active Cycles         cycle     14360.44
    Total L2 Elapsed Cycles          cycle       725004
    Average SM Active Cycles         cycle     11994.66
    Total SM Elapsed Cycles          cycle      2879056
    Average SMSP Active Cycles       cycle     11325.20
    Total SMSP Elapsed Cycles        cycle     11516224
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 21.74%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 40.77% above the average, while the minimum instance value is 66.85% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 21.67%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 43.04% above the average, while the minimum instance value is 71.74% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 21.74%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 40.77% above the average, while the minimum instance value is 66.85% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.19
    SM Frequency            cycle/nsecond         2.18
    Elapsed Cycles                  cycle        32048
    Memory Throughput                   %        66.00
    DRAM Throughput                     %        66.00
    Duration                      usecond        14.62
    L1/TEX Cache Throughput             %        28.78
    L2 Cache Throughput                 %        28.80
    SM Active Cycles                cycle     26515.38
    Compute (SM) Throughput             %        17.24
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        25.33
    Achieved Active Warps Per SM           warp        12.16
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 49.34%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (25.3%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     98390.67
    Total DRAM Elapsed Cycles        cycle      1788928
    Average L1 Active Cycles         cycle     26515.38
    Total L1 Elapsed Cycles          cycle      4058736
    Average L2 Active Cycles         cycle     22636.06
    Total L2 Elapsed Cycles          cycle      1021500
    Average SM Active Cycles         cycle     26515.38
    Total SM Elapsed Cycles          cycle      4058736
    Average SMSP Active Cycles       cycle     26623.58
    Total SMSP Elapsed Cycles        cycle     16234944
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.55%                                                                                           
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 9.03% above the average, while the minimum instance value is 18.63% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.118%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 7.29% above the average, while the minimum instance value is 22.13% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.55%                                                                                           
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 9.03% above the average, while the minimum instance value is 18.63% below the       
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.22
    SM Frequency            cycle/nsecond         2.19
    Elapsed Cycles                  cycle        42466
    Memory Throughput                   %        74.65
    DRAM Throughput                     %        74.65
    Duration                      usecond        19.33
    L1/TEX Cache Throughput             %        33.30
    L2 Cache Throughput                 %        32.61
    SM Active Cycles                cycle     34024.55
    Compute (SM) Throughput             %        18.28
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        36.67
    Achieved Active Warps Per SM           warp        17.60
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.65%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (36.7%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    147405.33
    Total DRAM Elapsed Cycles        cycle      2369536
    Average L1 Active Cycles         cycle     34024.55
    Total L1 Elapsed Cycles          cycle      5579546
    Average L2 Active Cycles         cycle     32643.83
    Total L2 Elapsed Cycles          cycle      1353060
    Average SM Active Cycles         cycle     34024.55
    Total SM Elapsed Cycles          cycle      5579546
    Average SMSP Active Cycles       cycle     34152.10
    Total SMSP Elapsed Cycles        cycle     22318184
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 10.91%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 13.98% above the average, while the minimum instance value is 32.35% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.62%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 16.11% above the average, while the minimum instance value is 32.59% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 10.91%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 13.98% above the average, while the minimum instance value is 32.35% below the      
          average.                                                                                                      

  MultKeyAccum_8(Params *, int, int, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long **) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.24
    SM Frequency            cycle/nsecond         2.23
    Elapsed Cycles                  cycle       180876
    Memory Throughput                   %        92.26
    DRAM Throughput                     %        92.26
    Duration                      usecond        80.96
    L1/TEX Cache Throughput             %         9.86
    L2 Cache Throughput                 %        38.63
    SM Active Cycles                cycle    175351.80
    Compute (SM) Throughput             %        10.19
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                2.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           12
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        87.31
    Achieved Active Warps Per SM           warp        41.91
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 12.69%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (87.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    764793.33
    Total DRAM Elapsed Cycles        cycle      9947136
    Average L1 Active Cycles         cycle    175351.80
    Total L1 Elapsed Cycles          cycle     22020254
    Average L2 Active Cycles         cycle    146971.78
    Total L2 Elapsed Cycles          cycle      5725764
    Average SM Active Cycles         cycle    175351.80
    Total SM Elapsed Cycles          cycle     22020254
    Average SMSP Active Cycles       cycle    165949.42
    Total SMSP Elapsed Cycles        cycle     88081016
    -------------------------- ----------- ------------

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.20
    SM Frequency            cycle/nsecond         2.21
    Elapsed Cycles                  cycle        32259
    Memory Throughput                   %        66.26
    DRAM Throughput                     %        66.26
    Duration                      usecond        14.56
    L1/TEX Cache Throughput             %        27.85
    L2 Cache Throughput                 %        28.74
    SM Active Cycles                cycle     27385.54
    Compute (SM) Throughput             %        17.08
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        24.87
    Achieved Active Warps Per SM           warp        11.94
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50.27%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (24.9%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        98380
    Total DRAM Elapsed Cycles        cycle      1781760
    Average L1 Active Cycles         cycle     27385.54
    Total L1 Elapsed Cycles          cycle      4097800
    Average L2 Active Cycles         cycle     22954.53
    Total L2 Elapsed Cycles          cycle      1023516
    Average SM Active Cycles         cycle     27385.54
    Total SM Elapsed Cycles          cycle      4097800
    Average SMSP Active Cycles       cycle     26824.28
    Total SMSP Elapsed Cycles        cycle     16391200
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.223%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 7.27% above the average, while the minimum instance value is 15.28% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.229%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 8.63% above the average, while the minimum instance value is 18.66% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.223%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 7.27% above the average, while the minimum instance value is 15.28% below the       
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.19
    SM Frequency            cycle/nsecond         2.22
    Elapsed Cycles                  cycle        23238
    Memory Throughput                   %        46.32
    DRAM Throughput                     %        46.32
    Duration                      usecond        10.46
    L1/TEX Cache Throughput             %        34.28
    L2 Cache Throughput                 %        19.99
    SM Active Cycles                cycle     11721.16
    Compute (SM) Throughput             %        13.79
    ----------------------- ------------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        26.12
    Achieved Active Warps Per SM           warp        12.54
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 47.76%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (26.1%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     49365.33
    Total DRAM Elapsed Cycles        cycle      1278976
    Average L1 Active Cycles         cycle     11721.16
    Total L1 Elapsed Cycles          cycle      2907022
    Average L2 Active Cycles         cycle     14189.06
    Total L2 Elapsed Cycles          cycle       735660
    Average SM Active Cycles         cycle     11721.16
    Total SM Elapsed Cycles          cycle      2907022
    Average SMSP Active Cycles       cycle     11281.96
    Total SMSP Elapsed Cycles        cycle     11628088
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 22.09%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 42.80% above the average, while the minimum instance value is 68.33% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 21.14%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 42.55% above the average, while the minimum instance value is 71.45% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 22.09%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 42.80% above the average, while the minimum instance value is 68.33% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.20
    SM Frequency            cycle/nsecond         2.18
    Elapsed Cycles                  cycle        43614
    Memory Throughput                   %        72.64
    DRAM Throughput                     %        72.64
    Duration                      usecond        19.90
    L1/TEX Cache Throughput             %        33.47
    L2 Cache Throughput                 %        31.72
    SM Active Cycles                cycle     33806.26
    Compute (SM) Throughput             %        18.51
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        37.71
    Achieved Active Warps Per SM           warp        18.10
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.57%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (37.7%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    147405.33
    Total DRAM Elapsed Cycles        cycle      2435072
    Average L1 Active Cycles         cycle     33806.26
    Total L1 Elapsed Cycles          cycle      5512814
    Average L2 Active Cycles         cycle     32538.50
    Total L2 Elapsed Cycles          cycle      1391148
    Average SM Active Cycles         cycle     33806.26
    Total SM Elapsed Cycles          cycle      5512814
    Average SMSP Active Cycles       cycle     34060.62
    Total SMSP Elapsed Cycles        cycle     22051256
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 13.55%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 17.26% above the average, while the minimum instance value is 30.61% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 13.21%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 16.71% above the average, while the minimum instance value is 29.77% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 13.55%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 17.26% above the average, while the minimum instance value is 30.61% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.23
    SM Frequency            cycle/nsecond         2.19
    Elapsed Cycles                  cycle        43029
    Memory Throughput                   %        73.48
    DRAM Throughput                     %        73.48
    Duration                      usecond        19.62
    L1/TEX Cache Throughput             %        33.53
    L2 Cache Throughput                 %        32.16
    SM Active Cycles                cycle     33777.06
    Compute (SM) Throughput             %        18.14
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        36.60
    Achieved Active Warps Per SM           warp        17.57
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.81%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (36.6%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       147408
    Total DRAM Elapsed Cycles        cycle      2407424
    Average L1 Active Cycles         cycle     33777.06
    Total L1 Elapsed Cycles          cycle      5623298
    Average L2 Active Cycles         cycle     32306.56
    Total L2 Elapsed Cycles          cycle      1372176
    Average SM Active Cycles         cycle     33777.06
    Total SM Elapsed Cycles          cycle      5623298
    Average SMSP Active Cycles       cycle     34191.60
    Total SMSP Elapsed Cycles        cycle     22493192
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 12.24%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 15.93% above the average, while the minimum instance value is 29.53% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.61%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 14.91% above the average, while the minimum instance value is 27.87% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.24%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 15.93% above the average, while the minimum instance value is 29.53% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.22
    SM Frequency            cycle/nsecond         2.19
    Elapsed Cycles                  cycle        42578
    Memory Throughput                   %        74.36
    DRAM Throughput                     %        74.36
    Duration                      usecond        19.39
    L1/TEX Cache Throughput             %        33.70
    L2 Cache Throughput                 %        32.52
    SM Active Cycles                cycle     33577.73
    Compute (SM) Throughput             %        18.12
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        36.78
    Achieved Active Warps Per SM           warp        17.65
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.45%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (36.8%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    147406.67
    Total DRAM Elapsed Cycles        cycle      2378752
    Average L1 Active Cycles         cycle     33577.73
    Total L1 Elapsed Cycles          cycle      5631042
    Average L2 Active Cycles         cycle     32525.17
    Total L2 Elapsed Cycles          cycle      1356804
    Average SM Active Cycles         cycle     33577.73
    Total SM Elapsed Cycles          cycle      5631042
    Average SMSP Active Cycles       cycle     34396.65
    Total SMSP Elapsed Cycles        cycle     22524168
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.96%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 15.67% above the average, while the minimum instance value is 26.80% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.58%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 16.09% above the average, while the minimum instance value is 31.25% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.96%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 15.67% above the average, while the minimum instance value is 26.80% below the      
          average.                                                                                                      

  MultKeyAccum_8(Params *, int, int, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long **) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.24
    SM Frequency            cycle/nsecond         2.23
    Elapsed Cycles                  cycle       180092
    Memory Throughput                   %        92.67
    DRAM Throughput                     %        92.67
    Duration                      usecond        80.67
    L1/TEX Cache Throughput             %         9.93
    L2 Cache Throughput                 %        38.80
    SM Active Cycles                cycle    174224.09
    Compute (SM) Throughput             %        10.15
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                2.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           12
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        87.03
    Achieved Active Warps Per SM           warp        41.77
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 12.97%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (87.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    765150.67
    Total DRAM Elapsed Cycles        cycle      9908224
    Average L1 Active Cycles         cycle    174224.09
    Total L1 Elapsed Cycles          cycle     22113274
    Average L2 Active Cycles         cycle    146415.75
    Total L2 Elapsed Cycles          cycle      5701320
    Average SM Active Cycles         cycle    174224.09
    Total SM Elapsed Cycles          cycle     22113274
    Average SMSP Active Cycles       cycle    165355.61
    Total SMSP Elapsed Cycles        cycle     88453096
    -------------------------- ----------- ------------

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.19
    SM Frequency            cycle/nsecond         2.21
    Elapsed Cycles                  cycle        33496
    Memory Throughput                   %        63.77
    DRAM Throughput                     %        63.77
    Duration                      usecond        15.14
    L1/TEX Cache Throughput             %        28.34
    L2 Cache Throughput                 %        27.67
    SM Active Cycles                cycle     26928.70
    Compute (SM) Throughput             %        17.35
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        25.88
    Achieved Active Warps Per SM           warp        12.42
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 48.24%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (25.9%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     98390.67
    Total DRAM Elapsed Cycles        cycle      1851392
    Average L1 Active Cycles         cycle     26928.70
    Total L1 Elapsed Cycles          cycle      4034758
    Average L2 Active Cycles         cycle     22907.83
    Total L2 Elapsed Cycles          cycle      1062900
    Average SM Active Cycles         cycle     26928.70
    Total SM Elapsed Cycles          cycle      4034758
    Average SMSP Active Cycles       cycle     26725.32
    Total SMSP Elapsed Cycles        cycle     16139032
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 10.5%                                                                                           
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 12.29% above the average, while the minimum instance value is 25.91% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.885%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 10.48% above the average, while the minimum instance value is 27.31% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 10.5%                                                                                           
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 12.29% above the average, while the minimum instance value is 25.91% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.22
    SM Frequency            cycle/nsecond         2.18
    Elapsed Cycles                  cycle        43092
    Memory Throughput                   %        73.42
    DRAM Throughput                     %        73.42
    Duration                      usecond        19.65
    L1/TEX Cache Throughput             %        33.37
    L2 Cache Throughput                 %        32.11
    SM Active Cycles                cycle     33934.52
    Compute (SM) Throughput             %        17.99
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        37.29
    Achieved Active Warps Per SM           warp        17.90
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.41%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (37.3%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    147417.33
    Total DRAM Elapsed Cycles        cycle      2409472
    Average L1 Active Cycles         cycle     33934.52
    Total L1 Elapsed Cycles          cycle      5671878
    Average L2 Active Cycles         cycle     32778.81
    Total L2 Elapsed Cycles          cycle      1374336
    Average SM Active Cycles         cycle     33934.52
    Total SM Elapsed Cycles          cycle      5671878
    Average SMSP Active Cycles       cycle     34755.81
    Total SMSP Elapsed Cycles        cycle     22687512
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 12.04%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 15.72% above the average, while the minimum instance value is 31.99% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.81%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 15.06% above the average, while the minimum instance value is 29.80% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.04%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 15.72% above the average, while the minimum instance value is 31.99% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.21
    SM Frequency            cycle/nsecond         2.19
    Elapsed Cycles                  cycle        43385
    Memory Throughput                   %        72.99
    DRAM Throughput                     %        72.99
    Duration                      usecond        19.78
    L1/TEX Cache Throughput             %        33.01
    L2 Cache Throughput                 %        31.91
    SM Active Cycles                cycle     34283.20
    Compute (SM) Throughput             %        18.53
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        36.06
    Achieved Active Warps Per SM           warp        17.31
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.89%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (36.1%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       147428
    Total DRAM Elapsed Cycles        cycle      2423808
    Average L1 Active Cycles         cycle     34283.20
    Total L1 Elapsed Cycles          cycle      5504440
    Average L2 Active Cycles         cycle     32437.28
    Total L2 Elapsed Cycles          cycle      1382940
    Average SM Active Cycles         cycle     34283.20
    Total SM Elapsed Cycles          cycle      5504440
    Average SMSP Active Cycles       cycle     34364.98
    Total SMSP Elapsed Cycles        cycle     22017760
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 12.57%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 15.77% above the average, while the minimum instance value is 30.68% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.8%                                                                                           
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 14.77% above the average, while the minimum instance value is 29.94% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.57%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 15.77% above the average, while the minimum instance value is 30.68% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.24
    SM Frequency            cycle/nsecond         2.19
    Elapsed Cycles                  cycle        42587
    Memory Throughput                   %        74.27
    DRAM Throughput                     %        74.27
    Duration                      usecond        19.39
    L1/TEX Cache Throughput             %        33.26
    L2 Cache Throughput                 %        32.49
    SM Active Cycles                cycle     34042.48
    Compute (SM) Throughput             %        18.63
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        36.68
    Achieved Active Warps Per SM           warp        17.61
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.64%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (36.7%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    147406.67
    Total DRAM Elapsed Cycles        cycle      2381824
    Average L1 Active Cycles         cycle     34042.48
    Total L1 Elapsed Cycles          cycle      5475228
    Average L2 Active Cycles         cycle     32773.17
    Total L2 Elapsed Cycles          cycle      1358100
    Average SM Active Cycles         cycle     34042.48
    Total SM Elapsed Cycles          cycle      5475228
    Average SMSP Active Cycles       cycle     34603.32
    Total SMSP Elapsed Cycles        cycle     21900912
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.64%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 14.62% above the average, while the minimum instance value is 28.40% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.66%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 15.65% above the average, while the minimum instance value is 32.66% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.64%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 14.62% above the average, while the minimum instance value is 28.40% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.21
    SM Frequency            cycle/nsecond         2.21
    Elapsed Cycles                  cycle        43859
    Memory Throughput                   %        72.99
    DRAM Throughput                     %        72.99
    Duration                      usecond        19.78
    L1/TEX Cache Throughput             %        32.60
    L2 Cache Throughput                 %        31.71
    SM Active Cycles                cycle     34712.09
    Compute (SM) Throughput             %        18.42
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        36.25
    Achieved Active Warps Per SM           warp        17.40
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.49%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (36.3%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    147405.33
    Total DRAM Elapsed Cycles        cycle      2423296
    Average L1 Active Cycles         cycle     34712.09
    Total L1 Elapsed Cycles          cycle      5539036
    Average L2 Active Cycles         cycle     32715.75
    Total L2 Elapsed Cycles          cycle      1391652
    Average SM Active Cycles         cycle     34712.09
    Total SM Elapsed Cycles          cycle      5539036
    Average SMSP Active Cycles       cycle     34359.30
    Total SMSP Elapsed Cycles        cycle     22156144
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 12.53%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 15.62% above the average, while the minimum instance value is 30.19% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.7%                                                                                           
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 14.73% above the average, while the minimum instance value is 30.25% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.53%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 15.62% above the average, while the minimum instance value is 30.19% below the      
          average.                                                                                                      

  MultKeyAccum_8(Params *, int, int, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long **) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.23
    SM Frequency            cycle/nsecond         2.23
    Elapsed Cycles                  cycle       179766
    Memory Throughput                   %        93.02
    DRAM Throughput                     %        93.02
    Duration                      usecond        80.51
    L1/TEX Cache Throughput             %        10.00
    L2 Cache Throughput                 %        38.87
    SM Active Cycles                cycle    172851.21
    Compute (SM) Throughput             %        10.13
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                2.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           12
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.34
    Achieved Active Warps Per SM           warp        42.40
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.66%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    766413.33
    Total DRAM Elapsed Cycles        cycle      9886720
    Average L1 Active Cycles         cycle    172851.21
    Total L1 Elapsed Cycles          cycle     22155938
    Average L2 Active Cycles         cycle    147298.25
    Total L2 Elapsed Cycles          cycle      5690628
    Average SM Active Cycles         cycle    172851.21
    Total SM Elapsed Cycles          cycle     22155938
    Average SMSP Active Cycles       cycle    166003.13
    Total SMSP Elapsed Cycles        cycle     88623752
    -------------------------- ----------- ------------

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.22
    SM Frequency            cycle/nsecond         2.19
    Elapsed Cycles                  cycle        43121
    Memory Throughput                   %        73.41
    DRAM Throughput                     %        73.41
    Duration                      usecond        19.65
    L1/TEX Cache Throughput             %        33.17
    L2 Cache Throughput                 %        32.10
    SM Active Cycles                cycle     34128.59
    Compute (SM) Throughput             %        18.35
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        37.53
    Achieved Active Warps Per SM           warp        18.02
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.93%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (37.5%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       147408
    Total DRAM Elapsed Cycles        cycle      2409472
    Average L1 Active Cycles         cycle     34128.59
    Total L1 Elapsed Cycles          cycle      5558220
    Average L2 Active Cycles         cycle     32548.69
    Total L2 Elapsed Cycles          cycle      1374516
    Average SM Active Cycles         cycle     34128.59
    Total SM Elapsed Cycles          cycle      5558220
    Average SMSP Active Cycles       cycle     34294.85
    Total SMSP Elapsed Cycles        cycle     22232880
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 12.08%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 15.37% above the average, while the minimum instance value is 32.27% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.14%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 15.38% above the average, while the minimum instance value is 31.05% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.08%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 15.37% above the average, while the minimum instance value is 32.27% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.21
    SM Frequency            cycle/nsecond         2.18
    Elapsed Cycles                  cycle        42600
    Memory Throughput                   %        74.30
    DRAM Throughput                     %        74.30
    Duration                      usecond        19.42
    L1/TEX Cache Throughput             %        33.33
    L2 Cache Throughput                 %        32.49
    SM Active Cycles                cycle     33980.67
    Compute (SM) Throughput             %        18.37
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        36.74
    Achieved Active Warps Per SM           warp        17.64
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.52%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (36.7%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    147406.67
    Total DRAM Elapsed Cycles        cycle      2380800
    Average L1 Active Cycles         cycle     33980.67
    Total L1 Elapsed Cycles          cycle      5552036
    Average L2 Active Cycles         cycle     32673.47
    Total L2 Elapsed Cycles          cycle      1357992
    Average SM Active Cycles         cycle     33980.67
    Total SM Elapsed Cycles          cycle      5552036
    Average SMSP Active Cycles       cycle     34557.22
    Total SMSP Elapsed Cycles        cycle     22208144
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.36%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 14.50% above the average, while the minimum instance value is 31.44% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.45%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 15.63% above the average, while the minimum instance value is 31.68% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.36%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 14.50% above the average, while the minimum instance value is 31.44% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.21
    SM Frequency            cycle/nsecond         2.19
    Elapsed Cycles                  cycle        43003
    Memory Throughput                   %        73.73
    DRAM Throughput                     %        73.73
    Duration                      usecond        19.58
    L1/TEX Cache Throughput             %        33.06
    L2 Cache Throughput                 %        32.23
    SM Active Cycles                cycle     34255.20
    Compute (SM) Throughput             %        18.38
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        37.06
    Achieved Active Warps Per SM           warp        17.79
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.88%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (37.1%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       147404
    Total DRAM Elapsed Cycles        cycle      2399232
    Average L1 Active Cycles         cycle     34255.20
    Total L1 Elapsed Cycles          cycle      5550960
    Average L2 Active Cycles         cycle     32673.47
    Total L2 Elapsed Cycles          cycle      1369188
    Average SM Active Cycles         cycle     34255.20
    Total SM Elapsed Cycles          cycle      5550960
    Average SMSP Active Cycles       cycle     34290.42
    Total SMSP Elapsed Cycles        cycle     22203840
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 12.04%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 15.25% above the average, while the minimum instance value is 30.64% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.35%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 15.62% above the average, while the minimum instance value is 29.40% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.04%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 15.25% above the average, while the minimum instance value is 30.64% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.22
    SM Frequency            cycle/nsecond         2.20
    Elapsed Cycles                  cycle        32365
    Memory Throughput                   %        65.69
    DRAM Throughput                     %        65.69
    Duration                      usecond        14.66
    L1/TEX Cache Throughput             %        28.25
    L2 Cache Throughput                 %        28.62
    SM Active Cycles                cycle     27013.50
    Compute (SM) Throughput             %        17.18
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        25.26
    Achieved Active Warps Per SM           warp        12.12
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 49.48%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (25.3%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     98382.67
    Total DRAM Elapsed Cycles        cycle      1797120
    Average L1 Active Cycles         cycle     27013.50
    Total L1 Elapsed Cycles          cycle      4073276
    Average L2 Active Cycles         cycle     22652.06
    Total L2 Elapsed Cycles          cycle      1027836
    Average SM Active Cycles         cycle     27013.50
    Total SM Elapsed Cycles          cycle      4073276
    Average SMSP Active Cycles       cycle     26636.73
    Total SMSP Elapsed Cycles        cycle     16293104
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.005%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 7.07% above the average, while the minimum instance value is 15.51% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.395%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 6.45% above the average, while the minimum instance value is 23.23% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.005%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 7.07% above the average, while the minimum instance value is 15.51% below the       
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.20
    SM Frequency            cycle/nsecond         2.23
    Elapsed Cycles                  cycle        22803
    Memory Throughput                   %        47.26
    DRAM Throughput                     %        47.26
    Duration                      usecond        10.24
    L1/TEX Cache Throughput             %        34.00
    L2 Cache Throughput                 %        20.52
    SM Active Cycles                cycle     11829.72
    Compute (SM) Throughput             %        13.72
    ----------------------- ------------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        26.60
    Achieved Active Warps Per SM           warp        12.77
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 46.8%                                                                                     
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (26.6%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     49357.33
    Total DRAM Elapsed Cycles        cycle      1253376
    Average L1 Active Cycles         cycle     11829.72
    Total L1 Elapsed Cycles          cycle      2923188
    Average L2 Active Cycles         cycle     14304.03
    Total L2 Elapsed Cycles          cycle       721692
    Average SM Active Cycles         cycle     11829.72
    Total SM Elapsed Cycles          cycle      2923188
    Average SMSP Active Cycles       cycle     11221.64
    Total SMSP Elapsed Cycles        cycle     11692752
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 21.24%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 41.00% above the average, while the minimum instance value is 68.98% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 21.33%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 43.41% above the average, while the minimum instance value is 71.74% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 21.24%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 41.00% above the average, while the minimum instance value is 68.98% below the      
          average.                                                                                                      

  MultKeyAccum_8(Params *, int, int, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long **) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.24
    SM Frequency            cycle/nsecond         2.23
    Elapsed Cycles                  cycle       180483
    Memory Throughput                   %        92.44
    DRAM Throughput                     %        92.44
    Duration                      usecond        80.80
    L1/TEX Cache Throughput             %         9.91
    L2 Cache Throughput                 %        38.72
    SM Active Cycles                cycle    174528.77
    Compute (SM) Throughput             %        10.17
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                2.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           12
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        87.13
    Achieved Active Warps Per SM           warp        41.82
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 12.87%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (87.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    764701.33
    Total DRAM Elapsed Cycles        cycle      9926656
    Average L1 Active Cycles         cycle    174528.77
    Total L1 Elapsed Cycles          cycle     22066886
    Average L2 Active Cycles         cycle    146507.22
    Total L2 Elapsed Cycles          cycle      5712372
    Average SM Active Cycles         cycle    174528.77
    Total SM Elapsed Cycles          cycle     22066886
    Average SMSP Active Cycles       cycle    165463.80
    Total SMSP Elapsed Cycles        cycle     88267544
    -------------------------- ----------- ------------

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.20
    SM Frequency            cycle/nsecond         2.18
    Elapsed Cycles                  cycle        42977
    Memory Throughput                   %        73.83
    DRAM Throughput                     %        73.83
    Duration                      usecond        19.58
    L1/TEX Cache Throughput             %        33.47
    L2 Cache Throughput                 %        32.23
    SM Active Cycles                cycle     33826.95
    Compute (SM) Throughput             %        18.44
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        37.23
    Achieved Active Warps Per SM           warp        17.87
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.54%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (37.2%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    147417.33
    Total DRAM Elapsed Cycles        cycle      2396160
    Average L1 Active Cycles         cycle     33826.95
    Total L1 Elapsed Cycles          cycle      5531550
    Average L2 Active Cycles         cycle     32430.92
    Total L2 Elapsed Cycles          cycle      1368900
    Average SM Active Cycles         cycle     33826.95
    Total SM Elapsed Cycles          cycle      5531550
    Average SMSP Active Cycles       cycle     34171.35
    Total SMSP Elapsed Cycles        cycle     22126200
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.85%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 15.14% above the average, while the minimum instance value is 28.58% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.87%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 16.27% above the average, while the minimum instance value is 31.50% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.85%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 15.14% above the average, while the minimum instance value is 28.58% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.23
    SM Frequency            cycle/nsecond         2.20
    Elapsed Cycles                  cycle        43167
    Memory Throughput                   %        73.73
    DRAM Throughput                     %        73.73
    Duration                      usecond        19.55
    L1/TEX Cache Throughput             %        33.02
    L2 Cache Throughput                 %        32.16
    SM Active Cycles                cycle     34281.73
    Compute (SM) Throughput             %        18.40
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        36.64
    Achieved Active Warps Per SM           warp        17.59
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.72%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (36.6%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       147408
    Total DRAM Elapsed Cycles        cycle      2399232
    Average L1 Active Cycles         cycle     34281.73
    Total L1 Elapsed Cycles          cycle      5543754
    Average L2 Active Cycles         cycle     32504.86
    Total L2 Elapsed Cycles          cycle      1371888
    Average SM Active Cycles         cycle     34281.73
    Total SM Elapsed Cycles          cycle      5543754
    Average SMSP Active Cycles       cycle     34127.34
    Total SMSP Elapsed Cycles        cycle     22175016
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 12.07%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 15.24% above the average, while the minimum instance value is 31.02% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.93%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 15.14% above the average, while the minimum instance value is 30.77% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.07%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 15.24% above the average, while the minimum instance value is 31.02% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.19
    SM Frequency            cycle/nsecond         2.22
    Elapsed Cycles                  cycle        22839
    Memory Throughput                   %        47.14
    DRAM Throughput                     %        47.14
    Duration                      usecond        10.27
    L1/TEX Cache Throughput             %        34.31
    L2 Cache Throughput                 %        20.35
    SM Active Cycles                cycle     11738.11
    Compute (SM) Throughput             %        13.70
    ----------------------- ------------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        26.50
    Achieved Active Warps Per SM           warp        12.72
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 47%                                                                                       
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (26.5%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        49356
    Total DRAM Elapsed Cycles        cycle      1256448
    Average L1 Active Cycles         cycle     11738.11
    Total L1 Elapsed Cycles          cycle      2926614
    Average L2 Active Cycles         cycle     14386.03
    Total L2 Elapsed Cycles          cycle       722664
    Average SM Active Cycles         cycle     11738.11
    Total SM Elapsed Cycles          cycle      2926614
    Average SMSP Active Cycles       cycle     11315.82
    Total SMSP Elapsed Cycles        cycle     11706456
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 21.21%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 41.32% above the average, while the minimum instance value is 66.17% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 21.05%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 42.54% above the average, while the minimum instance value is 70.55% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 21.21%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 41.32% above the average, while the minimum instance value is 66.17% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.22
    SM Frequency            cycle/nsecond         2.21
    Elapsed Cycles                  cycle        31890
    Memory Throughput                   %        67.00
    DRAM Throughput                     %        67.00
    Duration                      usecond        14.37
    L1/TEX Cache Throughput             %        28.45
    L2 Cache Throughput                 %        29.08
    SM Active Cycles                cycle     26817.15
    Compute (SM) Throughput             %        17.09
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        25.16
    Achieved Active Warps Per SM           warp        12.08
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 49.69%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (25.2%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     98389.33
    Total DRAM Elapsed Cycles        cycle      1762304
    Average L1 Active Cycles         cycle     26817.15
    Total L1 Elapsed Cycles          cycle      4095976
    Average L2 Active Cycles         cycle     22722.25
    Total L2 Elapsed Cycles          cycle      1011636
    Average SM Active Cycles         cycle     26817.15
    Total SM Elapsed Cycles          cycle      4095976
    Average SMSP Active Cycles       cycle     26719.79
    Total SMSP Elapsed Cycles        cycle     16383904
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.633%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 7.91% above the average, while the minimum instance value is 25.38% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.066%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 7.26% above the average, while the minimum instance value is 21.96% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.633%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 7.91% above the average, while the minimum instance value is 25.38% below the       
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.21
    SM Frequency            cycle/nsecond         2.18
    Elapsed Cycles                  cycle        42834
    Memory Throughput                   %        73.98
    DRAM Throughput                     %        73.98
    Duration                      usecond        19.52
    L1/TEX Cache Throughput             %        33.28
    L2 Cache Throughput                 %        32.34
    SM Active Cycles                cycle     34034.25
    Compute (SM) Throughput             %        18.44
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        37.44
    Achieved Active Warps Per SM           warp        17.97
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.12%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (37.4%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    147406.67
    Total DRAM Elapsed Cycles        cycle      2391040
    Average L1 Active Cycles         cycle     34034.25
    Total L1 Elapsed Cycles          cycle      5533312
    Average L2 Active Cycles         cycle     32739.28
    Total L2 Elapsed Cycles          cycle      1364292
    Average SM Active Cycles         cycle     34034.25
    Total SM Elapsed Cycles          cycle      5533312
    Average SMSP Active Cycles       cycle     34371.46
    Total SMSP Elapsed Cycles        cycle     22133248
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.78%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 14.97% above the average, while the minimum instance value is 31.81% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 13.14%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 16.53% above the average, while the minimum instance value is 33.15% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.78%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 14.97% above the average, while the minimum instance value is 31.81% below the      
          average.                                                                                                      

  MultKeyAccum_8(Params *, int, int, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long **) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.23
    SM Frequency            cycle/nsecond         2.23
    Elapsed Cycles                  cycle       180730
    Memory Throughput                   %        92.31
    DRAM Throughput                     %        92.31
    Duration                      usecond        80.99
    L1/TEX Cache Throughput             %         9.92
    L2 Cache Throughput                 %        38.66
    SM Active Cycles                cycle    174252.49
    Compute (SM) Throughput             %        10.18
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                2.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           12
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        87.15
    Achieved Active Warps Per SM           warp        41.83
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 12.85%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (87.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       765104
    Total DRAM Elapsed Cycles        cycle      9946112
    Average L1 Active Cycles         cycle    174252.49
    Total L1 Elapsed Cycles          cycle     22059506
    Average L2 Active Cycles         cycle    146769.94
    Total L2 Elapsed Cycles          cycle      5721696
    Average SM Active Cycles         cycle    174252.49
    Total SM Elapsed Cycles          cycle     22059506
    Average SMSP Active Cycles       cycle    165667.53
    Total SMSP Elapsed Cycles        cycle     88238024
    -------------------------- ----------- ------------

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.20
    SM Frequency            cycle/nsecond         2.20
    Elapsed Cycles                  cycle        32314
    Memory Throughput                   %        65.81
    DRAM Throughput                     %        65.81
    Duration                      usecond        14.66
    L1/TEX Cache Throughput             %        28.32
    L2 Cache Throughput                 %        28.65
    SM Active Cycles                cycle     26942.91
    Compute (SM) Throughput             %        16.54
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        25.14
    Achieved Active Warps Per SM           warp        12.07
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 49.71%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (25.1%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     98381.33
    Total DRAM Elapsed Cycles        cycle      1794048
    Average L1 Active Cycles         cycle     26942.91
    Total L1 Elapsed Cycles          cycle      4232116
    Average L2 Active Cycles         cycle     22770.03
    Total L2 Elapsed Cycles          cycle      1026720
    Average SM Active Cycles         cycle     26942.91
    Total SM Elapsed Cycles          cycle      4232116
    Average SMSP Active Cycles       cycle     26770.37
    Total SMSP Elapsed Cycles        cycle     16928464
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.765%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 8.30% above the average, while the minimum instance value is 22.52% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.452%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 6.73% above the average, while the minimum instance value is 21.31% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.765%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 8.30% above the average, while the minimum instance value is 22.52% below the       
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.18
    SM Frequency            cycle/nsecond         2.20
    Elapsed Cycles                  cycle        23092
    Memory Throughput                   %        46.35
    DRAM Throughput                     %        46.35
    Duration                      usecond        10.46
    L1/TEX Cache Throughput             %        34.15
    L2 Cache Throughput                 %        20.06
    SM Active Cycles                cycle     11780.05
    Compute (SM) Throughput             %        13.80
    ----------------------- ------------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        27.02
    Achieved Active Warps Per SM           warp        12.97
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 45.95%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (27.0%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        49356
    Total DRAM Elapsed Cycles        cycle      1277952
    Average L1 Active Cycles         cycle     11780.05
    Total L1 Elapsed Cycles          cycle      2905188
    Average L2 Active Cycles         cycle        14340
    Total L2 Elapsed Cycles          cycle       733140
    Average SM Active Cycles         cycle     11780.05
    Total SM Elapsed Cycles          cycle      2905188
    Average SMSP Active Cycles       cycle     11284.22
    Total SMSP Elapsed Cycles        cycle     11620752
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 21.58%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 41.57% above the average, while the minimum instance value is 68.64% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 21.64%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 43.53% above the average, while the minimum instance value is 72.25% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 21.58%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 41.57% above the average, while the minimum instance value is 68.64% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.23
    SM Frequency            cycle/nsecond         2.22
    Elapsed Cycles                  cycle        43425
    Memory Throughput                   %        73.86
    DRAM Throughput                     %        73.86
    Duration                      usecond        19.52
    L1/TEX Cache Throughput             %        32.84
    L2 Cache Throughput                 %        32.07
    SM Active Cycles                cycle     34488.02
    Compute (SM) Throughput             %        18.25
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        36.54
    Achieved Active Warps Per SM           warp        17.54
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.92%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (36.5%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    147417.33
    Total DRAM Elapsed Cycles        cycle      2395136
    Average L1 Active Cycles         cycle     34488.02
    Total L1 Elapsed Cycles          cycle      5589850
    Average L2 Active Cycles         cycle     32180.56
    Total L2 Elapsed Cycles          cycle      1376028
    Average SM Active Cycles         cycle     34488.02
    Total SM Elapsed Cycles          cycle      5589850
    Average SMSP Active Cycles       cycle     34016.31
    Total SMSP Elapsed Cycles        cycle     22359400
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.94%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 15.13% above the average, while the minimum instance value is 26.34% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.07%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 14.21% above the average, while the minimum instance value is 34.28% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.94%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 15.13% above the average, while the minimum instance value is 26.34% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.22
    SM Frequency            cycle/nsecond         2.18
    Elapsed Cycles                  cycle        42993
    Memory Throughput                   %        73.63
    DRAM Throughput                     %        73.63
    Duration                      usecond        19.58
    L1/TEX Cache Throughput             %        33.29
    L2 Cache Throughput                 %        32.21
    SM Active Cycles                cycle     34016.30
    Compute (SM) Throughput             %        18.48
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        37.17
    Achieved Active Warps Per SM           warp        17.84
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.67%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (37.2%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    147406.67
    Total DRAM Elapsed Cycles        cycle      2402304
    Average L1 Active Cycles         cycle     34016.30
    Total L1 Elapsed Cycles          cycle      5520884
    Average L2 Active Cycles         cycle     32551.78
    Total L2 Elapsed Cycles          cycle      1369836
    Average SM Active Cycles         cycle     34016.30
    Total SM Elapsed Cycles          cycle      5520884
    Average SMSP Active Cycles       cycle     34203.84
    Total SMSP Elapsed Cycles        cycle     22083536
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 12.16%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 15.42% above the average, while the minimum instance value is 30.31% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 13.41%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 16.92% above the average, while the minimum instance value is 32.68% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.16%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 15.42% above the average, while the minimum instance value is 30.31% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.21
    SM Frequency            cycle/nsecond         2.18
    Elapsed Cycles                  cycle        42883
    Memory Throughput                   %        73.83
    DRAM Throughput                     %        73.83
    Duration                      usecond        19.55
    L1/TEX Cache Throughput             %        33.38
    L2 Cache Throughput                 %        32.30
    SM Active Cycles                cycle     33905.88
    Compute (SM) Throughput             %        17.94
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        36.74
    Achieved Active Warps Per SM           warp        17.64
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.52%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (36.7%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    147417.33
    Total DRAM Elapsed Cycles        cycle      2396160
    Average L1 Active Cycles         cycle     33905.88
    Total L1 Elapsed Cycles          cycle      5685026
    Average L2 Active Cycles         cycle     32623.31
    Total L2 Elapsed Cycles          cycle      1366128
    Average SM Active Cycles         cycle     33905.88
    Total SM Elapsed Cycles          cycle      5685026
    Average SMSP Active Cycles       cycle     34088.95
    Total SMSP Elapsed Cycles        cycle     22740104
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.85%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 15.52% above the average, while the minimum instance value is 27.08% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.71%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 16.56% above the average, while the minimum instance value is 30.16% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.85%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 15.52% above the average, while the minimum instance value is 27.08% below the      
          average.                                                                                                      

  MultKeyAccum_8(Params *, int, int, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long **) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.24
    SM Frequency            cycle/nsecond         2.23
    Elapsed Cycles                  cycle       181124
    Memory Throughput                   %        92.18
    DRAM Throughput                     %        92.18
    Duration                      usecond        81.09
    L1/TEX Cache Throughput             %         9.84
    L2 Cache Throughput                 %        38.58
    SM Active Cycles                cycle    175674.83
    Compute (SM) Throughput             %        10.16
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                2.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           12
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        86.68
    Achieved Active Warps Per SM           warp        41.60
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 13.32%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (86.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    765074.67
    Total DRAM Elapsed Cycles        cycle      9959424
    Average L1 Active Cycles         cycle    175674.83
    Total L1 Elapsed Cycles          cycle     22082572
    Average L2 Active Cycles         cycle    146645.36
    Total L2 Elapsed Cycles          cycle      5733324
    Average SM Active Cycles         cycle    175674.83
    Total SM Elapsed Cycles          cycle     22082572
    Average SMSP Active Cycles       cycle    165717.79
    Total SMSP Elapsed Cycles        cycle     88330288
    -------------------------- ----------- ------------

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.22
    SM Frequency            cycle/nsecond         2.21
    Elapsed Cycles                  cycle        33184
    Memory Throughput                   %        64.30
    DRAM Throughput                     %        64.30
    Duration                      usecond        14.98
    L1/TEX Cache Throughput             %        27.94
    L2 Cache Throughput                 %        27.94
    SM Active Cycles                cycle     27307.60
    Compute (SM) Throughput             %        16.78
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        24.89
    Achieved Active Warps Per SM           warp        11.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50.22%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (24.9%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     98381.33
    Total DRAM Elapsed Cycles        cycle      1836032
    Average L1 Active Cycles         cycle     27307.60
    Total L1 Elapsed Cycles          cycle      4172234
    Average L2 Active Cycles         cycle     22963.83
    Total L2 Elapsed Cycles          cycle      1052676
    Average SM Active Cycles         cycle     27307.60
    Total SM Elapsed Cycles          cycle      4172234
    Average SMSP Active Cycles       cycle     27188.55
    Total SMSP Elapsed Cycles        cycle     16688936
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.737%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 10.43% above the average, while the minimum instance value is 24.36% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.877%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 9.44% above the average, while the minimum instance value is 17.62% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.737%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 10.43% above the average, while the minimum instance value is 24.36% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.20
    SM Frequency            cycle/nsecond         2.21
    Elapsed Cycles                  cycle        43776
    Memory Throughput                   %        73.17
    DRAM Throughput                     %        73.17
    Duration                      usecond        19.74
    L1/TEX Cache Throughput             %        32.63
    L2 Cache Throughput                 %        31.79
    SM Active Cycles                cycle     34684.15
    Compute (SM) Throughput             %        18.65
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        36.32
    Achieved Active Warps Per SM           warp        17.43
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.36%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (36.3%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       147416
    Total DRAM Elapsed Cycles        cycle      2417664
    Average L1 Active Cycles         cycle     34684.15
    Total L1 Elapsed Cycles          cycle      5471146
    Average L2 Active Cycles         cycle     32432.31
    Total L2 Elapsed Cycles          cycle      1388052
    Average SM Active Cycles         cycle     34684.15
    Total SM Elapsed Cycles          cycle      5471146
    Average SMSP Active Cycles       cycle     34456.64
    Total SMSP Elapsed Cycles        cycle     21884584
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 12.45%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 15.34% above the average, while the minimum instance value is 31.07% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.85%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 14.70% above the average, while the minimum instance value is 31.10% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.45%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 15.34% above the average, while the minimum instance value is 31.07% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.21
    SM Frequency            cycle/nsecond         2.18
    Elapsed Cycles                  cycle        43401
    Memory Throughput                   %        72.98
    DRAM Throughput                     %        72.98
    Duration                      usecond        19.78
    L1/TEX Cache Throughput             %        32.89
    L2 Cache Throughput                 %        31.91
    SM Active Cycles                cycle     34409.74
    Compute (SM) Throughput             %        18.74
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        36.74
    Achieved Active Warps Per SM           warp        17.64
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.51%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (36.7%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       147408
    Total DRAM Elapsed Cycles        cycle      2423808
    Average L1 Active Cycles         cycle     34409.74
    Total L1 Elapsed Cycles          cycle      5444294
    Average L2 Active Cycles         cycle     32362.39
    Total L2 Elapsed Cycles          cycle      1382724
    Average SM Active Cycles         cycle     34409.74
    Total SM Elapsed Cycles          cycle      5444294
    Average SMSP Active Cycles       cycle     34309.53
    Total SMSP Elapsed Cycles        cycle     21777176
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.88%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 14.69% above the average, while the minimum instance value is 27.81% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.93%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 14.79% above the average, while the minimum instance value is 31.18% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.88%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 14.69% above the average, while the minimum instance value is 27.81% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.21
    SM Frequency            cycle/nsecond         2.18
    Elapsed Cycles                  cycle        43319
    Memory Throughput                   %        73.13
    DRAM Throughput                     %        73.13
    Duration                      usecond        19.74
    L1/TEX Cache Throughput             %        33.07
    L2 Cache Throughput                 %        31.95
    SM Active Cycles                cycle     34234.29
    Compute (SM) Throughput             %        18.54
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        36.30
    Achieved Active Warps Per SM           warp        17.43
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.39%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (36.3%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    147406.67
    Total DRAM Elapsed Cycles        cycle      2418688
    Average L1 Active Cycles         cycle     34234.29
    Total L1 Elapsed Cycles          cycle      5501486
    Average L2 Active Cycles         cycle     32281.42
    Total L2 Elapsed Cycles          cycle      1381068
    Average SM Active Cycles         cycle     34234.29
    Total SM Elapsed Cycles          cycle      5501486
    Average SMSP Active Cycles       cycle     34068.48
    Total SMSP Elapsed Cycles        cycle     22005944
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 12.56%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 15.77% above the average, while the minimum instance value is 32.23% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.18%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 14.11% above the average, while the minimum instance value is 32.09% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.56%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 15.77% above the average, while the minimum instance value is 32.23% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.21
    SM Frequency            cycle/nsecond         2.23
    Elapsed Cycles                  cycle        43274
    Memory Throughput                   %        74.33
    DRAM Throughput                     %        74.33
    Duration                      usecond        19.42
    L1/TEX Cache Throughput             %        32.41
    L2 Cache Throughput                 %        32.21
    SM Active Cycles                cycle     34911.01
    Compute (SM) Throughput             %        18.15
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        36.21
    Achieved Active Warps Per SM           warp        17.38
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.59%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (36.2%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    147405.33
    Total DRAM Elapsed Cycles        cycle      2379776
    Average L1 Active Cycles         cycle     34911.01
    Total L1 Elapsed Cycles          cycle      5620504
    Average L2 Active Cycles         cycle     32301.78
    Total L2 Elapsed Cycles          cycle      1369980
    Average SM Active Cycles         cycle     34911.01
    Total SM Elapsed Cycles          cycle      5620504
    Average SMSP Active Cycles       cycle     34391.34
    Total SMSP Elapsed Cycles        cycle     22482016
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.05%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 13.90% above the average, while the minimum instance value is 32.83% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 10.87%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 13.88% above the average, while the minimum instance value is 28.27% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.05%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 13.90% above the average, while the minimum instance value is 32.83% below the      
          average.                                                                                                      

  MultKeyAccum_8(Params *, int, int, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long **) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.23
    SM Frequency            cycle/nsecond         2.23
    Elapsed Cycles                  cycle       180056
    Memory Throughput                   %        92.82
    DRAM Throughput                     %        92.82
    Duration                      usecond        80.64
    L1/TEX Cache Throughput             %         9.97
    L2 Cache Throughput                 %        38.80
    SM Active Cycles                cycle    173482.73
    Compute (SM) Throughput             %        10.16
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                2.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           12
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.02
    Achieved Active Warps Per SM           warp        42.25
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.98%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       766124
    Total DRAM Elapsed Cycles        cycle      9904128
    Average L1 Active Cycles         cycle    173482.73
    Total L1 Elapsed Cycles          cycle     22091254
    Average L2 Active Cycles         cycle    146538.89
    Total L2 Elapsed Cycles          cycle      5700024
    Average SM Active Cycles         cycle    173482.73
    Total SM Elapsed Cycles          cycle     22091254
    Average SMSP Active Cycles       cycle    165262.91
    Total SMSP Elapsed Cycles        cycle     88365016
    -------------------------- ----------- ------------

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.21
    SM Frequency            cycle/nsecond         2.22
    Elapsed Cycles                  cycle        43901
    Memory Throughput                   %        73.10
    DRAM Throughput                     %        73.10
    Duration                      usecond        19.74
    L1/TEX Cache Throughput             %        32.56
    L2 Cache Throughput                 %        31.71
    SM Active Cycles                cycle     34768.24
    Compute (SM) Throughput             %        18.31
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        36.51
    Achieved Active Warps Per SM           warp        17.52
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.98%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (36.5%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    147406.67
    Total DRAM Elapsed Cycles        cycle      2419712
    Average L1 Active Cycles         cycle     34768.24
    Total L1 Elapsed Cycles          cycle      5572286
    Average L2 Active Cycles         cycle     32518.64
    Total L2 Elapsed Cycles          cycle      1391364
    Average SM Active Cycles         cycle     34768.24
    Total SM Elapsed Cycles          cycle      5572286
    Average SMSP Active Cycles       cycle     34345.40
    Total SMSP Elapsed Cycles        cycle     22289144
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 12.56%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 15.73% above the average, while the minimum instance value is 30.95% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.74%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 14.88% above the average, while the minimum instance value is 30.00% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.56%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 15.73% above the average, while the minimum instance value is 30.95% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.22
    SM Frequency            cycle/nsecond         2.18
    Elapsed Cycles                  cycle        42952
    Memory Throughput                   %        73.66
    DRAM Throughput                     %        73.66
    Duration                      usecond        19.58
    L1/TEX Cache Throughput             %        33.33
    L2 Cache Throughput                 %        32.22
    SM Active Cycles                cycle     33959.04
    Compute (SM) Throughput             %        18.56
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        37.57
    Achieved Active Warps Per SM           warp        18.03
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.86%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (37.6%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       147408
    Total DRAM Elapsed Cycles        cycle      2401280
    Average L1 Active Cycles         cycle     33959.04
    Total L1 Elapsed Cycles          cycle      5496386
    Average L2 Active Cycles         cycle     32432.97
    Total L2 Elapsed Cycles          cycle      1369620
    Average SM Active Cycles         cycle     33959.04
    Total SM Elapsed Cycles          cycle      5496386
    Average SMSP Active Cycles       cycle     34349.41
    Total SMSP Elapsed Cycles        cycle     21985544
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 12.09%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 15.29% above the average, while the minimum instance value is 31.77% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.51%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 14.39% above the average, while the minimum instance value is 31.54% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.09%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 15.29% above the average, while the minimum instance value is 31.77% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.22
    SM Frequency            cycle/nsecond         2.19
    Elapsed Cycles                  cycle        43483
    Memory Throughput                   %        72.83
    DRAM Throughput                     %        72.83
    Duration                      usecond        19.81
    L1/TEX Cache Throughput             %        33.00
    L2 Cache Throughput                 %        31.84
    SM Active Cycles                cycle     34313.91
    Compute (SM) Throughput             %        18.38
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        37.31
    Achieved Active Warps Per SM           warp        17.91
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.38%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (37.3%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    147414.67
    Total DRAM Elapsed Cycles        cycle      2428928
    Average L1 Active Cycles         cycle     34313.91
    Total L1 Elapsed Cycles          cycle      5549128
    Average L2 Active Cycles         cycle     32345.25
    Total L2 Elapsed Cycles          cycle      1385964
    Average SM Active Cycles         cycle     34313.91
    Total SM Elapsed Cycles          cycle      5549128
    Average SMSP Active Cycles       cycle     34351.86
    Total SMSP Elapsed Cycles        cycle     22196512
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 12.38%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 15.65% above the average, while the minimum instance value is 29.18% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.06%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 15.22% above the average, while the minimum instance value is 30.96% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.38%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 15.65% above the average, while the minimum instance value is 29.18% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.20
    SM Frequency            cycle/nsecond         2.19
    Elapsed Cycles                  cycle        33070
    Memory Throughput                   %        64.26
    DRAM Throughput                     %        64.26
    Duration                      usecond        15.01
    L1/TEX Cache Throughput             %        28.05
    L2 Cache Throughput                 %        27.98
    SM Active Cycles                cycle     27205.91
    Compute (SM) Throughput             %        16.85
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        24.99
    Achieved Active Warps Per SM           warp        12.00
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50.02%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (25.0%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     98381.33
    Total DRAM Elapsed Cycles        cycle      1837056
    Average L1 Active Cycles         cycle     27205.91
    Total L1 Elapsed Cycles          cycle      4153240
    Average L2 Active Cycles         cycle     22827.92
    Total L2 Elapsed Cycles          cycle      1051524
    Average SM Active Cycles         cycle     27205.91
    Total SM Elapsed Cycles          cycle      4153240
    Average SMSP Active Cycles       cycle     27011.77
    Total SMSP Elapsed Cycles        cycle     16612960
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.072%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 9.63% above the average, while the minimum instance value is 17.41% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.072%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 9.63% above the average, while the minimum instance value is 17.41% below the       
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.18
    SM Frequency            cycle/nsecond         2.22
    Elapsed Cycles                  cycle        22771
    Memory Throughput                   %        47.33
    DRAM Throughput                     %        47.33
    Duration                      usecond        10.24
    L1/TEX Cache Throughput             %        33.80
    L2 Cache Throughput                 %        20.42
    SM Active Cycles                cycle     11907.91
    Compute (SM) Throughput             %        13.97
    ----------------------- ------------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        25.38
    Achieved Active Warps Per SM           warp        12.18
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 49.24%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (25.4%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     49358.67
    Total DRAM Elapsed Cycles        cycle      1251328
    Average L1 Active Cycles         cycle     11907.91
    Total L1 Elapsed Cycles          cycle      2869226
    Average L2 Active Cycles         cycle     14358.53
    Total L2 Elapsed Cycles          cycle       720468
    Average SM Active Cycles         cycle     11907.91
    Total SM Elapsed Cycles          cycle      2869226
    Average SMSP Active Cycles       cycle     11474.77
    Total SMSP Elapsed Cycles        cycle     11476904
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 21.63%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 40.72% above the average, while the minimum instance value is 69.10% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 22.14%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 43.24% above the average, while the minimum instance value is 70.60% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 21.63%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 40.72% above the average, while the minimum instance value is 69.10% below the      
          average.                                                                                                      

  MultKeyAccum_8(Params *, int, int, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long **) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.23
    SM Frequency            cycle/nsecond         2.23
    Elapsed Cycles                  cycle       180773
    Memory Throughput                   %        92.30
    DRAM Throughput                     %        92.30
    Duration                      usecond        80.93
    L1/TEX Cache Throughput             %         9.88
    L2 Cache Throughput                 %        38.65
    SM Active Cycles                cycle    175116.70
    Compute (SM) Throughput             %        10.15
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                2.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           12
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        86.93
    Achieved Active Warps Per SM           warp        41.73
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 13.07%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (86.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    764465.33
    Total DRAM Elapsed Cycles        cycle      9938944
    Average L1 Active Cycles         cycle    175116.70
    Total L1 Elapsed Cycles          cycle     22115168
    Average L2 Active Cycles         cycle    146997.42
    Total L2 Elapsed Cycles          cycle      5722200
    Average SM Active Cycles         cycle    175116.70
    Total SM Elapsed Cycles          cycle     22115168
    Average SMSP Active Cycles       cycle    166144.29
    Total SMSP Elapsed Cycles        cycle     88460672
    -------------------------- ----------- ------------

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.22
    SM Frequency            cycle/nsecond         2.19
    Elapsed Cycles                  cycle        43867
    Memory Throughput                   %        72.22
    DRAM Throughput                     %        72.22
    Duration                      usecond        19.97
    L1/TEX Cache Throughput             %        33.18
    L2 Cache Throughput                 %        31.57
    SM Active Cycles                cycle     34123.06
    Compute (SM) Throughput             %        18.15
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        36.90
    Achieved Active Warps Per SM           warp        17.71
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.2%                                                                                     
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (36.9%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    147406.67
    Total DRAM Elapsed Cycles        cycle      2449408
    Average L1 Active Cycles         cycle     34123.06
    Total L1 Elapsed Cycles          cycle      5622224
    Average L2 Active Cycles         cycle     32106.69
    Total L2 Elapsed Cycles          cycle      1397700
    Average SM Active Cycles         cycle     34123.06
    Total SM Elapsed Cycles          cycle      5622224
    Average SMSP Active Cycles       cycle     33783.42
    Total SMSP Elapsed Cycles        cycle     22488896
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 12.68%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 16.32% above the average, while the minimum instance value is 31.79% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.32%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 14.72% above the average, while the minimum instance value is 30.44% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.68%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 16.32% above the average, while the minimum instance value is 31.79% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.22
    SM Frequency            cycle/nsecond         2.18
    Elapsed Cycles                  cycle        42336
    Memory Throughput                   %        74.75
    DRAM Throughput                     %        74.75
    Duration                      usecond        19.30
    L1/TEX Cache Throughput             %        33.40
    L2 Cache Throughput                 %        32.70
    SM Active Cycles                cycle     33903.16
    Compute (SM) Throughput             %        18.64
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        36.79
    Achieved Active Warps Per SM           warp        17.66
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.42%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (36.8%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    147405.33
    Total DRAM Elapsed Cycles        cycle      2366464
    Average L1 Active Cycles         cycle     33903.16
    Total L1 Elapsed Cycles          cycle      5471722
    Average L2 Active Cycles         cycle     32275.42
    Total L2 Elapsed Cycles          cycle      1349424
    Average SM Active Cycles         cycle     33903.16
    Total SM Elapsed Cycles          cycle      5471722
    Average SMSP Active Cycles       cycle     34161.53
    Total SMSP Elapsed Cycles        cycle     21886888
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.25%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 14.18% above the average, while the minimum instance value is 29.53% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.34%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 14.19% above the average, while the minimum instance value is 27.26% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.25%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 14.18% above the average, while the minimum instance value is 29.53% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.20
    SM Frequency            cycle/nsecond         2.21
    Elapsed Cycles                  cycle        22903
    Memory Throughput                   %        46.80
    DRAM Throughput                     %        46.80
    Duration                      usecond        10.34
    L1/TEX Cache Throughput             %        34.38
    L2 Cache Throughput                 %        20.24
    SM Active Cycles                cycle     11715.93
    Compute (SM) Throughput             %        13.88
    ----------------------- ------------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        25.80
    Achieved Active Warps Per SM           warp        12.38
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 48.4%                                                                                     
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (25.8%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     49358.67
    Total DRAM Elapsed Cycles        cycle      1265664
    Average L1 Active Cycles         cycle     11715.93
    Total L1 Elapsed Cycles          cycle      2888686
    Average L2 Active Cycles         cycle     14262.25
    Total L2 Elapsed Cycles          cycle       726732
    Average SM Active Cycles         cycle     11715.93
    Total SM Elapsed Cycles          cycle      2888686
    Average SMSP Active Cycles       cycle     11321.72
    Total SMSP Elapsed Cycles        cycle     11554744
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 21.81%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 42.01% above the average, while the minimum instance value is 66.46% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 21.59%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 43.03% above the average, while the minimum instance value is 71.06% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 21.81%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 42.01% above the average, while the minimum instance value is 66.46% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.21
    SM Frequency            cycle/nsecond         2.22
    Elapsed Cycles                  cycle        32219
    Memory Throughput                   %        66.49
    DRAM Throughput                     %        66.49
    Duration                      usecond        14.50
    L1/TEX Cache Throughput             %        27.96
    L2 Cache Throughput                 %        28.82
    SM Active Cycles                cycle     27289.17
    Compute (SM) Throughput             %        16.64
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        24.55
    Achieved Active Warps Per SM           warp        11.79
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50.89%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (24.6%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     98381.33
    Total DRAM Elapsed Cycles        cycle      1775616
    Average L1 Active Cycles         cycle     27289.17
    Total L1 Elapsed Cycles          cycle      4206730
    Average L2 Active Cycles         cycle     22670.42
    Total L2 Elapsed Cycles          cycle      1020780
    Average SM Active Cycles         cycle     27289.17
    Total SM Elapsed Cycles          cycle      4206730
    Average SMSP Active Cycles       cycle     26536.92
    Total SMSP Elapsed Cycles        cycle     16826920
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.366%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 7.67% above the average, while the minimum instance value is 21.26% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.461%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 8.00% above the average, while the minimum instance value is 27.33% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.366%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 7.67% above the average, while the minimum instance value is 21.26% below the       
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.21
    SM Frequency            cycle/nsecond         2.20
    Elapsed Cycles                  cycle        42785
    Memory Throughput                   %        74.43
    DRAM Throughput                     %        74.43
    Duration                      usecond        19.39
    L1/TEX Cache Throughput             %        33.14
    L2 Cache Throughput                 %        32.42
    SM Active Cycles                cycle     34192.64
    Compute (SM) Throughput             %        18.72
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        36.65
    Achieved Active Warps Per SM           warp        17.59
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.69%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (36.7%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    147406.67
    Total DRAM Elapsed Cycles        cycle      2376704
    Average L1 Active Cycles         cycle     34192.64
    Total L1 Elapsed Cycles          cycle      5450980
    Average L2 Active Cycles         cycle     32518.22
    Total L2 Elapsed Cycles          cycle      1360836
    Average SM Active Cycles         cycle     34192.64
    Total SM Elapsed Cycles          cycle      5450980
    Average SMSP Active Cycles       cycle     33755.57
    Total SMSP Elapsed Cycles        cycle     21803920
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.52%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 14.35% above the average, while the minimum instance value is 32.17% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 13.62%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 17.18% above the average, while the minimum instance value is 30.90% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.52%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 14.35% above the average, while the minimum instance value is 32.17% below the      
          average.                                                                                                      

  MultKeyAccum_8(Params *, int, int, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long **) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.23
    SM Frequency            cycle/nsecond         2.23
    Elapsed Cycles                  cycle       180763
    Memory Throughput                   %        92.33
    DRAM Throughput                     %        92.33
    Duration                      usecond        80.99
    L1/TEX Cache Throughput             %         9.90
    L2 Cache Throughput                 %        38.65
    SM Active Cycles                cycle    174608.55
    Compute (SM) Throughput             %        10.11
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                2.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           12
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        87.45
    Achieved Active Warps Per SM           warp        41.97
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 12.55%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (87.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    765386.67
    Total DRAM Elapsed Cycles        cycle      9947136
    Average L1 Active Cycles         cycle    174608.55
    Total L1 Elapsed Cycles          cycle     22207540
    Average L2 Active Cycles         cycle    147016.36
    Total L2 Elapsed Cycles          cycle      5722776
    Average SM Active Cycles         cycle    174608.55
    Total SM Elapsed Cycles          cycle     22207540
    Average SMSP Active Cycles       cycle    166277.72
    Total SMSP Elapsed Cycles        cycle     88830160
    -------------------------- ----------- ------------

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.20
    SM Frequency            cycle/nsecond         2.18
    Elapsed Cycles                  cycle        31590
    Memory Throughput                   %        66.95
    DRAM Throughput                     %        66.95
    Duration                      usecond        14.40
    L1/TEX Cache Throughput             %        28.45
    L2 Cache Throughput                 %        29.23
    SM Active Cycles                cycle     26796.80
    Compute (SM) Throughput             %        17.08
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        25.17
    Achieved Active Warps Per SM           warp        12.08
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 49.65%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (25.2%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     98382.67
    Total DRAM Elapsed Cycles        cycle      1763328
    Average L1 Active Cycles         cycle     26796.80
    Total L1 Elapsed Cycles          cycle      4098812
    Average L2 Active Cycles         cycle     22919.03
    Total L2 Elapsed Cycles          cycle      1006632
    Average SM Active Cycles         cycle     26796.80
    Total SM Elapsed Cycles          cycle      4098812
    Average SMSP Active Cycles       cycle     26843.53
    Total SMSP Elapsed Cycles        cycle     16395248
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.896%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 7.05% above the average, while the minimum instance value is 22.48% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.707%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 8.00% above the average, while the minimum instance value is 22.38% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.896%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 7.05% above the average, while the minimum instance value is 22.48% below the       
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.20
    SM Frequency            cycle/nsecond         2.22
    Elapsed Cycles                  cycle        23092
    Memory Throughput                   %        46.53
    DRAM Throughput                     %        46.53
    Duration                      usecond        10.40
    L1/TEX Cache Throughput             %        33.76
    L2 Cache Throughput                 %        20.12
    SM Active Cycles                cycle     11906.48
    Compute (SM) Throughput             %        13.97
    ----------------------- ------------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        25.48
    Achieved Active Warps Per SM           warp        12.23
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 49.04%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (25.5%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        49356
    Total DRAM Elapsed Cycles        cycle      1272832
    Average L1 Active Cycles         cycle     11906.48
    Total L1 Elapsed Cycles          cycle      2868348
    Average L2 Active Cycles         cycle     14276.58
    Total L2 Elapsed Cycles          cycle       731196
    Average SM Active Cycles         cycle     11906.48
    Total SM Elapsed Cycles          cycle      2868348
    Average SMSP Active Cycles       cycle     11319.35
    Total SMSP Elapsed Cycles        cycle     11473392
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 22.15%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 41.68% above the average, while the minimum instance value is 67.73% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 21.54%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 42.64% above the average, while the minimum instance value is 71.19% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 22.15%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 41.68% above the average, while the minimum instance value is 67.73% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.23
    SM Frequency            cycle/nsecond         2.22
    Elapsed Cycles                  cycle        43935
    Memory Throughput                   %        72.89
    DRAM Throughput                     %        72.89
    Duration                      usecond        19.78
    L1/TEX Cache Throughput             %        32.95
    L2 Cache Throughput                 %        31.67
    SM Active Cycles                cycle     34374.14
    Compute (SM) Throughput             %        18.63
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        36.88
    Achieved Active Warps Per SM           warp        17.70
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.23%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (36.9%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    147406.67
    Total DRAM Elapsed Cycles        cycle      2426880
    Average L1 Active Cycles         cycle     34374.14
    Total L1 Elapsed Cycles          cycle      5476944
    Average L2 Active Cycles         cycle     32613.08
    Total L2 Elapsed Cycles          cycle      1393416
    Average SM Active Cycles         cycle     34374.14
    Total SM Elapsed Cycles          cycle      5476944
    Average SMSP Active Cycles       cycle     34201.78
    Total SMSP Elapsed Cycles        cycle     21907776
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 13.4%                                                                                           
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 16.67% above the average, while the minimum instance value is 27.93% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 14.1%                                                                                           
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 17.64% above the average, while the minimum instance value is 30.80% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 13.4%                                                                                           
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 16.67% above the average, while the minimum instance value is 27.93% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.22
    SM Frequency            cycle/nsecond         2.21
    Elapsed Cycles                  cycle        43606
    Memory Throughput                   %        73.42
    DRAM Throughput                     %        73.42
    Duration                      usecond        19.65
    L1/TEX Cache Throughput             %        33.31
    L2 Cache Throughput                 %        31.90
    SM Active Cycles                cycle     33993.37
    Compute (SM) Throughput             %        18.55
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        36.93
    Achieved Active Warps Per SM           warp        17.73
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.14%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (36.9%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    147413.33
    Total DRAM Elapsed Cycles        cycle      2409472
    Average L1 Active Cycles         cycle     33993.37
    Total L1 Elapsed Cycles          cycle      5499866
    Average L2 Active Cycles         cycle     32271.92
    Total L2 Elapsed Cycles          cycle      1383192
    Average SM Active Cycles         cycle     33993.37
    Total SM Elapsed Cycles          cycle      5499866
    Average SMSP Active Cycles       cycle     33888.16
    Total SMSP Elapsed Cycles        cycle     21999464
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 13.33%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 16.85% above the average, while the minimum instance value is 30.88% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.66%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 16.06% above the average, while the minimum instance value is 27.99% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 13.33%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 16.85% above the average, while the minimum instance value is 30.88% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.21
    SM Frequency            cycle/nsecond         2.18
    Elapsed Cycles                  cycle        43237
    Memory Throughput                   %        73.26
    DRAM Throughput                     %        73.26
    Duration                      usecond        19.71
    L1/TEX Cache Throughput             %        33.57
    L2 Cache Throughput                 %        32.04
    SM Active Cycles                cycle     33736.23
    Compute (SM) Throughput             %        18.49
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        37.77
    Achieved Active Warps Per SM           warp        18.13
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.46%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (37.8%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       147408
    Total DRAM Elapsed Cycles        cycle      2414592
    Average L1 Active Cycles         cycle     33736.23
    Total L1 Elapsed Cycles          cycle      5517050
    Average L2 Active Cycles         cycle     32387.75
    Total L2 Elapsed Cycles          cycle      1377288
    Average SM Active Cycles         cycle     33736.23
    Total SM Elapsed Cycles          cycle      5517050
    Average SMSP Active Cycles       cycle     33958.35
    Total SMSP Elapsed Cycles        cycle     22068200
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 12.56%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 16.05% above the average, while the minimum instance value is 28.57% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 13.29%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 16.87% above the average, while the minimum instance value is 31.18% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.56%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 16.05% above the average, while the minimum instance value is 28.57% below the      
          average.                                                                                                      

  MultKeyAccum_8(Params *, int, int, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long **) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.23
    SM Frequency            cycle/nsecond         2.23
    Elapsed Cycles                  cycle       180323
    Memory Throughput                   %        92.54
    DRAM Throughput                     %        92.54
    Duration                      usecond        80.80
    L1/TEX Cache Throughput             %         9.93
    L2 Cache Throughput                 %        38.74
    SM Active Cycles                cycle    174166.77
    Compute (SM) Throughput             %        10.21
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                2.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           12
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        86.87
    Achieved Active Warps Per SM           warp        41.70
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 13.13%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (86.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    765218.67
    Total DRAM Elapsed Cycles        cycle      9922560
    Average L1 Active Cycles         cycle    174166.77
    Total L1 Elapsed Cycles          cycle     21988800
    Average L2 Active Cycles         cycle    146688.56
    Total L2 Elapsed Cycles          cycle      5709024
    Average SM Active Cycles         cycle    174166.77
    Total SM Elapsed Cycles          cycle     21988800
    Average SMSP Active Cycles       cycle    165716.49
    Total SMSP Elapsed Cycles        cycle     87955200
    -------------------------- ----------- ------------

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.21
    SM Frequency            cycle/nsecond         2.18
    Elapsed Cycles                  cycle        32149
    Memory Throughput                   %        65.77
    DRAM Throughput                     %        65.77
    Duration                      usecond        14.66
    L1/TEX Cache Throughput             %        28.18
    L2 Cache Throughput                 %        28.73
    SM Active Cycles                cycle     27084.82
    Compute (SM) Throughput             %        17.19
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        24.68
    Achieved Active Warps Per SM           warp        11.85
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50.64%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (24.7%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     98381.33
    Total DRAM Elapsed Cycles        cycle      1795072
    Average L1 Active Cycles         cycle     27084.82
    Total L1 Elapsed Cycles          cycle      4072196
    Average L2 Active Cycles         cycle     22785.17
    Total L2 Elapsed Cycles          cycle      1023840
    Average SM Active Cycles         cycle     27084.82
    Total SM Elapsed Cycles          cycle      4072196
    Average SMSP Active Cycles       cycle     26944.41
    Total SMSP Elapsed Cycles        cycle     16288784
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.737%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 6.74% above the average, while the minimum instance value is 19.99% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.231%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 7.36% above the average, while the minimum instance value is 22.60% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.737%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 6.74% above the average, while the minimum instance value is 19.99% below the       
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.21
    SM Frequency            cycle/nsecond         2.19
    Elapsed Cycles                  cycle        43156
    Memory Throughput                   %        73.69
    DRAM Throughput                     %        73.69
    Duration                      usecond        19.58
    L1/TEX Cache Throughput             %        33.02
    L2 Cache Throughput                 %        32.16
    SM Active Cycles                cycle     34271.52
    Compute (SM) Throughput             %        18.59
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        37.00
    Achieved Active Warps Per SM           warp        17.76
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26%                                                                                       
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (37.0%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       147404
    Total DRAM Elapsed Cycles        cycle      2400256
    Average L1 Active Cycles         cycle     34271.52
    Total L1 Elapsed Cycles          cycle      5488762
    Average L2 Active Cycles         cycle     32277.39
    Total L2 Elapsed Cycles          cycle      1371852
    Average SM Active Cycles         cycle     34271.52
    Total SM Elapsed Cycles          cycle      5488762
    Average SMSP Active Cycles       cycle     33978.99
    Total SMSP Elapsed Cycles        cycle     21955048
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.99%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 15.01% above the average, while the minimum instance value is 28.12% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.66%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 15.98% above the average, while the minimum instance value is 31.39% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.99%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 15.01% above the average, while the minimum instance value is 28.12% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.22
    SM Frequency            cycle/nsecond         2.22
    Elapsed Cycles                  cycle        43308
    Memory Throughput                   %        74.11
    DRAM Throughput                     %        74.11
    Duration                      usecond        19.46
    L1/TEX Cache Throughput             %        32.79
    L2 Cache Throughput                 %        32.16
    SM Active Cycles                cycle     34531.69
    Compute (SM) Throughput             %        18.53
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        36.47
    Achieved Active Warps Per SM           warp        17.51
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.06%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (36.5%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       147408
    Total DRAM Elapsed Cycles        cycle      2386944
    Average L1 Active Cycles         cycle     34531.69
    Total L1 Elapsed Cycles          cycle      5504818
    Average L2 Active Cycles         cycle     32332.64
    Total L2 Elapsed Cycles          cycle      1372176
    Average SM Active Cycles         cycle     34531.69
    Total SM Elapsed Cycles          cycle      5504818
    Average SMSP Active Cycles       cycle     34179.99
    Total SMSP Elapsed Cycles        cycle     22019272
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.87%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 14.78% above the average, while the minimum instance value is 28.87% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 13.62%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 17.13% above the average, while the minimum instance value is 30.80% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.87%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 14.78% above the average, while the minimum instance value is 28.87% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.20
    SM Frequency            cycle/nsecond         2.21
    Elapsed Cycles                  cycle        43604
    Memory Throughput                   %        73.41
    DRAM Throughput                     %        73.41
    Duration                      usecond        19.68
    L1/TEX Cache Throughput             %        32.87
    L2 Cache Throughput                 %        31.89
    SM Active Cycles                cycle     34449.51
    Compute (SM) Throughput             %        18.29
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        36.77
    Achieved Active Warps Per SM           warp        17.65
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.46%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (36.8%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    147406.67
    Total DRAM Elapsed Cycles        cycle      2409472
    Average L1 Active Cycles         cycle     34449.51
    Total L1 Elapsed Cycles          cycle      5579068
    Average L2 Active Cycles         cycle     32394.14
    Total L2 Elapsed Cycles          cycle      1383516
    Average SM Active Cycles         cycle     34449.51
    Total SM Elapsed Cycles          cycle      5579068
    Average SMSP Active Cycles       cycle     34427.53
    Total SMSP Elapsed Cycles        cycle     22316272
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 12.32%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 15.59% above the average, while the minimum instance value is 29.73% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.14%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 15.37% above the average, while the minimum instance value is 28.73% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.32%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 15.59% above the average, while the minimum instance value is 29.73% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.21
    SM Frequency            cycle/nsecond         2.18
    Elapsed Cycles                  cycle        43105
    Memory Throughput                   %        73.45
    DRAM Throughput                     %        73.45
    Duration                      usecond        19.65
    L1/TEX Cache Throughput             %        32.80
    L2 Cache Throughput                 %        32.11
    SM Active Cycles                cycle     34488.20
    Compute (SM) Throughput             %        18.47
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        36.46
    Achieved Active Warps Per SM           warp        17.50
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.07%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (36.5%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    147417.33
    Total DRAM Elapsed Cycles        cycle      2408448
    Average L1 Active Cycles         cycle     34488.20
    Total L1 Elapsed Cycles          cycle      5522924
    Average L2 Active Cycles         cycle     32429.50
    Total L2 Elapsed Cycles          cycle      1374120
    Average SM Active Cycles         cycle     34488.20
    Total SM Elapsed Cycles          cycle      5522924
    Average SMSP Active Cycles       cycle     34346.85
    Total SMSP Elapsed Cycles        cycle     22091696
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.66%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 14.59% above the average, while the minimum instance value is 31.14% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.79%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 14.81% above the average, while the minimum instance value is 29.61% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.66%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 14.59% above the average, while the minimum instance value is 31.14% below the      
          average.                                                                                                      

  MultKeyAccum_8(Params *, int, int, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long **) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.24
    SM Frequency            cycle/nsecond         2.23
    Elapsed Cycles                  cycle       179643
    Memory Throughput                   %        92.95
    DRAM Throughput                     %        92.95
    Duration                      usecond        80.51
    L1/TEX Cache Throughput             %         9.99
    L2 Cache Throughput                 %        38.89
    SM Active Cycles                cycle    173060.98
    Compute (SM) Throughput             %        10.16
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                2.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           12
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.13
    Achieved Active Warps Per SM           warp        42.30
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.87%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       765960
    Total DRAM Elapsed Cycles        cycle      9888768
    Average L1 Active Cycles         cycle    173060.98
    Total L1 Elapsed Cycles          cycle     22103054
    Average L2 Active Cycles         cycle    146436.94
    Total L2 Elapsed Cycles          cycle      5687748
    Average SM Active Cycles         cycle    173060.98
    Total SM Elapsed Cycles          cycle     22103054
    Average SMSP Active Cycles       cycle    165267.99
    Total SMSP Elapsed Cycles        cycle     88412216
    -------------------------- ----------- ------------

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.21
    SM Frequency            cycle/nsecond         2.18
    Elapsed Cycles                  cycle        42855
    Memory Throughput                   %        73.95
    DRAM Throughput                     %        73.95
    Duration                      usecond        19.52
    L1/TEX Cache Throughput             %        33.27
    L2 Cache Throughput                 %        32.33
    SM Active Cycles                cycle     34032.24
    Compute (SM) Throughput             %        18.41
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        36.70
    Achieved Active Warps Per SM           warp        17.61
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.61%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (36.7%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    147406.67
    Total DRAM Elapsed Cycles        cycle      2392064
    Average L1 Active Cycles         cycle     34032.24
    Total L1 Elapsed Cycles          cycle      5542702
    Average L2 Active Cycles         cycle     32475.53
    Total L2 Elapsed Cycles          cycle      1364796
    Average SM Active Cycles         cycle     34032.24
    Total SM Elapsed Cycles          cycle      5542702
    Average SMSP Active Cycles       cycle     34176.20
    Total SMSP Elapsed Cycles        cycle     22170808
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.83%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 15.05% above the average, while the minimum instance value is 31.39% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 13.36%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 16.93% above the average, while the minimum instance value is 34.37% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.83%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 15.05% above the average, while the minimum instance value is 31.39% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.22
    SM Frequency            cycle/nsecond         2.20
    Elapsed Cycles                  cycle        43305
    Memory Throughput                   %        73.54
    DRAM Throughput                     %        73.54
    Duration                      usecond        19.62
    L1/TEX Cache Throughput             %        33.02
    L2 Cache Throughput                 %        32.05
    SM Active Cycles                cycle     34259.19
    Compute (SM) Throughput             %        18.14
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        36.81
    Achieved Active Warps Per SM           warp        17.67
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.39%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (36.8%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    147405.33
    Total DRAM Elapsed Cycles        cycle      2405376
    Average L1 Active Cycles         cycle     34259.19
    Total L1 Elapsed Cycles          cycle      5624422
    Average L2 Active Cycles         cycle     32388.08
    Total L2 Elapsed Cycles          cycle      1376496
    Average SM Active Cycles         cycle     34259.19
    Total SM Elapsed Cycles          cycle      5624422
    Average SMSP Active Cycles       cycle     33963.34
    Total SMSP Elapsed Cycles        cycle     22497688
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.72%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 15.04% above the average, while the minimum instance value is 30.07% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.32%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 15.93% above the average, while the minimum instance value is 29.13% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.72%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 15.04% above the average, while the minimum instance value is 30.07% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.21
    SM Frequency            cycle/nsecond         2.22
    Elapsed Cycles                  cycle        43411
    Memory Throughput                   %        73.95
    DRAM Throughput                     %        73.95
    Duration                      usecond        19.52
    L1/TEX Cache Throughput             %        32.75
    L2 Cache Throughput                 %        32.07
    SM Active Cycles                cycle     34579.46
    Compute (SM) Throughput             %        18.56
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        36.61
    Achieved Active Warps Per SM           warp        17.57
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.78%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (36.6%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    147414.67
    Total DRAM Elapsed Cycles        cycle      2392064
    Average L1 Active Cycles         cycle     34579.46
    Total L1 Elapsed Cycles          cycle      5496304
    Average L2 Active Cycles         cycle        32641
    Total L2 Elapsed Cycles          cycle      1375920
    Average SM Active Cycles         cycle     34579.46
    Total SM Elapsed Cycles          cycle      5496304
    Average SMSP Active Cycles       cycle     34202.52
    Total SMSP Elapsed Cycles        cycle     21985216
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 12.12%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 15.05% above the average, while the minimum instance value is 25.05% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 14.23%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 17.87% above the average, while the minimum instance value is 29.69% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.12%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 15.05% above the average, while the minimum instance value is 25.05% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.22
    SM Frequency            cycle/nsecond         2.18
    Elapsed Cycles                  cycle        31947
    Memory Throughput                   %        66.11
    DRAM Throughput                     %        66.11
    Duration                      usecond        14.56
    L1/TEX Cache Throughput             %        28.17
    L2 Cache Throughput                 %        28.89
    SM Active Cycles                cycle     27091.70
    Compute (SM) Throughput             %        17.10
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        25.35
    Achieved Active Warps Per SM           warp        12.17
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 49.3%                                                                                     
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (25.3%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        98380
    Total DRAM Elapsed Cycles        cycle      1785856
    Average L1 Active Cycles         cycle     27091.70
    Total L1 Elapsed Cycles          cycle      4093632
    Average L2 Active Cycles         cycle     22877.47
    Total L2 Elapsed Cycles          cycle      1018188
    Average SM Active Cycles         cycle     27091.70
    Total SM Elapsed Cycles          cycle      4093632
    Average SMSP Active Cycles       cycle     27057.51
    Total SMSP Elapsed Cycles        cycle     16374528
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.155%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 6.08% above the average, while the minimum instance value is 13.88% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.802%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 9.22% above the average, while the minimum instance value is 18.54% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.155%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 6.08% above the average, while the minimum instance value is 13.88% below the       
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.17
    SM Frequency            cycle/nsecond         2.19
    Elapsed Cycles                  cycle        22854
    Memory Throughput                   %        46.65
    DRAM Throughput                     %        46.65
    Duration                      usecond        10.40
    L1/TEX Cache Throughput             %        33.82
    L2 Cache Throughput                 %        20.26
    SM Active Cycles                cycle     11890.73
    Compute (SM) Throughput             %        14.05
    ----------------------- ------------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        25.56
    Achieved Active Warps Per SM           warp        12.27
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 48.88%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (25.6%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     49357.33
    Total DRAM Elapsed Cycles        cycle      1269760
    Average L1 Active Cycles         cycle     11890.73
    Total L1 Elapsed Cycles          cycle      2852674
    Average L2 Active Cycles         cycle     14226.33
    Total L2 Elapsed Cycles          cycle       725976
    Average SM Active Cycles         cycle     11890.73
    Total SM Elapsed Cycles          cycle      2852674
    Average SMSP Active Cycles       cycle     11168.46
    Total SMSP Elapsed Cycles        cycle     11410696
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 21.87%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 40.99% above the average, while the minimum instance value is 67.22% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 21.76%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 43.43% above the average, while the minimum instance value is 71.71% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 21.87%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 40.99% above the average, while the minimum instance value is 67.22% below the      
          average.                                                                                                      

  MultKeyAccum_8(Params *, int, int, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long **) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.24
    SM Frequency            cycle/nsecond         2.23
    Elapsed Cycles                  cycle       180704
    Memory Throughput                   %        92.23
    DRAM Throughput                     %        92.23
    Duration                      usecond        80.93
    L1/TEX Cache Throughput             %         9.91
    L2 Cache Throughput                 %        38.67
    SM Active Cycles                cycle    174581.17
    Compute (SM) Throughput             %        10.18
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                2.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           12
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        86.84
    Achieved Active Warps Per SM           warp        41.68
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 13.16%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (86.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    764038.67
    Total DRAM Elapsed Cycles        cycle      9940992
    Average L1 Active Cycles         cycle    174581.17
    Total L1 Elapsed Cycles          cycle     22050624
    Average L2 Active Cycles         cycle    146543.42
    Total L2 Elapsed Cycles          cycle      5720364
    Average SM Active Cycles         cycle    174581.17
    Total SM Elapsed Cycles          cycle     22050624
    Average SMSP Active Cycles       cycle    165643.58
    Total SMSP Elapsed Cycles        cycle     88202496
    -------------------------- ----------- ------------

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.22
    SM Frequency            cycle/nsecond         2.21
    Elapsed Cycles                  cycle        43657
    Memory Throughput                   %        73.29
    DRAM Throughput                     %        73.29
    Duration                      usecond        19.68
    L1/TEX Cache Throughput             %        32.79
    L2 Cache Throughput                 %        31.86
    SM Active Cycles                cycle     34536.71
    Compute (SM) Throughput             %        18.40
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        36.43
    Achieved Active Warps Per SM           warp        17.49
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.14%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (36.4%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    147406.67
    Total DRAM Elapsed Cycles        cycle      2413568
    Average L1 Active Cycles         cycle     34536.71
    Total L1 Elapsed Cycles          cycle      5543534
    Average L2 Active Cycles         cycle     32366.31
    Total L2 Elapsed Cycles          cycle      1385064
    Average SM Active Cycles         cycle     34536.71
    Total SM Elapsed Cycles          cycle      5543534
    Average SMSP Active Cycles       cycle     33878.86
    Total SMSP Elapsed Cycles        cycle     22174136
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 12.38%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 15.52% above the average, while the minimum instance value is 31.67% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.56%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 16.05% above the average, while the minimum instance value is 31.69% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.38%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 15.52% above the average, while the minimum instance value is 31.67% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.22
    SM Frequency            cycle/nsecond         2.22
    Elapsed Cycles                  cycle        42810
    Memory Throughput                   %        74.97
    DRAM Throughput                     %        74.97
    Duration                      usecond        19.23
    L1/TEX Cache Throughput             %        33.14
    L2 Cache Throughput                 %        32.52
    SM Active Cycles                cycle     34159.08
    Compute (SM) Throughput             %        18.50
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        36.87
    Achieved Active Warps Per SM           warp        17.70
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.26%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (36.9%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    147406.67
    Total DRAM Elapsed Cycles        cycle      2359296
    Average L1 Active Cycles         cycle     34159.08
    Total L1 Elapsed Cycles          cycle      5515552
    Average L2 Active Cycles         cycle     32498.42
    Total L2 Elapsed Cycles          cycle      1356660
    Average SM Active Cycles         cycle     34159.08
    Total SM Elapsed Cycles          cycle      5515552
    Average SMSP Active Cycles       cycle     34473.89
    Total SMSP Elapsed Cycles        cycle     22062208
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.66%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 14.71% above the average, while the minimum instance value is 29.42% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.94%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 14.93% above the average, while the minimum instance value is 31.47% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.66%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 14.71% above the average, while the minimum instance value is 29.42% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.20
    SM Frequency            cycle/nsecond         2.22
    Elapsed Cycles                  cycle        22996
    Memory Throughput                   %        46.83
    DRAM Throughput                     %        46.83
    Duration                      usecond        10.34
    L1/TEX Cache Throughput             %        34.02
    L2 Cache Throughput                 %        20.21
    SM Active Cycles                cycle     11829.89
    Compute (SM) Throughput             %        14.01
    ----------------------- ------------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        26.10
    Achieved Active Warps Per SM           warp        12.53
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 47.81%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (26.1%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     49357.33
    Total DRAM Elapsed Cycles        cycle      1264640
    Average L1 Active Cycles         cycle     11829.89
    Total L1 Elapsed Cycles          cycle      2862434
    Average L2 Active Cycles         cycle     14636.25
    Total L2 Elapsed Cycles          cycle       727596
    Average SM Active Cycles         cycle     11829.89
    Total SM Elapsed Cycles          cycle      2862434
    Average SMSP Active Cycles       cycle     11524.81
    Total SMSP Elapsed Cycles        cycle     11449736
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 21.99%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 41.57% above the average, while the minimum instance value is 66.17% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 22.12%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 42.91% above the average, while the minimum instance value is 70.74% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 21.99%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 41.57% above the average, while the minimum instance value is 66.17% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.23
    SM Frequency            cycle/nsecond         2.19
    Elapsed Cycles                  cycle        31485
    Memory Throughput                   %        67.11
    DRAM Throughput                     %        67.11
    Duration                      usecond        14.34
    L1/TEX Cache Throughput             %        28.84
    L2 Cache Throughput                 %        29.34
    SM Active Cycles                cycle     26441.37
    Compute (SM) Throughput             %        17.21
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        25.62
    Achieved Active Warps Per SM           warp        12.30
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 48.76%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (25.6%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        98384
    Total DRAM Elapsed Cycles        cycle      1759232
    Average L1 Active Cycles         cycle     26441.37
    Total L1 Elapsed Cycles          cycle      4065850
    Average L2 Active Cycles         cycle     22635.83
    Total L2 Elapsed Cycles          cycle      1002816
    Average SM Active Cycles         cycle     26441.37
    Total SM Elapsed Cycles          cycle      4065850
    Average SMSP Active Cycles       cycle     26680.40
    Total SMSP Elapsed Cycles        cycle     16263400
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.264%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 7.53% above the average, while the minimum instance value is 22.18% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.049%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 7.20% above the average, while the minimum instance value is 20.23% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.264%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 7.53% above the average, while the minimum instance value is 22.18% below the       
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.22
    SM Frequency            cycle/nsecond         2.19
    Elapsed Cycles                  cycle        43381
    Memory Throughput                   %        73.07
    DRAM Throughput                     %        73.07
    Duration                      usecond        19.74
    L1/TEX Cache Throughput             %        33.08
    L2 Cache Throughput                 %        31.92
    SM Active Cycles                cycle     34224.62
    Compute (SM) Throughput             %        18.51
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        36.52
    Achieved Active Warps Per SM           warp        17.53
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.97%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (36.5%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       147408
    Total DRAM Elapsed Cycles        cycle      2420736
    Average L1 Active Cycles         cycle     34224.62
    Total L1 Elapsed Cycles          cycle      5511494
    Average L2 Active Cycles         cycle     32631.06
    Total L2 Elapsed Cycles          cycle      1382400
    Average SM Active Cycles         cycle     34224.62
    Total SM Elapsed Cycles          cycle      5511494
    Average SMSP Active Cycles       cycle     34278.31
    Total SMSP Elapsed Cycles        cycle     22045976
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 12.36%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 15.55% above the average, while the minimum instance value is 26.59% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.39%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 15.56% above the average, while the minimum instance value is 29.07% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.36%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 15.55% above the average, while the minimum instance value is 26.59% below the      
          average.                                                                                                      

  MultKeyAccum_8(Params *, int, int, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long **) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.24
    SM Frequency            cycle/nsecond         2.23
    Elapsed Cycles                  cycle       181022
    Memory Throughput                   %        92.10
    DRAM Throughput                     %        92.10
    Duration                      usecond        81.12
    L1/TEX Cache Throughput             %         9.94
    L2 Cache Throughput                 %        38.60
    SM Active Cycles                cycle    173995.22
    Compute (SM) Throughput             %        10.14
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                2.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           12
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        87.69
    Achieved Active Warps Per SM           warp        42.09
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 12.31%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (87.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    764762.67
    Total DRAM Elapsed Cycles        cycle      9964544
    Average L1 Active Cycles         cycle    173995.22
    Total L1 Elapsed Cycles          cycle     22141586
    Average L2 Active Cycles         cycle    147012.83
    Total L2 Elapsed Cycles          cycle      5731272
    Average SM Active Cycles         cycle    173995.22
    Total SM Elapsed Cycles          cycle     22141586
    Average SMSP Active Cycles       cycle    166036.82
    Total SMSP Elapsed Cycles        cycle     88566344
    -------------------------- ----------- ------------

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.22
    SM Frequency            cycle/nsecond         2.20
    Elapsed Cycles                  cycle        32066
    Memory Throughput                   %        66.26
    DRAM Throughput                     %        66.26
    Duration                      usecond        14.53
    L1/TEX Cache Throughput             %        28.01
    L2 Cache Throughput                 %        28.86
    SM Active Cycles                cycle     27233.78
    Compute (SM) Throughput             %        17.29
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        24.87
    Achieved Active Warps Per SM           warp        11.94
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50.27%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (24.9%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     98381.33
    Total DRAM Elapsed Cycles        cycle      1781760
    Average L1 Active Cycles         cycle     27233.78
    Total L1 Elapsed Cycles          cycle      4047826
    Average L2 Active Cycles         cycle     22924.06
    Total L2 Elapsed Cycles          cycle      1019196
    Average SM Active Cycles         cycle     27233.78
    Total SM Elapsed Cycles          cycle      4047826
    Average SMSP Active Cycles       cycle     26997.82
    Total SMSP Elapsed Cycles        cycle     16191304
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.537%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 6.43% above the average, while the minimum instance value is 19.39% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.831%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 6.83% above the average, while the minimum instance value is 21.47% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.537%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 6.43% above the average, while the minimum instance value is 19.39% below the       
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.21
    SM Frequency            cycle/nsecond         2.22
    Elapsed Cycles                  cycle        22868
    Memory Throughput                   %        47.07
    DRAM Throughput                     %        47.07
    Duration                      usecond        10.27
    L1/TEX Cache Throughput             %        33.95
    L2 Cache Throughput                 %        20.32
    SM Active Cycles                cycle     11857.45
    Compute (SM) Throughput             %        14.15
    ----------------------- ------------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        25.49
    Achieved Active Warps Per SM           warp        12.23
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 49.03%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (25.5%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     49365.33
    Total DRAM Elapsed Cycles        cycle      1258496
    Average L1 Active Cycles         cycle     11857.45
    Total L1 Elapsed Cycles          cycle      2831908
    Average L2 Active Cycles         cycle     14433.42
    Total L2 Elapsed Cycles          cycle       723708
    Average SM Active Cycles         cycle     11857.45
    Total SM Elapsed Cycles          cycle      2831908
    Average SMSP Active Cycles       cycle     11347.43
    Total SMSP Elapsed Cycles        cycle     11327632
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 22.03%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 41.10% above the average, while the minimum instance value is 68.48% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.39%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 45.60% above the average, while the minimum instance value is 71.72% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 22.03%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 41.10% above the average, while the minimum instance value is 68.48% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.20
    SM Frequency            cycle/nsecond         2.18
    Elapsed Cycles                  cycle        43184
    Memory Throughput                   %        73.41
    DRAM Throughput                     %        73.41
    Duration                      usecond        19.68
    L1/TEX Cache Throughput             %        33.36
    L2 Cache Throughput                 %        32.06
    SM Active Cycles                cycle     33914.87
    Compute (SM) Throughput             %        18.35
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        37.12
    Achieved Active Warps Per SM           warp        17.82
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.75%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (37.1%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       147404
    Total DRAM Elapsed Cycles        cycle      2409472
    Average L1 Active Cycles         cycle     33914.87
    Total L1 Elapsed Cycles          cycle      5558790
    Average L2 Active Cycles         cycle     32491.72
    Total L2 Elapsed Cycles          cycle      1376424
    Average SM Active Cycles         cycle     33914.87
    Total SM Elapsed Cycles          cycle      5558790
    Average SMSP Active Cycles       cycle     34482.12
    Total SMSP Elapsed Cycles        cycle     22235160
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 12.08%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 15.47% above the average, while the minimum instance value is 27.09% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.74%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 14.78% above the average, while the minimum instance value is 27.75% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.08%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 15.47% above the average, while the minimum instance value is 27.09% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.22
    SM Frequency            cycle/nsecond         2.18
    Elapsed Cycles                  cycle        42852
    Memory Throughput                   %        73.92
    DRAM Throughput                     %        73.92
    Duration                      usecond        19.52
    L1/TEX Cache Throughput             %        33.50
    L2 Cache Throughput                 %        32.35
    SM Active Cycles                cycle     33773.16
    Compute (SM) Throughput             %        18.50
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        36.31
    Achieved Active Warps Per SM           warp        17.43
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.39%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (36.3%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    147406.67
    Total DRAM Elapsed Cycles        cycle      2393088
    Average L1 Active Cycles         cycle     33773.16
    Total L1 Elapsed Cycles          cycle      5513536
    Average L2 Active Cycles         cycle     32536.86
    Total L2 Elapsed Cycles          cycle      1364004
    Average SM Active Cycles         cycle     33773.16
    Total SM Elapsed Cycles          cycle      5513536
    Average SMSP Active Cycles       cycle     34340.67
    Total SMSP Elapsed Cycles        cycle     22054144
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 12.32%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 15.72% above the average, while the minimum instance value is 31.24% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.81%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 16.07% above the average, while the minimum instance value is 28.36% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.32%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 15.72% above the average, while the minimum instance value is 31.24% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.22
    SM Frequency            cycle/nsecond         2.19
    Elapsed Cycles                  cycle        43485
    Memory Throughput                   %        72.92
    DRAM Throughput                     %        72.92
    Duration                      usecond        19.78
    L1/TEX Cache Throughput             %        33.57
    L2 Cache Throughput                 %        31.85
    SM Active Cycles                cycle     33730.58
    Compute (SM) Throughput             %        18.47
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        36.98
    Achieved Active Warps Per SM           warp        17.75
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.05%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (37.0%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    147406.67
    Total DRAM Elapsed Cycles        cycle      2425856
    Average L1 Active Cycles         cycle     33730.58
    Total L1 Elapsed Cycles          cycle      5522504
    Average L2 Active Cycles         cycle     32266.69
    Total L2 Elapsed Cycles          cycle      1385424
    Average SM Active Cycles         cycle     33730.58
    Total SM Elapsed Cycles          cycle      5522504
    Average SMSP Active Cycles       cycle     34160.78
    Total SMSP Elapsed Cycles        cycle     22090016
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 13.09%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 16.74% above the average, while the minimum instance value is 28.12% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.74%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 14.83% above the average, while the minimum instance value is 30.49% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 13.09%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 16.74% above the average, while the minimum instance value is 28.12% below the      
          average.                                                                                                      

  MultKeyAccum_8(Params *, int, int, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long **) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.24
    SM Frequency            cycle/nsecond         2.23
    Elapsed Cycles                  cycle       181283
    Memory Throughput                   %        92.12
    DRAM Throughput                     %        92.12
    Duration                      usecond        81.12
    L1/TEX Cache Throughput             %         9.89
    L2 Cache Throughput                 %        38.55
    SM Active Cycles                cycle    174935.92
    Compute (SM) Throughput             %        10.15
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                2.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           12
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        86.91
    Achieved Active Warps Per SM           warp        41.72
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 13.09%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (86.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    765202.67
    Total DRAM Elapsed Cycles        cycle      9967616
    Average L1 Active Cycles         cycle    174935.92
    Total L1 Elapsed Cycles          cycle     22123640
    Average L2 Active Cycles         cycle    146264.39
    Total L2 Elapsed Cycles          cycle      5738112
    Average SM Active Cycles         cycle    174935.92
    Total SM Elapsed Cycles          cycle     22123640
    Average SMSP Active Cycles       cycle    165341.72
    Total SMSP Elapsed Cycles        cycle     88494560
    -------------------------- ----------- ------------

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.20
    SM Frequency            cycle/nsecond         2.21
    Elapsed Cycles                  cycle        32498
    Memory Throughput                   %        65.80
    DRAM Throughput                     %        65.80
    Duration                      usecond        14.66
    L1/TEX Cache Throughput             %        27.92
    L2 Cache Throughput                 %        28.55
    SM Active Cycles                cycle     27309.25
    Compute (SM) Throughput             %        17.25
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        24.97
    Achieved Active Warps Per SM           warp        11.98
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50.07%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (25.0%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        98380
    Total DRAM Elapsed Cycles        cycle      1794048
    Average L1 Active Cycles         cycle     27309.25
    Total L1 Elapsed Cycles          cycle      4058296
    Average L2 Active Cycles         cycle     22880.86
    Total L2 Elapsed Cycles          cycle      1030356
    Average SM Active Cycles         cycle     27309.25
    Total SM Elapsed Cycles          cycle      4058296
    Average SMSP Active Cycles       cycle     27052.02
    Total SMSP Elapsed Cycles        cycle     16233184
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.125%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 8.27% above the average, while the minimum instance value is 21.77% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.443%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 9.90% above the average, while the minimum instance value is 24.49% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.125%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 8.27% above the average, while the minimum instance value is 21.77% below the       
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.22
    SM Frequency            cycle/nsecond         2.18
    Elapsed Cycles                  cycle        43026
    Memory Throughput                   %        73.51
    DRAM Throughput                     %        73.51
    Duration                      usecond        19.62
    L1/TEX Cache Throughput             %        33.10
    L2 Cache Throughput                 %        32.17
    SM Active Cycles                cycle     34173.02
    Compute (SM) Throughput             %        18.33
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        37.10
    Achieved Active Warps Per SM           warp        17.81
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.81%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (37.1%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    147405.33
    Total DRAM Elapsed Cycles        cycle      2406400
    Average L1 Active Cycles         cycle     34173.02
    Total L1 Elapsed Cycles          cycle      5566112
    Average L2 Active Cycles         cycle     32220.61
    Total L2 Elapsed Cycles          cycle      1371600
    Average SM Active Cycles         cycle     34173.02
    Total SM Elapsed Cycles          cycle      5566112
    Average SMSP Active Cycles       cycle     34458.36
    Total SMSP Elapsed Cycles        cycle     22264448
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.6%                                                                                           
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 14.76% above the average, while the minimum instance value is 31.60% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 10.59%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 13.36% above the average, while the minimum instance value is 29.94% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.6%                                                                                           
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 14.76% above the average, while the minimum instance value is 31.60% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.21
    SM Frequency            cycle/nsecond         2.18
    Elapsed Cycles                  cycle        42818
    Memory Throughput                   %        73.99
    DRAM Throughput                     %        73.99
    Duration                      usecond        19.52
    L1/TEX Cache Throughput             %        33.05
    L2 Cache Throughput                 %        32.34
    SM Active Cycles                cycle     34249.95
    Compute (SM) Throughput             %        18.60
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        36.28
    Achieved Active Warps Per SM           warp        17.41
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.45%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (36.3%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    147426.67
    Total DRAM Elapsed Cycles        cycle      2391040
    Average L1 Active Cycles         cycle     34249.95
    Total L1 Elapsed Cycles          cycle      5484828
    Average L2 Active Cycles         cycle     32572.64
    Total L2 Elapsed Cycles          cycle      1364472
    Average SM Active Cycles         cycle     34249.95
    Total SM Elapsed Cycles          cycle      5484828
    Average SMSP Active Cycles       cycle     34556.22
    Total SMSP Elapsed Cycles        cycle     21939312
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.42%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 14.29% above the average, while the minimum instance value is 31.83% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.7%                                                                                           
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 14.51% above the average, while the minimum instance value is 29.65% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.42%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 14.29% above the average, while the minimum instance value is 31.83% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.21
    SM Frequency            cycle/nsecond         2.18
    Elapsed Cycles                  cycle        43179
    Memory Throughput                   %        73.26
    DRAM Throughput                     %        73.26
    Duration                      usecond        19.71
    L1/TEX Cache Throughput             %        33.02
    L2 Cache Throughput                 %        32.05
    SM Active Cycles                cycle     34300.23
    Compute (SM) Throughput             %        18.40
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        36.43
    Achieved Active Warps Per SM           warp        17.49
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.14%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (36.4%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    147406.67
    Total DRAM Elapsed Cycles        cycle      2414592
    Average L1 Active Cycles         cycle     34300.23
    Total L1 Elapsed Cycles          cycle      5544018
    Average L2 Active Cycles         cycle     32604.75
    Total L2 Elapsed Cycles          cycle      1377000
    Average SM Active Cycles         cycle     34300.23
    Total SM Elapsed Cycles          cycle      5544018
    Average SMSP Active Cycles       cycle     34671.92
    Total SMSP Elapsed Cycles        cycle     22176072
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.86%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 14.97% above the average, while the minimum instance value is 29.68% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.35%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 14.18% above the average, while the minimum instance value is 24.42% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.86%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 14.97% above the average, while the minimum instance value is 29.68% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.21
    SM Frequency            cycle/nsecond         2.22
    Elapsed Cycles                  cycle        43689
    Memory Throughput                   %        73.44
    DRAM Throughput                     %        73.44
    Duration                      usecond        19.65
    L1/TEX Cache Throughput             %        32.43
    L2 Cache Throughput                 %        31.88
    SM Active Cycles                cycle     34915.63
    Compute (SM) Throughput             %        18.68
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        36.11
    Achieved Active Warps Per SM           warp        17.33
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.77%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (36.1%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    147406.67
    Total DRAM Elapsed Cycles        cycle      2408448
    Average L1 Active Cycles         cycle     34915.63
    Total L1 Elapsed Cycles          cycle      5461734
    Average L2 Active Cycles         cycle     32185.31
    Total L2 Elapsed Cycles          cycle      1384236
    Average SM Active Cycles         cycle     34915.63
    Total SM Elapsed Cycles          cycle      5461734
    Average SMSP Active Cycles       cycle     34364.15
    Total SMSP Elapsed Cycles        cycle     21846936
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.99%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 14.66% above the average, while the minimum instance value is 25.57% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.74%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 14.58% above the average, while the minimum instance value is 28.22% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.99%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 14.66% above the average, while the minimum instance value is 25.57% below the      
          average.                                                                                                      

  MultKeyAccum_8(Params *, int, int, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long **) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.24
    SM Frequency            cycle/nsecond         2.23
    Elapsed Cycles                  cycle       180346
    Memory Throughput                   %        92.74
    DRAM Throughput                     %        92.74
    Duration                      usecond        80.74
    L1/TEX Cache Throughput             %         9.97
    L2 Cache Throughput                 %        38.75
    SM Active Cycles                cycle    173387.98
    Compute (SM) Throughput             %        10.17
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                2.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           12
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.18
    Achieved Active Warps Per SM           warp        42.33
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.82%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       766492
    Total DRAM Elapsed Cycles        cycle      9917440
    Average L1 Active Cycles         cycle    173387.98
    Total L1 Elapsed Cycles          cycle     22063222
    Average L2 Active Cycles         cycle    147052.97
    Total L2 Elapsed Cycles          cycle      5708016
    Average SM Active Cycles         cycle    173387.98
    Total SM Elapsed Cycles          cycle     22063222
    Average SMSP Active Cycles       cycle    165791.54
    Total SMSP Elapsed Cycles        cycle     88252888
    -------------------------- ----------- ------------

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.21
    SM Frequency            cycle/nsecond         2.18
    Elapsed Cycles                  cycle        42759
    Memory Throughput                   %        74.07
    DRAM Throughput                     %        74.07
    Duration                      usecond        19.49
    L1/TEX Cache Throughput             %        33.49
    L2 Cache Throughput                 %        32.37
    SM Active Cycles                cycle     33818.50
    Compute (SM) Throughput             %        18.50
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        37.27
    Achieved Active Warps Per SM           warp        17.89
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.45%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (37.3%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    147405.33
    Total DRAM Elapsed Cycles        cycle      2387968
    Average L1 Active Cycles         cycle     33818.50
    Total L1 Elapsed Cycles          cycle      5515190
    Average L2 Active Cycles         cycle     32749.44
    Total L2 Elapsed Cycles          cycle      1363284
    Average SM Active Cycles         cycle     33818.50
    Total SM Elapsed Cycles          cycle      5515190
    Average SMSP Active Cycles       cycle     34364.65
    Total SMSP Elapsed Cycles        cycle     22060760
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.84%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 15.08% above the average, while the minimum instance value is 30.71% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 13.19%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 16.54% above the average, while the minimum instance value is 31.50% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.84%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 15.08% above the average, while the minimum instance value is 30.71% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.21
    SM Frequency            cycle/nsecond         2.18
    Elapsed Cycles                  cycle        43025
    Memory Throughput                   %        73.63
    DRAM Throughput                     %        73.63
    Duration                      usecond        19.62
    L1/TEX Cache Throughput             %        32.95
    L2 Cache Throughput                 %        32.20
    SM Active Cycles                cycle     34376.88
    Compute (SM) Throughput             %        18.25
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        36.29
    Achieved Active Warps Per SM           warp        17.42
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.41%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (36.3%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    147405.33
    Total DRAM Elapsed Cycles        cycle      2402304
    Average L1 Active Cycles         cycle     34376.88
    Total L1 Elapsed Cycles          cycle      5590510
    Average L2 Active Cycles         cycle        32628
    Total L2 Elapsed Cycles          cycle      1370484
    Average SM Active Cycles         cycle     34376.88
    Total SM Elapsed Cycles          cycle      5590510
    Average SMSP Active Cycles       cycle     34415.75
    Total SMSP Elapsed Cycles        cycle     22362040
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.52%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 14.64% above the average, while the minimum instance value is 29.88% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.91%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 15.12% above the average, while the minimum instance value is 30.76% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.52%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 14.64% above the average, while the minimum instance value is 29.88% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.21
    SM Frequency            cycle/nsecond         2.20
    Elapsed Cycles                  cycle        43187
    Memory Throughput                   %        73.82
    DRAM Throughput                     %        73.82
    Duration                      usecond        19.55
    L1/TEX Cache Throughput             %        32.91
    L2 Cache Throughput                 %        32.16
    SM Active Cycles                cycle     34395.36
    Compute (SM) Throughput             %        18.08
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        36.18
    Achieved Active Warps Per SM           warp        17.37
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.63%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (36.2%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    147406.67
    Total DRAM Elapsed Cycles        cycle      2396160
    Average L1 Active Cycles         cycle     34395.36
    Total L1 Elapsed Cycles          cycle      5641606
    Average L2 Active Cycles         cycle     32461.33
    Total L2 Elapsed Cycles          cycle      1372176
    Average SM Active Cycles         cycle     34395.36
    Total SM Elapsed Cycles          cycle      5641606
    Average SMSP Active Cycles       cycle     34346.52
    Total SMSP Elapsed Cycles        cycle     22566424
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.59%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 14.85% above the average, while the minimum instance value is 28.46% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.53%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 14.80% above the average, while the minimum instance value is 30.55% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.59%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 14.85% above the average, while the minimum instance value is 28.46% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.20
    SM Frequency            cycle/nsecond         2.21
    Elapsed Cycles                  cycle        32071
    Memory Throughput                   %        66.57
    DRAM Throughput                     %        66.57
    Duration                      usecond        14.50
    L1/TEX Cache Throughput             %        27.98
    L2 Cache Throughput                 %        28.90
    SM Active Cycles                cycle     27259.01
    Compute (SM) Throughput             %        17.43
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        25.03
    Achieved Active Warps Per SM           warp        12.01
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 49.94%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (25.0%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     98382.67
    Total DRAM Elapsed Cycles        cycle      1773568
    Average L1 Active Cycles         cycle     27259.01
    Total L1 Elapsed Cycles          cycle      4015166
    Average L2 Active Cycles         cycle     22950.17
    Total L2 Elapsed Cycles          cycle      1017900
    Average SM Active Cycles         cycle     27259.01
    Total SM Elapsed Cycles          cycle      4015166
    Average SMSP Active Cycles       cycle     27109.06
    Total SMSP Elapsed Cycles        cycle     16060664
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.397%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 7.36% above the average, while the minimum instance value is 16.07% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.147%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 7.11% above the average, while the minimum instance value is 18.08% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.397%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 7.36% above the average, while the minimum instance value is 16.07% below the       
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.22
    SM Frequency            cycle/nsecond         2.22
    Elapsed Cycles                  cycle        23272
    Memory Throughput                   %        46.16
    DRAM Throughput                     %        46.16
    Duration                      usecond        10.46
    L1/TEX Cache Throughput             %        33.91
    L2 Cache Throughput                 %        19.96
    SM Active Cycles                cycle     11865.55
    Compute (SM) Throughput             %        14.03
    ----------------------- ------------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        25.56
    Achieved Active Warps Per SM           warp        12.27
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 48.88%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (25.6%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     49357.33
    Total DRAM Elapsed Cycles        cycle      1283072
    Average L1 Active Cycles         cycle     11865.55
    Total L1 Elapsed Cycles          cycle      2855744
    Average L2 Active Cycles         cycle     14219.42
    Total L2 Elapsed Cycles          cycle       736992
    Average SM Active Cycles         cycle     11865.55
    Total SM Elapsed Cycles          cycle      2855744
    Average SMSP Active Cycles       cycle     11240.63
    Total SMSP Elapsed Cycles        cycle     11422976
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 22.29%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 41.91% above the average, while the minimum instance value is 68.59% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 22.12%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 43.90% above the average, while the minimum instance value is 71.53% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 22.29%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 41.91% above the average, while the minimum instance value is 68.59% below the      
          average.                                                                                                      

  MultKeyAccum_8(Params *, int, int, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long **) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.23
    SM Frequency            cycle/nsecond         2.23
    Elapsed Cycles                  cycle       180544
    Memory Throughput                   %        92.37
    DRAM Throughput                     %        92.37
    Duration                      usecond        80.90
    L1/TEX Cache Throughput             %         9.91
    L2 Cache Throughput                 %        38.70
    SM Active Cycles                cycle    174429.07
    Compute (SM) Throughput             %        10.15
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                2.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           12
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        87.37
    Achieved Active Warps Per SM           warp        41.94
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 12.63%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (87.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    764714.67
    Total DRAM Elapsed Cycles        cycle      9934848
    Average L1 Active Cycles         cycle    174429.07
    Total L1 Elapsed Cycles          cycle     22122836
    Average L2 Active Cycles         cycle    146627.64
    Total L2 Elapsed Cycles          cycle      5715144
    Average SM Active Cycles         cycle    174429.07
    Total SM Elapsed Cycles          cycle     22122836
    Average SMSP Active Cycles       cycle    166320.22
    Total SMSP Elapsed Cycles        cycle     88491344
    -------------------------- ----------- ------------

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.23
    SM Frequency            cycle/nsecond         2.19
    Elapsed Cycles                  cycle        43322
    Memory Throughput                   %        73.13
    DRAM Throughput                     %        73.13
    Duration                      usecond        19.71
    L1/TEX Cache Throughput             %        33.16
    L2 Cache Throughput                 %        31.98
    SM Active Cycles                cycle     34155.73
    Compute (SM) Throughput             %        18.80
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        36.94
    Achieved Active Warps Per SM           warp        17.73
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.12%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (36.9%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    147406.67
    Total DRAM Elapsed Cycles        cycle      2418688
    Average L1 Active Cycles         cycle     34155.73
    Total L1 Elapsed Cycles          cycle      5427534
    Average L2 Active Cycles         cycle     32434.11
    Total L2 Elapsed Cycles          cycle      1379844
    Average SM Active Cycles         cycle     34155.73
    Total SM Elapsed Cycles          cycle      5427534
    Average SMSP Active Cycles       cycle     34157.61
    Total SMSP Elapsed Cycles        cycle     21710136
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 12.95%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 16.07% above the average, while the minimum instance value is 29.65% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.76%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 14.59% above the average, while the minimum instance value is 28.87% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.95%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 16.07% above the average, while the minimum instance value is 29.65% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.24
    SM Frequency            cycle/nsecond         2.20
    Elapsed Cycles                  cycle        43643
    Memory Throughput                   %        72.92
    DRAM Throughput                     %        72.92
    Duration                      usecond        19.74
    L1/TEX Cache Throughput             %        32.92
    L2 Cache Throughput                 %        31.82
    SM Active Cycles                cycle     34383.70
    Compute (SM) Throughput             %        18.51
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        36.29
    Achieved Active Warps Per SM           warp        17.42
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.41%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (36.3%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    147405.33
    Total DRAM Elapsed Cycles        cycle      2425856
    Average L1 Active Cycles         cycle     34383.70
    Total L1 Elapsed Cycles          cycle      5510714
    Average L2 Active Cycles         cycle     32263.44
    Total L2 Elapsed Cycles          cycle      1386684
    Average SM Active Cycles         cycle     34383.70
    Total SM Elapsed Cycles          cycle      5510714
    Average SMSP Active Cycles       cycle     34140.68
    Total SMSP Elapsed Cycles        cycle     22042856
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 12.4%                                                                                           
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 15.53% above the average, while the minimum instance value is 30.48% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.24%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 14.17% above the average, while the minimum instance value is 27.28% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.4%                                                                                           
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 15.53% above the average, while the minimum instance value is 30.48% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.23
    SM Frequency            cycle/nsecond         2.23
    Elapsed Cycles                  cycle        23330
    Memory Throughput                   %        46.13
    DRAM Throughput                     %        46.13
    Duration                      usecond        10.46
    L1/TEX Cache Throughput             %        33.84
    L2 Cache Throughput                 %        19.92
    SM Active Cycles                cycle     11899.47
    Compute (SM) Throughput             %        14.16
    ----------------------- ------------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        25.53
    Achieved Active Warps Per SM           warp        12.26
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 48.93%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (25.5%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     49358.67
    Total DRAM Elapsed Cycles        cycle      1284096
    Average L1 Active Cycles         cycle     11899.47
    Total L1 Elapsed Cycles          cycle      2831826
    Average L2 Active Cycles         cycle     14345.94
    Total L2 Elapsed Cycles          cycle       738324
    Average SM Active Cycles         cycle     11899.47
    Total SM Elapsed Cycles          cycle      2831826
    Average SMSP Active Cycles       cycle     11297.16
    Total SMSP Elapsed Cycles        cycle     11327304
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 22.66%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 42.12% above the average, while the minimum instance value is 67.33% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 21.7%                                                                                           
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 42.50% above the average, while the minimum instance value is 71.40% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 22.66%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 42.12% above the average, while the minimum instance value is 67.33% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.20
    SM Frequency            cycle/nsecond         2.21
    Elapsed Cycles                  cycle        32212
    Memory Throughput                   %        66.42
    DRAM Throughput                     %        66.42
    Duration                      usecond        14.53
    L1/TEX Cache Throughput             %        28.40
    L2 Cache Throughput                 %        28.79
    SM Active Cycles                cycle     26850.82
    Compute (SM) Throughput             %        17.46
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        25.02
    Achieved Active Warps Per SM           warp        12.01
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 49.95%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (25.0%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     98389.33
    Total DRAM Elapsed Cycles        cycle      1777664
    Average L1 Active Cycles         cycle     26850.82
    Total L1 Elapsed Cycles          cycle      4008158
    Average L2 Active Cycles         cycle     22609.33
    Total L2 Elapsed Cycles          cycle      1021608
    Average SM Active Cycles         cycle     26850.82
    Total SM Elapsed Cycles          cycle      4008158
    Average SMSP Active Cycles       cycle     26625.06
    Total SMSP Elapsed Cycles        cycle     16032632
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.408%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 8.64% above the average, while the minimum instance value is 24.96% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.987%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 8.22% above the average, while the minimum instance value is 22.68% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.408%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 8.64% above the average, while the minimum instance value is 24.96% below the       
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.20
    SM Frequency            cycle/nsecond         2.18
    Elapsed Cycles                  cycle        42937
    Memory Throughput                   %        73.76
    DRAM Throughput                     %        73.76
    Duration                      usecond        19.58
    L1/TEX Cache Throughput             %        33.13
    L2 Cache Throughput                 %        32.24
    SM Active Cycles                cycle     34157.92
    Compute (SM) Throughput             %        18.22
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        37.18
    Achieved Active Warps Per SM           warp        17.85
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.63%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (37.2%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       147408
    Total DRAM Elapsed Cycles        cycle      2398208
    Average L1 Active Cycles         cycle     34157.92
    Total L1 Elapsed Cycles          cycle      5598804
    Average L2 Active Cycles         cycle     32763.06
    Total L2 Elapsed Cycles          cycle      1368468
    Average SM Active Cycles         cycle     34157.92
    Total SM Elapsed Cycles          cycle      5598804
    Average SMSP Active Cycles       cycle     34327.56
    Total SMSP Elapsed Cycles        cycle     22395216
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.56%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 14.81% above the average, while the minimum instance value is 31.89% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.41%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 15.81% above the average, while the minimum instance value is 30.03% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.56%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 14.81% above the average, while the minimum instance value is 31.89% below the      
          average.                                                                                                      

  MultKeyAccum_8(Params *, int, int, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long **) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.24
    SM Frequency            cycle/nsecond         2.23
    Elapsed Cycles                  cycle       181056
    Memory Throughput                   %        92.17
    DRAM Throughput                     %        92.17
    Duration                      usecond        81.09
    L1/TEX Cache Throughput             %         9.90
    L2 Cache Throughput                 %        38.59
    SM Active Cycles                cycle    174647.84
    Compute (SM) Throughput             %        10.17
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                2.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           12
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        86.98
    Achieved Active Warps Per SM           warp        41.75
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 13.02%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (87.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    765065.33
    Total DRAM Elapsed Cycles        cycle      9960448
    Average L1 Active Cycles         cycle    174647.84
    Total L1 Elapsed Cycles          cycle     22074590
    Average L2 Active Cycles         cycle    146721.42
    Total L2 Elapsed Cycles          cycle      5731524
    Average SM Active Cycles         cycle    174647.84
    Total SM Elapsed Cycles          cycle     22074590
    Average SMSP Active Cycles       cycle    165898.01
    Total SMSP Elapsed Cycles        cycle     88298360
    -------------------------- ----------- ------------

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.20
    SM Frequency            cycle/nsecond         2.21
    Elapsed Cycles                  cycle        32444
    Memory Throughput                   %        65.81
    DRAM Throughput                     %        65.81
    Duration                      usecond        14.66
    L1/TEX Cache Throughput             %        27.99
    L2 Cache Throughput                 %        28.57
    SM Active Cycles                cycle     27257.47
    Compute (SM) Throughput             %        17.27
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        25.14
    Achieved Active Warps Per SM           warp        12.07
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 49.72%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (25.1%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     98381.33
    Total DRAM Elapsed Cycles        cycle      1794048
    Average L1 Active Cycles         cycle     27257.47
    Total L1 Elapsed Cycles          cycle      4052504
    Average L2 Active Cycles         cycle     22932.64
    Total L2 Elapsed Cycles          cycle      1029492
    Average SM Active Cycles         cycle     27257.47
    Total SM Elapsed Cycles          cycle      4052504
    Average SMSP Active Cycles       cycle     27156.38
    Total SMSP Elapsed Cycles        cycle     16210016
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.789%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 7.89% above the average, while the minimum instance value is 19.79% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.208%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 6.07% above the average, while the minimum instance value is 13.24% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.789%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 7.89% above the average, while the minimum instance value is 19.79% below the       
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.19
    SM Frequency            cycle/nsecond         2.22
    Elapsed Cycles                  cycle        22672
    Memory Throughput                   %        47.45
    DRAM Throughput                     %        47.45
    Duration                      usecond        10.21
    L1/TEX Cache Throughput             %        34.40
    L2 Cache Throughput                 %        20.47
    SM Active Cycles                cycle     11704.43
    Compute (SM) Throughput             %        13.64
    ----------------------- ------------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        25.99
    Achieved Active Warps Per SM           warp        12.47
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 48.03%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (26.0%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     49357.33
    Total DRAM Elapsed Cycles        cycle      1248256
    Average L1 Active Cycles         cycle     11704.43
    Total L1 Elapsed Cycles          cycle      2939572
    Average L2 Active Cycles         cycle     14291.33
    Total L2 Elapsed Cycles          cycle       718452
    Average SM Active Cycles         cycle     11704.43
    Total SM Elapsed Cycles          cycle      2939572
    Average SMSP Active Cycles       cycle     11332.88
    Total SMSP Elapsed Cycles        cycle     11758288
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 20.86%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 40.93% above the average, while the minimum instance value is 68.64% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 21.2%                                                                                           
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 42.95% above the average, while the minimum instance value is 71.90% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.86%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 40.93% above the average, while the minimum instance value is 68.64% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.22
    SM Frequency            cycle/nsecond         2.21
    Elapsed Cycles                  cycle        43883
    Memory Throughput                   %        72.95
    DRAM Throughput                     %        72.95
    Duration                      usecond        19.78
    L1/TEX Cache Throughput             %        32.56
    L2 Cache Throughput                 %        31.70
    SM Active Cycles                cycle     34777.40
    Compute (SM) Throughput             %        18.72
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        36.25
    Achieved Active Warps Per SM           warp        17.40
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.51%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (36.2%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       147408
    Total DRAM Elapsed Cycles        cycle      2424832
    Average L1 Active Cycles         cycle     34777.40
    Total L1 Elapsed Cycles          cycle      5450366
    Average L2 Active Cycles         cycle     32406.50
    Total L2 Elapsed Cycles          cycle      1391688
    Average SM Active Cycles         cycle     34777.40
    Total SM Elapsed Cycles          cycle      5450366
    Average SMSP Active Cycles       cycle     34163.21
    Total SMSP Elapsed Cycles        cycle     21801464
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 12.72%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 15.57% above the average, while the minimum instance value is 31.00% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.05%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 15.02% above the average, while the minimum instance value is 25.03% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.72%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 15.57% above the average, while the minimum instance value is 31.00% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.22
    SM Frequency            cycle/nsecond         2.19
    Elapsed Cycles                  cycle        43169
    Memory Throughput                   %        73.42
    DRAM Throughput                     %        73.42
    Duration                      usecond        19.65
    L1/TEX Cache Throughput             %        33.49
    L2 Cache Throughput                 %        32.11
    SM Active Cycles                cycle     33805.39
    Compute (SM) Throughput             %        18.35
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        37.24
    Achieved Active Warps Per SM           warp        17.87
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.53%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (37.2%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    147413.33
    Total DRAM Elapsed Cycles        cycle      2409472
    Average L1 Active Cycles         cycle     33805.39
    Total L1 Elapsed Cycles          cycle      5560160
    Average L2 Active Cycles         cycle     32489.47
    Total L2 Elapsed Cycles          cycle      1374228
    Average SM Active Cycles         cycle     33805.39
    Total SM Elapsed Cycles          cycle      5560160
    Average SMSP Active Cycles       cycle     34192.87
    Total SMSP Elapsed Cycles        cycle     22240640
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 12.36%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 15.88% above the average, while the minimum instance value is 30.55% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 13.55%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 17.22% above the average, while the minimum instance value is 29.50% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.36%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 15.88% above the average, while the minimum instance value is 30.55% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.21
    SM Frequency            cycle/nsecond         2.18
    Elapsed Cycles                  cycle        43038
    Memory Throughput                   %        73.63
    DRAM Throughput                     %        73.63
    Duration                      usecond        19.62
    L1/TEX Cache Throughput             %        33.18
    L2 Cache Throughput                 %        32.18
    SM Active Cycles                cycle     34129.08
    Compute (SM) Throughput             %        18.39
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        36.34
    Achieved Active Warps Per SM           warp        17.44
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.32%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (36.3%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    147406.67
    Total DRAM Elapsed Cycles        cycle      2402304
    Average L1 Active Cycles         cycle     34129.08
    Total L1 Elapsed Cycles          cycle      5548368
    Average L2 Active Cycles         cycle     32260.58
    Total L2 Elapsed Cycles          cycle      1371276
    Average SM Active Cycles         cycle     34129.08
    Total SM Elapsed Cycles          cycle      5548368
    Average SMSP Active Cycles       cycle     33819.14
    Total SMSP Elapsed Cycles        cycle     22193472
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.7%                                                                                           
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 14.85% above the average, while the minimum instance value is 29.05% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 13.43%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 17.21% above the average, while the minimum instance value is 27.99% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.7%                                                                                           
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 14.85% above the average, while the minimum instance value is 29.05% below the      
          average.                                                                                                      

  MultKeyAccum_8(Params *, int, int, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long **) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.24
    SM Frequency            cycle/nsecond         2.23
    Elapsed Cycles                  cycle       180123
    Memory Throughput                   %        92.66
    DRAM Throughput                     %        92.66
    Duration                      usecond        80.67
    L1/TEX Cache Throughput             %         9.96
    L2 Cache Throughput                 %        38.79
    SM Active Cycles                cycle    173713.91
    Compute (SM) Throughput             %        10.22
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                2.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           12
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        87.71
    Achieved Active Warps Per SM           warp        42.10
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 12.29%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (87.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    765093.33
    Total DRAM Elapsed Cycles        cycle      9908224
    Average L1 Active Cycles         cycle    173713.91
    Total L1 Elapsed Cycles          cycle     21960946
    Average L2 Active Cycles         cycle    147086.53
    Total L2 Elapsed Cycles          cycle      5701788
    Average SM Active Cycles         cycle    173713.91
    Total SM Elapsed Cycles          cycle     21960946
    Average SMSP Active Cycles       cycle    166570.96
    Total SMSP Elapsed Cycles        cycle     87843784
    -------------------------- ----------- ------------

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.21
    SM Frequency            cycle/nsecond         2.21
    Elapsed Cycles                  cycle        32187
    Memory Throughput                   %        66.30
    DRAM Throughput                     %        66.30
    Duration                      usecond        14.53
    L1/TEX Cache Throughput             %        28.30
    L2 Cache Throughput                 %        28.80
    SM Active Cycles                cycle     26953.91
    Compute (SM) Throughput             %        17.04
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        25.14
    Achieved Active Warps Per SM           warp        12.07
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 49.71%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (25.1%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     98382.67
    Total DRAM Elapsed Cycles        cycle      1780736
    Average L1 Active Cycles         cycle     26953.91
    Total L1 Elapsed Cycles          cycle      4106304
    Average L2 Active Cycles         cycle     22913.50
    Total L2 Elapsed Cycles          cycle      1021500
    Average SM Active Cycles         cycle     26953.91
    Total SM Elapsed Cycles          cycle      4106304
    Average SMSP Active Cycles       cycle     26763.18
    Total SMSP Elapsed Cycles        cycle     16425216
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.964%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 8.29% above the average, while the minimum instance value is 21.28% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.419%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 7.69% above the average, while the minimum instance value is 22.64% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.964%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 8.29% above the average, while the minimum instance value is 21.28% below the       
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.22
    SM Frequency            cycle/nsecond         2.22
    Elapsed Cycles                  cycle        44071
    Memory Throughput                   %        72.68
    DRAM Throughput                     %        72.68
    Duration                      usecond        19.84
    L1/TEX Cache Throughput             %        32.52
    L2 Cache Throughput                 %        31.59
    SM Active Cycles                cycle     34816.85
    Compute (SM) Throughput             %        18.60
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        36.43
    Achieved Active Warps Per SM           warp        17.49
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.14%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (36.4%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    147417.33
    Total DRAM Elapsed Cycles        cycle      2434048
    Average L1 Active Cycles         cycle     34816.85
    Total L1 Elapsed Cycles          cycle      5484762
    Average L2 Active Cycles         cycle     32645.78
    Total L2 Elapsed Cycles          cycle      1396872
    Average SM Active Cycles         cycle     34816.85
    Total SM Elapsed Cycles          cycle      5484762
    Average SMSP Active Cycles       cycle     34485.13
    Total SMSP Elapsed Cycles        cycle     21939048
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 12.79%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 15.74% above the average, while the minimum instance value is 31.43% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.69%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 14.52% above the average, while the minimum instance value is 31.79% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.79%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 15.74% above the average, while the minimum instance value is 31.43% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.22
    SM Frequency            cycle/nsecond         2.18
    Elapsed Cycles                  cycle        42991
    Memory Throughput                   %        73.54
    DRAM Throughput                     %        73.54
    Duration                      usecond        19.62
    L1/TEX Cache Throughput             %        33.14
    L2 Cache Throughput                 %        32.18
    SM Active Cycles                cycle     34157.01
    Compute (SM) Throughput             %        18.36
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        36.70
    Achieved Active Warps Per SM           warp        17.61
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.61%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (36.7%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    147405.33
    Total DRAM Elapsed Cycles        cycle      2405376
    Average L1 Active Cycles         cycle     34157.01
    Total L1 Elapsed Cycles          cycle      5555130
    Average L2 Active Cycles         cycle     32270.75
    Total L2 Elapsed Cycles          cycle      1371348
    Average SM Active Cycles         cycle     34157.01
    Total SM Elapsed Cycles          cycle      5555130
    Average SMSP Active Cycles       cycle     34353.49
    Total SMSP Elapsed Cycles        cycle     22220520
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.64%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 14.79% above the average, while the minimum instance value is 28.03% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.98%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 15.14% above the average, while the minimum instance value is 31.10% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.64%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 14.79% above the average, while the minimum instance value is 28.03% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.22
    SM Frequency            cycle/nsecond         2.18
    Elapsed Cycles                  cycle        43372
    Memory Throughput                   %        72.95
    DRAM Throughput                     %        72.95
    Duration                      usecond        19.78
    L1/TEX Cache Throughput             %        33.05
    L2 Cache Throughput                 %        31.92
    SM Active Cycles                cycle     34239.57
    Compute (SM) Throughput             %        18.50
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        36.54
    Achieved Active Warps Per SM           warp        17.54
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.92%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (36.5%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    147406.67
    Total DRAM Elapsed Cycles        cycle      2424832
    Average L1 Active Cycles         cycle     34239.57
    Total L1 Elapsed Cycles          cycle      5513054
    Average L2 Active Cycles         cycle     32772.56
    Total L2 Elapsed Cycles          cycle      1382508
    Average SM Active Cycles         cycle     34239.57
    Total SM Elapsed Cycles          cycle      5513054
    Average SMSP Active Cycles       cycle     34527.43
    Total SMSP Elapsed Cycles        cycle     22052216
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 12.48%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 15.70% above the average, while the minimum instance value is 33.67% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.01%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 14.98% above the average, while the minimum instance value is 30.46% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.48%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 15.70% above the average, while the minimum instance value is 33.67% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.24
    SM Frequency            cycle/nsecond         2.18
    Elapsed Cycles                  cycle        42909
    Memory Throughput                   %        73.63
    DRAM Throughput                     %        73.63
    Duration                      usecond        19.55
    L1/TEX Cache Throughput             %        33.01
    L2 Cache Throughput                 %        32.24
    SM Active Cycles                cycle     34293.58
    Compute (SM) Throughput             %        18.54
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        36.36
    Achieved Active Warps Per SM           warp        17.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.29%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (36.4%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    147406.67
    Total DRAM Elapsed Cycles        cycle      2402304
    Average L1 Active Cycles         cycle     34293.58
    Total L1 Elapsed Cycles          cycle      5503320
    Average L2 Active Cycles         cycle     32544.81
    Total L2 Elapsed Cycles          cycle      1368792
    Average SM Active Cycles         cycle     34293.58
    Total SM Elapsed Cycles          cycle      5503320
    Average SMSP Active Cycles       cycle     34644.79
    Total SMSP Elapsed Cycles        cycle     22013280
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.53%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 14.46% above the average, while the minimum instance value is 31.02% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.32%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 14.05% above the average, while the minimum instance value is 30.42% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.53%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 14.46% above the average, while the minimum instance value is 31.02% below the      
          average.                                                                                                      

  MultKeyAccum_8(Params *, int, int, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long **) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.23
    SM Frequency            cycle/nsecond         2.23
    Elapsed Cycles                  cycle       180015
    Memory Throughput                   %        92.89
    DRAM Throughput                     %        92.89
    Duration                      usecond        80.61
    L1/TEX Cache Throughput             %         9.98
    L2 Cache Throughput                 %        38.82
    SM Active Cycles                cycle    173316.03
    Compute (SM) Throughput             %        10.16
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                2.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           12
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        87.92
    Achieved Active Warps Per SM           warp        42.20
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 12.08%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (87.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    766197.33
    Total DRAM Elapsed Cycles        cycle      9897984
    Average L1 Active Cycles         cycle    173316.03
    Total L1 Elapsed Cycles          cycle     22094072
    Average L2 Active Cycles         cycle    146516.06
    Total L2 Elapsed Cycles          cycle      5698152
    Average SM Active Cycles         cycle    173316.03
    Total SM Elapsed Cycles          cycle     22094072
    Average SMSP Active Cycles       cycle    165123.31
    Total SMSP Elapsed Cycles        cycle     88376288
    -------------------------- ----------- ------------

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.23
    SM Frequency            cycle/nsecond         2.22
    Elapsed Cycles                  cycle        43849
    Memory Throughput                   %        73.01
    DRAM Throughput                     %        73.01
    Duration                      usecond        19.74
    L1/TEX Cache Throughput             %        32.51
    L2 Cache Throughput                 %        31.73
    SM Active Cycles                cycle     34829.01
    Compute (SM) Throughput             %        18.67
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        36.55
    Achieved Active Warps Per SM           warp        17.55
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.89%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (36.6%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    147405.33
    Total DRAM Elapsed Cycles        cycle      2422784
    Average L1 Active Cycles         cycle     34829.01
    Total L1 Elapsed Cycles          cycle      5463940
    Average L2 Active Cycles         cycle     32418.33
    Total L2 Elapsed Cycles          cycle      1390788
    Average SM Active Cycles         cycle     34829.01
    Total SM Elapsed Cycles          cycle      5463940
    Average SMSP Active Cycles       cycle     34139.45
    Total SMSP Elapsed Cycles        cycle     21855760
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 12.66%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 15.52% above the average, while the minimum instance value is 30.48% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.26%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 15.33% above the average, while the minimum instance value is 30.53% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.66%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 15.52% above the average, while the minimum instance value is 30.48% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.21
    SM Frequency            cycle/nsecond         2.18
    Elapsed Cycles                  cycle        43043
    Memory Throughput                   %        73.60
    DRAM Throughput                     %        73.60
    Duration                      usecond        19.62
    L1/TEX Cache Throughput             %        33.01
    L2 Cache Throughput                 %        32.19
    SM Active Cycles                cycle     34302.99
    Compute (SM) Throughput             %        18.74
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        37.03
    Achieved Active Warps Per SM           warp        17.78
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.93%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (37.0%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    147406.67
    Total DRAM Elapsed Cycles        cycle      2403328
    Average L1 Active Cycles         cycle     34302.99
    Total L1 Elapsed Cycles          cycle      5443510
    Average L2 Active Cycles         cycle     32716.22
    Total L2 Elapsed Cycles          cycle      1370700
    Average SM Active Cycles         cycle     34302.99
    Total SM Elapsed Cycles          cycle      5443510
    Average SMSP Active Cycles       cycle     34397.78
    Total SMSP Elapsed Cycles        cycle     21774040
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.77%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 14.59% above the average, while the minimum instance value is 28.37% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.29%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 15.19% above the average, while the minimum instance value is 28.40% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.77%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 14.59% above the average, while the minimum instance value is 28.37% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.22
    SM Frequency            cycle/nsecond         2.19
    Elapsed Cycles                  cycle        43652
    Memory Throughput                   %        72.58
    DRAM Throughput                     %        72.58
    Duration                      usecond        19.87
    L1/TEX Cache Throughput             %        32.95
    L2 Cache Throughput                 %        31.72
    SM Active Cycles                cycle     34358.92
    Compute (SM) Throughput             %        18.64
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        36.13
    Achieved Active Warps Per SM           warp        17.34
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.73%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (36.1%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    147406.67
    Total DRAM Elapsed Cycles        cycle      2437120
    Average L1 Active Cycles         cycle     34358.92
    Total L1 Elapsed Cycles          cycle      5472976
    Average L2 Active Cycles         cycle     32523.81
    Total L2 Elapsed Cycles          cycle      1391112
    Average SM Active Cycles         cycle     34358.92
    Total SM Elapsed Cycles          cycle      5472976
    Average SMSP Active Cycles       cycle     34505.87
    Total SMSP Elapsed Cycles        cycle     21891904
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 12.76%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 15.88% above the average, while the minimum instance value is 31.05% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.5%                                                                                           
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 15.49% above the average, while the minimum instance value is 32.40% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.76%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 15.88% above the average, while the minimum instance value is 31.05% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.21
    SM Frequency            cycle/nsecond         2.18
    Elapsed Cycles                  cycle        31616
    Memory Throughput                   %        66.80
    DRAM Throughput                     %        66.80
    Duration                      usecond        14.43
    L1/TEX Cache Throughput             %        28.18
    L2 Cache Throughput                 %        29.19
    SM Active Cycles                cycle     27072.41
    Compute (SM) Throughput             %        17.13
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        24.79
    Achieved Active Warps Per SM           warp        11.90
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50.43%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (24.8%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     98381.33
    Total DRAM Elapsed Cycles        cycle      1767424
    Average L1 Active Cycles         cycle     27072.41
    Total L1 Elapsed Cycles          cycle      4085776
    Average L2 Active Cycles         cycle        22954
    Total L2 Elapsed Cycles          cycle      1007784
    Average SM Active Cycles         cycle     27072.41
    Total SM Elapsed Cycles          cycle      4085776
    Average SMSP Active Cycles       cycle     26929.43
    Total SMSP Elapsed Cycles        cycle     16343104
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.517%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 6.50% above the average, while the minimum instance value is 18.32% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.314%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 7.48% above the average, while the minimum instance value is 17.27% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.517%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 6.50% above the average, while the minimum instance value is 18.32% below the       
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.22
    SM Frequency            cycle/nsecond         2.22
    Elapsed Cycles                  cycle        23465
    Memory Throughput                   %        45.87
    DRAM Throughput                     %        45.87
    Duration                      usecond        10.53
    L1/TEX Cache Throughput             %        33.87
    L2 Cache Throughput                 %        19.81
    SM Active Cycles                cycle     11868.84
    Compute (SM) Throughput             %        13.77
    ----------------------- ------------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        25.53
    Achieved Active Warps Per SM           warp        12.26
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 48.94%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (25.5%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     49357.33
    Total DRAM Elapsed Cycles        cycle      1291264
    Average L1 Active Cycles         cycle     11868.84
    Total L1 Elapsed Cycles          cycle      2909790
    Average L2 Active Cycles         cycle     14217.42
    Total L2 Elapsed Cycles          cycle       742644
    Average SM Active Cycles         cycle     11868.84
    Total SM Elapsed Cycles          cycle      2909790
    Average SMSP Active Cycles       cycle     11198.38
    Total SMSP Elapsed Cycles        cycle     11639160
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 22.38%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 42.86% above the average, while the minimum instance value is 68.33% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.97%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 42.57% above the average, while the minimum instance value is 71.51% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 22.38%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 42.86% above the average, while the minimum instance value is 68.33% below the      
          average.                                                                                                      

  MultKeyAccum_8(Params *, int, int, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long **) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.24
    SM Frequency            cycle/nsecond         2.23
    Elapsed Cycles                  cycle       180591
    Memory Throughput                   %        92.39
    DRAM Throughput                     %        92.39
    Duration                      usecond        80.83
    L1/TEX Cache Throughput             %         9.88
    L2 Cache Throughput                 %        38.70
    SM Active Cycles                cycle    175027.16
    Compute (SM) Throughput             %        10.17
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                2.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           12
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        86.66
    Achieved Active Warps Per SM           warp        41.60
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 13.34%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (86.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    764506.67
    Total DRAM Elapsed Cycles        cycle      9929728
    Average L1 Active Cycles         cycle    175027.16
    Total L1 Elapsed Cycles          cycle     22070630
    Average L2 Active Cycles         cycle    147190.53
    Total L2 Elapsed Cycles          cycle      5715864
    Average SM Active Cycles         cycle    175027.16
    Total SM Elapsed Cycles          cycle     22070630
    Average SMSP Active Cycles       cycle    166353.21
    Total SMSP Elapsed Cycles        cycle     88282520
    -------------------------- ----------- ------------

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.21
    SM Frequency            cycle/nsecond         2.18
    Elapsed Cycles                  cycle        43090
    Memory Throughput                   %        73.44
    DRAM Throughput                     %        73.44
    Duration                      usecond        19.65
    L1/TEX Cache Throughput             %        33.10
    L2 Cache Throughput                 %        32.13
    SM Active Cycles                cycle     34189.70
    Compute (SM) Throughput             %        18.88
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        37.05
    Achieved Active Warps Per SM           warp        17.78
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.91%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (37.0%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       147404
    Total DRAM Elapsed Cycles        cycle      2408448
    Average L1 Active Cycles         cycle     34189.70
    Total L1 Elapsed Cycles          cycle      5403470
    Average L2 Active Cycles         cycle     32118.03
    Total L2 Elapsed Cycles          cycle      1373400
    Average SM Active Cycles         cycle     34189.70
    Total SM Elapsed Cycles          cycle      5403470
    Average SMSP Active Cycles       cycle     33818.18
    Total SMSP Elapsed Cycles        cycle     21613880
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 12.26%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 15.14% above the average, while the minimum instance value is 31.35% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.24%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 15.28% above the average, while the minimum instance value is 31.02% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.26%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 15.14% above the average, while the minimum instance value is 31.35% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.21
    SM Frequency            cycle/nsecond         2.19
    Elapsed Cycles                  cycle        43096
    Memory Throughput                   %        73.57
    DRAM Throughput                     %        73.57
    Duration                      usecond        19.62
    L1/TEX Cache Throughput             %        33.22
    L2 Cache Throughput                 %        32.14
    SM Active Cycles                cycle     34094.57
    Compute (SM) Throughput             %        18.22
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        36.67
    Achieved Active Warps Per SM           warp        17.60
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.65%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (36.7%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    147406.67
    Total DRAM Elapsed Cycles        cycle      2404352
    Average L1 Active Cycles         cycle     34094.57
    Total L1 Elapsed Cycles          cycle      5599246
    Average L2 Active Cycles         cycle     32349.44
    Total L2 Elapsed Cycles          cycle      1372896
    Average SM Active Cycles         cycle     34094.57
    Total SM Elapsed Cycles          cycle      5599246
    Average SMSP Active Cycles       cycle     34115.35
    Total SMSP Elapsed Cycles        cycle     22396984
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.63%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 14.92% above the average, while the minimum instance value is 31.81% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.05%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 15.45% above the average, while the minimum instance value is 29.89% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.63%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 14.92% above the average, while the minimum instance value is 31.81% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.19
    SM Frequency            cycle/nsecond         2.19
    Elapsed Cycles                  cycle        22679
    Memory Throughput                   %        47.02
    DRAM Throughput                     %        47.02
    Duration                      usecond        10.30
    L1/TEX Cache Throughput             %        34.28
    L2 Cache Throughput                 %        20.40
    SM Active Cycles                cycle     11719.39
    Compute (SM) Throughput             %        13.78
    ----------------------- ------------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        26.38
    Achieved Active Warps Per SM           warp        12.66
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 47.25%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (26.4%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     49357.33
    Total DRAM Elapsed Cycles        cycle      1259520
    Average L1 Active Cycles         cycle     11719.39
    Total L1 Elapsed Cycles          cycle      2910712
    Average L2 Active Cycles         cycle     14260.81
    Total L2 Elapsed Cycles          cycle       721008
    Average SM Active Cycles         cycle     11719.39
    Total SM Elapsed Cycles          cycle      2910712
    Average SMSP Active Cycles       cycle     11355.57
    Total SMSP Elapsed Cycles        cycle     11642848
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 21.43%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 41.58% above the average, while the minimum instance value is 66.93% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.91%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 41.86% above the average, while the minimum instance value is 70.53% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 21.43%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 41.58% above the average, while the minimum instance value is 66.93% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.20
    SM Frequency            cycle/nsecond         2.22
    Elapsed Cycles                  cycle        31947
    Memory Throughput                   %        67.15
    DRAM Throughput                     %        67.15
    Duration                      usecond        14.37
    L1/TEX Cache Throughput             %        28.04
    L2 Cache Throughput                 %        29.06
    SM Active Cycles                cycle     27208.44
    Compute (SM) Throughput             %        17.37
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        25.04
    Achieved Active Warps Per SM           warp        12.02
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 49.93%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (25.0%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     98381.33
    Total DRAM Elapsed Cycles        cycle      1758208
    Average L1 Active Cycles         cycle     27208.44
    Total L1 Elapsed Cycles          cycle      4030116
    Average L2 Active Cycles         cycle     22869.06
    Total L2 Elapsed Cycles          cycle      1012392
    Average SM Active Cycles         cycle     27208.44
    Total SM Elapsed Cycles          cycle      4030116
    Average SMSP Active Cycles       cycle     26880.86
    Total SMSP Elapsed Cycles        cycle     16120464
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.158%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 7.13% above the average, while the minimum instance value is 19.30% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.87%                                                                                           
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 8.05% above the average, while the minimum instance value is 17.84% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.158%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 7.13% above the average, while the minimum instance value is 19.30% below the       
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.21
    SM Frequency            cycle/nsecond         2.21
    Elapsed Cycles                  cycle        43631
    Memory Throughput                   %        73.35
    DRAM Throughput                     %        73.35
    Duration                      usecond        19.68
    L1/TEX Cache Throughput             %        32.54
    L2 Cache Throughput                 %        31.87
    SM Active Cycles                cycle     34801.41
    Compute (SM) Throughput             %        18.16
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        36.21
    Achieved Active Warps Per SM           warp        17.38
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.57%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (36.2%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    147406.67
    Total DRAM Elapsed Cycles        cycle      2411520
    Average L1 Active Cycles         cycle     34801.41
    Total L1 Elapsed Cycles          cycle      5616994
    Average L2 Active Cycles         cycle     32287.92
    Total L2 Elapsed Cycles          cycle      1384416
    Average SM Active Cycles         cycle     34801.41
    Total SM Elapsed Cycles          cycle      5616994
    Average SMSP Active Cycles       cycle     34114.09
    Total SMSP Elapsed Cycles        cycle     22467976
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.78%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 14.86% above the average, while the minimum instance value is 31.84% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 10.79%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 13.88% above the average, while the minimum instance value is 28.23% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.78%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 14.86% above the average, while the minimum instance value is 31.84% below the      
          average.                                                                                                      

  MultKeyAccum_8(Params *, int, int, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long **) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.23
    SM Frequency            cycle/nsecond         2.23
    Elapsed Cycles                  cycle       181732
    Memory Throughput                   %        91.84
    DRAM Throughput                     %        91.84
    Duration                      usecond        81.44
    L1/TEX Cache Throughput             %         9.90
    L2 Cache Throughput                 %        38.45
    SM Active Cycles                cycle    174610.64
    Compute (SM) Throughput             %        10.17
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                2.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           12
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        87.60
    Achieved Active Warps Per SM           warp        42.05
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 12.4%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (87.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       765376
    Total DRAM Elapsed Cycles        cycle     10000384
    Average L1 Active Cycles         cycle    174610.64
    Total L1 Elapsed Cycles          cycle     22078458
    Average L2 Active Cycles         cycle    146602.97
    Total L2 Elapsed Cycles          cycle      5753124
    Average SM Active Cycles         cycle    174610.64
    Total SM Elapsed Cycles          cycle     22078458
    Average SMSP Active Cycles       cycle    165270.67
    Total SMSP Elapsed Cycles        cycle     88313832
    -------------------------- ----------- ------------

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.20
    SM Frequency            cycle/nsecond         2.20
    Elapsed Cycles                  cycle        32509
    Memory Throughput                   %        65.36
    DRAM Throughput                     %        65.36
    Duration                      usecond        14.75
    L1/TEX Cache Throughput             %        28.25
    L2 Cache Throughput                 %        28.47
    SM Active Cycles                cycle     26999.84
    Compute (SM) Throughput             %        17.15
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        24.99
    Achieved Active Warps Per SM           warp        12.00
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50.01%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (25.0%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        98380
    Total DRAM Elapsed Cycles        cycle      1806336
    Average L1 Active Cycles         cycle     26999.84
    Total L1 Elapsed Cycles          cycle      4081778
    Average L2 Active Cycles         cycle     22994.78
    Total L2 Elapsed Cycles          cycle      1033416
    Average SM Active Cycles         cycle     26999.84
    Total SM Elapsed Cycles          cycle      4081778
    Average SMSP Active Cycles       cycle     26749.87
    Total SMSP Elapsed Cycles        cycle     16327112
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.92%                                                                                           
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 9.35% above the average, while the minimum instance value is 16.79% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.912%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 8.24% above the average, while the minimum instance value is 21.91% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.92%                                                                                           
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 9.35% above the average, while the minimum instance value is 16.79% below the       
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.18
    SM Frequency            cycle/nsecond         2.22
    Elapsed Cycles                  cycle        23031
    Memory Throughput                   %        46.76
    DRAM Throughput                     %        46.76
    Duration                      usecond        10.37
    L1/TEX Cache Throughput             %        33.75
    L2 Cache Throughput                 %        20.16
    SM Active Cycles                cycle     11923.65
    Compute (SM) Throughput             %        13.91
    ----------------------- ------------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        25.26
    Achieved Active Warps Per SM           warp        12.13
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 49.48%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (25.3%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        49356
    Total DRAM Elapsed Cycles        cycle      1266688
    Average L1 Active Cycles         cycle     11923.65
    Total L1 Elapsed Cycles          cycle      2881428
    Average L2 Active Cycles         cycle     14326.81
    Total L2 Elapsed Cycles          cycle       729396
    Average SM Active Cycles         cycle     11923.65
    Total SM Elapsed Cycles          cycle      2881428
    Average SMSP Active Cycles       cycle     11285.94
    Total SMSP Elapsed Cycles        cycle     11525712
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 22.05%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 41.64% above the average, while the minimum instance value is 66.99% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 21.95%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 43.79% above the average, while the minimum instance value is 72.08% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 22.05%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 41.64% above the average, while the minimum instance value is 66.99% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.21
    SM Frequency            cycle/nsecond         2.21
    Elapsed Cycles                  cycle        44308
    Memory Throughput                   %        72.34
    DRAM Throughput                     %        72.34
    Duration                      usecond        19.97
    L1/TEX Cache Throughput             %        32.77
    L2 Cache Throughput                 %        31.40
    SM Active Cycles                cycle     34572.54
    Compute (SM) Throughput             %        18.48
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        36.08
    Achieved Active Warps Per SM           warp        17.32
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.84%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (36.1%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    147414.67
    Total DRAM Elapsed Cycles        cycle      2445312
    Average L1 Active Cycles         cycle     34572.54
    Total L1 Elapsed Cycles          cycle      5521810
    Average L2 Active Cycles         cycle     32620.42
    Total L2 Elapsed Cycles          cycle      1405188
    Average SM Active Cycles         cycle     34572.54
    Total SM Elapsed Cycles          cycle      5521810
    Average SMSP Active Cycles       cycle     34045.67
    Total SMSP Elapsed Cycles        cycle     22087240
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 13.29%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 16.59% above the average, while the minimum instance value is 32.86% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.78%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 16.19% above the average, while the minimum instance value is 31.74% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 13.29%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 16.59% above the average, while the minimum instance value is 32.86% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.22
    SM Frequency            cycle/nsecond         2.19
    Elapsed Cycles                  cycle        43583
    Memory Throughput                   %        72.70
    DRAM Throughput                     %        72.70
    Duration                      usecond        19.84
    L1/TEX Cache Throughput             %        33.65
    L2 Cache Throughput                 %        31.80
    SM Active Cycles                cycle     33646.20
    Compute (SM) Throughput             %        18.51
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        37.66
    Achieved Active Warps Per SM           warp        18.08
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.68%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (37.7%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    147405.33
    Total DRAM Elapsed Cycles        cycle      2433024
    Average L1 Active Cycles         cycle     33646.20
    Total L1 Elapsed Cycles          cycle      5512072
    Average L2 Active Cycles         cycle     32467.47
    Total L2 Elapsed Cycles          cycle      1387512
    Average SM Active Cycles         cycle     33646.20
    Total SM Elapsed Cycles          cycle      5512072
    Average SMSP Active Cycles       cycle     34160.16
    Total SMSP Elapsed Cycles        cycle     22048288
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 13.58%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 17.38% above the average, while the minimum instance value is 28.42% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.99%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 16.38% above the average, while the minimum instance value is 29.13% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 13.58%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 17.38% above the average, while the minimum instance value is 28.42% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.21
    SM Frequency            cycle/nsecond         2.19
    Elapsed Cycles                  cycle        43304
    Memory Throughput                   %        73.26
    DRAM Throughput                     %        73.26
    Duration                      usecond        19.71
    L1/TEX Cache Throughput             %        33.46
    L2 Cache Throughput                 %        31.97
    SM Active Cycles                cycle     33854.96
    Compute (SM) Throughput             %        18.60
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        36.65
    Achieved Active Warps Per SM           warp        17.59
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.7%                                                                                     
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (36.6%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    147405.33
    Total DRAM Elapsed Cycles        cycle      2414592
    Average L1 Active Cycles         cycle     33854.96
    Total L1 Elapsed Cycles          cycle      5484464
    Average L2 Active Cycles         cycle     32647.75
    Total L2 Elapsed Cycles          cycle      1380096
    Average SM Active Cycles         cycle     33854.96
    Total SM Elapsed Cycles          cycle      5484464
    Average SMSP Active Cycles       cycle     34515.98
    Total SMSP Elapsed Cycles        cycle     21937856
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 13.02%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 16.47% above the average, while the minimum instance value is 30.57% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 13.28%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 16.48% above the average, while the minimum instance value is 29.92% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 13.02%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 16.47% above the average, while the minimum instance value is 30.57% below the      
          average.                                                                                                      

  MultKeyAccum_8(Params *, int, int, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long **) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.24
    SM Frequency            cycle/nsecond         2.23
    Elapsed Cycles                  cycle       180771
    Memory Throughput                   %        92.36
    DRAM Throughput                     %        92.36
    Duration                      usecond        80.93
    L1/TEX Cache Throughput             %         9.93
    L2 Cache Throughput                 %        38.65
    SM Active Cycles                cycle    174204.22
    Compute (SM) Throughput             %        10.15
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                2.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           12
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        87.30
    Achieved Active Warps Per SM           warp        41.90
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 12.7%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (87.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       765156
    Total DRAM Elapsed Cycles        cycle      9940992
    Average L1 Active Cycles         cycle    174204.22
    Total L1 Elapsed Cycles          cycle     22107494
    Average L2 Active Cycles         cycle    146989.81
    Total L2 Elapsed Cycles          cycle      5722272
    Average SM Active Cycles         cycle    174204.22
    Total SM Elapsed Cycles          cycle     22107494
    Average SMSP Active Cycles       cycle    166644.62
    Total SMSP Elapsed Cycles        cycle     88429976
    -------------------------- ----------- ------------

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.21
    SM Frequency            cycle/nsecond         2.19
    Elapsed Cycles                  cycle        32316
    Memory Throughput                   %        65.58
    DRAM Throughput                     %        65.58
    Duration                      usecond        14.69
    L1/TEX Cache Throughput             %        28.11
    L2 Cache Throughput                 %        28.63
    SM Active Cycles                cycle     27134.95
    Compute (SM) Throughput             %        16.90
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        24.77
    Achieved Active Warps Per SM           warp        11.89
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50.46%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (24.8%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     98381.33
    Total DRAM Elapsed Cycles        cycle      1800192
    Average L1 Active Cycles         cycle     27134.95
    Total L1 Elapsed Cycles          cycle      4141028
    Average L2 Active Cycles         cycle     22821.17
    Total L2 Elapsed Cycles          cycle      1027584
    Average SM Active Cycles         cycle     27134.95
    Total SM Elapsed Cycles          cycle      4141028
    Average SMSP Active Cycles       cycle     26608.73
    Total SMSP Elapsed Cycles        cycle     16564112
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.959%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 8.30% above the average, while the minimum instance value is 21.94% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.311%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 7.67% above the average, while the minimum instance value is 22.39% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.959%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 8.30% above the average, while the minimum instance value is 21.94% below the       
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.22
    SM Frequency            cycle/nsecond         2.21
    Elapsed Cycles                  cycle        42805
    Memory Throughput                   %        74.78
    DRAM Throughput                     %        74.78
    Duration                      usecond        19.30
    L1/TEX Cache Throughput             %        32.85
    L2 Cache Throughput                 %        32.49
    SM Active Cycles                cycle     34459.30
    Compute (SM) Throughput             %        18.03
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        36.63
    Achieved Active Warps Per SM           warp        17.58
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.73%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (36.6%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       147416
    Total DRAM Elapsed Cycles        cycle      2365440
    Average L1 Active Cycles         cycle     34459.30
    Total L1 Elapsed Cycles          cycle      5658004
    Average L2 Active Cycles         cycle     32729.47
    Total L2 Elapsed Cycles          cycle      1358208
    Average SM Active Cycles         cycle     34459.30
    Total SM Elapsed Cycles          cycle      5658004
    Average SMSP Active Cycles       cycle     34380.71
    Total SMSP Elapsed Cycles        cycle     22632016
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 10.79%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 13.84% above the average, while the minimum instance value is 30.71% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.22%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 14.42% above the average, while the minimum instance value is 30.08% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 10.79%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 13.84% above the average, while the minimum instance value is 30.71% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.22
    SM Frequency            cycle/nsecond         2.18
    Elapsed Cycles                  cycle        42795
    Memory Throughput                   %        73.98
    DRAM Throughput                     %        73.98
    Duration                      usecond        19.49
    L1/TEX Cache Throughput             %        33.17
    L2 Cache Throughput                 %        32.37
    SM Active Cycles                cycle     34105.40
    Compute (SM) Throughput             %        18.47
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        37.25
    Achieved Active Warps Per SM           warp        17.88
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.51%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (37.2%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    147417.33
    Total DRAM Elapsed Cycles        cycle      2391040
    Average L1 Active Cycles         cycle     34105.40
    Total L1 Elapsed Cycles          cycle      5524104
    Average L2 Active Cycles         cycle     32561.44
    Total L2 Elapsed Cycles          cycle      1363176
    Average SM Active Cycles         cycle     34105.40
    Total SM Elapsed Cycles          cycle      5524104
    Average SMSP Active Cycles       cycle     34649.15
    Total SMSP Elapsed Cycles        cycle     22096416
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.69%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 14.80% above the average, while the minimum instance value is 31.17% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 13.12%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 16.34% above the average, while the minimum instance value is 26.74% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.69%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 14.80% above the average, while the minimum instance value is 31.17% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.22
    SM Frequency            cycle/nsecond         2.18
    Elapsed Cycles                  cycle        42598
    Memory Throughput                   %        74.27
    DRAM Throughput                     %        74.27
    Duration                      usecond        19.42
    L1/TEX Cache Throughput             %        33.03
    L2 Cache Throughput                 %        32.48
    SM Active Cycles                cycle     34268.64
    Compute (SM) Throughput             %        18.46
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        36.57
    Achieved Active Warps Per SM           warp        17.55
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.87%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (36.6%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       147408
    Total DRAM Elapsed Cycles        cycle      2381824
    Average L1 Active Cycles         cycle     34268.64
    Total L1 Elapsed Cycles          cycle      5525616
    Average L2 Active Cycles         cycle     32219.03
    Total L2 Elapsed Cycles          cycle      1358568
    Average SM Active Cycles         cycle     34268.64
    Total SM Elapsed Cycles          cycle      5525616
    Average SMSP Active Cycles       cycle     34172.42
    Total SMSP Elapsed Cycles        cycle     22102464
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.07%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 13.94% above the average, while the minimum instance value is 27.42% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.09%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 15.27% above the average, while the minimum instance value is 30.65% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.07%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 13.94% above the average, while the minimum instance value is 27.42% below the      
          average.                                                                                                      

  NTTPhase2_general(Params *, int, int, int, int, int, int, unsigned long *, unsigned long *, const unsigned long *, const unsigned long *, const DModulus *) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.23
    SM Frequency            cycle/nsecond         2.19
    Elapsed Cycles                  cycle        43996
    Memory Throughput                   %        71.95
    DRAM Throughput                     %        71.95
    Duration                      usecond        20.03
    L1/TEX Cache Throughput             %        32.63
    L2 Cache Throughput                 %        31.47
    SM Active Cycles                cycle     34675.52
    Compute (SM) Throughput             %        18.50
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            8.19
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                5.33
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block            7
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        35.97
    Achieved Active Warps Per SM           warp        17.26
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.07%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (36.0%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle    147406.67
    Total DRAM Elapsed Cycles        cycle      2458624
    Average L1 Active Cycles         cycle     34675.52
    Total L1 Elapsed Cycles          cycle      5514920
    Average L2 Active Cycles         cycle     32603.19
    Total L2 Elapsed Cycles          cycle      1402272
    Average SM Active Cycles         cycle     34675.52
    Total SM Elapsed Cycles          cycle      5514920
    Average SMSP Active Cycles       cycle     34585.57
    Total SMSP Elapsed Cycles        cycle     22059680
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 12.72%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 15.80% above the average, while the minimum instance value is 31.39% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.37%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 15.41% above the average, while the minimum instance value is 30.37% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.72%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 15.80% above the average, while the minimum instance value is 31.39% below the      
          average.                                                                                                      

  MultKeyAccum_8(Params *, int, int, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long *, unsigned long **) (4096, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9

    NVTX Push/Pop Stack for Thread 41596:
     <default domain>
        <0,compute>
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond        10.24
    SM Frequency            cycle/nsecond         2.23
    Elapsed Cycles                  cycle       180264
    Memory Throughput                   %        92.73
    DRAM Throughput                     %        92.73
    Duration                      usecond        80.70
    L1/TEX Cache Throughput             %         9.97
    L2 Cache Throughput                 %        38.77
    SM Active Cycles                cycle    173440.96
    Compute (SM) Throughput             %        10.19
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             128
    Threads                                   thread          524288
    Uses Green Context                                             0
    Waves Per SM                                                2.67
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           12
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        87.78
    Achieved Active Warps Per SM           warp        42.14
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 12.22%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (87.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       766192
    Total DRAM Elapsed Cycles        cycle      9915392
    Average L1 Active Cycles         cycle    173440.96
    Total L1 Elapsed Cycles          cycle     22035444
    Average L2 Active Cycles         cycle    146800.94
    Total L2 Elapsed Cycles          cycle      5705856
    Average SM Active Cycles         cycle    173440.96
    Total SM Elapsed Cycles          cycle     22035444
    Average SMSP Active Cycles       cycle    165840.65
    Total SMSP Elapsed Cycles        cycle     88141776
    -------------------------- ----------- ------------

