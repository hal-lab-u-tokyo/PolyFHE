Starting Benchmarking...
==PROF== Connected to process 807360 (/opt/mount/PolyFHE/build-noopt/bench)
[36m[INFO][polyfhe/kernel/device_context.cu][231][FHEContext][39m Initializing Params
### Warm up and Test
N : 65536
n1: 256
n2: 256
L : 4
q[0] : 998244353
root[0] : 3
------------------------------
==PROF== Profiling "Add" - 0: 0%....50%....100% - 9 passes
==PROF== Profiling "NTTPhase1" - 1: 0%....50%....100% - 9 passes
==PROF== Profiling "NTTPhase2" - 2: 0%....50%....100% - 9 passes
==PROF== Profiling "Mult" - 3: 0%....50%....100% - 9 passes
==PROF== Profiling "Mult" - 4: 0%....50%....100% - 9 passes
==PROF== Profiling "Add" - 5: 0%....50%....100% - 9 passes
Check passed
### Benchmark
==PROF== Profiling "Add" - 6: 0%....50%....100% - 9 passes
==PROF== Profiling "NTTPhase1" - 7: 0%....50%....100% - 9 passes
==PROF== Profiling "NTTPhase2" - 8: 0%....50%....100% - 9 passes
==PROF== Profiling "Mult" - 9: 0%....50%....100% - 9 passes
==PROF== Profiling "Mult" - 10: 0%....50%....100% - 9 passes
==PROF== Profiling "Add" - 11: 0%....50%....100% - 9 passes
Elapsed time: 324139us
==PROF== Profiling "Add" - 12: 0%....50%....100% - 9 passes
==PROF== Profiling "NTTPhase1" - 13: 0%....50%....100% - 9 passes
==PROF== Profiling "NTTPhase2" - 14: 0%....50%....100% - 9 passes
==PROF== Profiling "Mult" - 15: 0%....50%....100% - 9 passes
==PROF== Profiling "Mult" - 16: 0%....50%....100% - 9 passes
==PROF== Profiling "Add" - 17: 0%....50%....100% - 9 passes
Elapsed time: 319947us
==PROF== Profiling "Add" - 18: 0%....50%....100% - 9 passes
==PROF== Profiling "NTTPhase1" - 19: 0%....50%....100% - 9 passes
==PROF== Profiling "NTTPhase2" - 20: 0%....50%....100% - 9 passes
==PROF== Profiling "Mult" - 21: 0%....50%....100% - 9 passes
==PROF== Profiling "Mult" - 22: 0%....50%....100% - 9 passes
==PROF== Profiling "Add" - 23: 0%....50%....100% - 9 passes
Elapsed time: 319485us
==PROF== Profiling "Add" - 24: 0%....50%....100% - 9 passes
==PROF== Profiling "NTTPhase1" - 25: 0%....50%....100% - 9 passes
==PROF== Profiling "NTTPhase2" - 26: 0%....50%....100% - 9 passes
==PROF== Profiling "Mult" - 27: 0%....50%....100% - 9 passes
==PROF== Profiling "Mult" - 28: 0%....50%....100% - 9 passes
==PROF== Profiling "Add" - 29: 0%....50%....100% - 9 passes
Elapsed time: 320318us
==PROF== Profiling "Add" - 30: 0%....50%....100% - 9 passes
==PROF== Profiling "NTTPhase1" - 31: 0%....50%....100% - 9 passes
==PROF== Profiling "NTTPhase2" - 32: 0%....50%....100% - 9 passes
==PROF== Profiling "Mult" - 33: 0%....50%....100% - 9 passes
==PROF== Profiling "Mult" - 34: 0%....50%....100% - 9 passes
==PROF== Profiling "Add" - 35: 0%....50%....100% - 9 passes
Elapsed time: 319616us
Average time: 319842us
Finished Benchmarking...
==PROF== Disconnected from process 807360
[807360] bench@127.0.0.1
  Add(Params *, int, int, unsigned long *, unsigned long *, unsigned long *) (2048, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         5.16
    SM Frequency            cycle/usecond       583.79
    Elapsed Cycles                  cycle        12893
    Memory Throughput                   %        70.47
    DRAM Throughput                     %        70.47
    Duration                      usecond        22.08
    L1/TEX Cache Throughput             %        31.48
    L2 Cache Throughput                 %        49.99
    SM Active Cycles                cycle     10841.69
    Compute (SM) Throughput             %        26.48
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2048
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread          262144
    Waves Per SM                                                3.56
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 25%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 3 full waves and a partial wave of 319 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 25.0% of the total kernel runtime with a lower occupancy of 21.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           21
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.57
    Achieved Active Warps Per SM           warp        37.71
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.43%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

  NTTPhase1(Params *, int, int, unsigned long *, unsigned long *) (1024, 1, 1)x(32, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         6.29
    SM Frequency            cycle/usecond       711.43
    Elapsed Cycles                  cycle        42919
    Memory Throughput                   %        53.51
    DRAM Throughput                     %        11.65
    Duration                      usecond        60.32
    L1/TEX Cache Throughput             %        63.54
    L2 Cache Throughput                 %        53.51
    SM Active Cycles                cycle     39401.60
    Compute (SM) Throughput             %        19.19
    ----------------------- ------------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              68
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            2.05
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread           32768
    Waves Per SM                                                1.33
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 255 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 25.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           28
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        24.89
    Achieved Active Warps Per SM           warp        11.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

  NTTPhase2(Params *, int, int, unsigned long *, unsigned long *, unsigned long *) (1024, 1, 1)x(32, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         6.30
    SM Frequency            cycle/usecond       712.92
    Elapsed Cycles                  cycle        33362
    Memory Throughput                   %        51.01
    DRAM Throughput                     %        51.01
    Duration                      usecond        46.78
    L1/TEX Cache Throughput             %        51.35
    L2 Cache Throughput                 %        32.30
    SM Active Cycles                cycle     30322.17
    Compute (SM) Throughput             %        27.87
    ----------------------- ------------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            2.05
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread           32768
    Waves Per SM                                                1.33
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 255 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 24.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           24
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        25.15
    Achieved Active Warps Per SM           warp        12.07
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

  Mult(Params *, int, int, unsigned long *, unsigned long *, unsigned long *) (2048, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         4.92
    SM Frequency            cycle/usecond       556.74
    Elapsed Cycles                  cycle        14504
    Memory Throughput                   %        63.93
    DRAM Throughput                     %        63.93
    Duration                      usecond        26.05
    L1/TEX Cache Throughput             %        27.25
    L2 Cache Throughput                 %        44.45
    SM Active Cycles                cycle     12525.69
    Compute (SM) Throughput             %        48.11
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2048
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread          262144
    Waves Per SM                                                3.56
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 25%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 3 full waves and a partial wave of 319 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 25.0% of the total kernel runtime with a lower occupancy of 20.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           21
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.71
    Achieved Active Warps Per SM           warp        38.26
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.29%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

  Mult(Params *, int, int, unsigned long *, unsigned long *, unsigned long *) (2048, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         5.11
    SM Frequency            cycle/usecond       578.24
    Elapsed Cycles                  cycle        14589
    Memory Throughput                   %        63.99
    DRAM Throughput                     %        63.99
    Duration                      usecond        25.22
    L1/TEX Cache Throughput             %        27.19
    L2 Cache Throughput                 %        44.25
    SM Active Cycles                cycle     12555.60
    Compute (SM) Throughput             %        47.88
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2048
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread          262144
    Waves Per SM                                                3.56
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 25%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 3 full waves and a partial wave of 319 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 25.0% of the total kernel runtime with a lower occupancy of 21.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           21
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.85
    Achieved Active Warps Per SM           warp        37.85
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.15%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

  Add(Params *, int, int, unsigned long *, unsigned long *, unsigned long *) (2048, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         5.35
    SM Frequency            cycle/usecond       604.50
    Elapsed Cycles                  cycle        13141
    Memory Throughput                   %        69.78
    DRAM Throughput                     %        69.78
    Duration                      usecond        21.73
    L1/TEX Cache Throughput             %        31.48
    L2 Cache Throughput                 %        49.13
    SM Active Cycles                cycle     10844.08
    Compute (SM) Throughput             %        25.99
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2048
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread          262144
    Waves Per SM                                                3.56
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 25%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 3 full waves and a partial wave of 319 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 25.0% of the total kernel runtime with a lower occupancy of 22.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           21
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.76
    Achieved Active Warps Per SM           warp        37.32
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.24%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

  Add(Params *, int, int, unsigned long *, unsigned long *, unsigned long *) (2048, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         5.20
    SM Frequency            cycle/usecond       586.71
    Elapsed Cycles                  cycle        13108
    Memory Throughput                   %        69.46
    DRAM Throughput                     %        69.46
    Duration                      usecond        22.34
    L1/TEX Cache Throughput             %        31.44
    L2 Cache Throughput                 %        49.17
    SM Active Cycles                cycle     10857.96
    Compute (SM) Throughput             %        26.05
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2048
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread          262144
    Waves Per SM                                                3.56
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 25%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 3 full waves and a partial wave of 319 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 25.0% of the total kernel runtime with a lower occupancy of 21.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           21
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.56
    Achieved Active Warps Per SM           warp        37.71
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.44%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

  NTTPhase1(Params *, int, int, unsigned long *, unsigned long *) (1024, 1, 1)x(32, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         6.42
    SM Frequency            cycle/usecond       727.51
    Elapsed Cycles                  cycle        43492
    Memory Throughput                   %        52.57
    DRAM Throughput                     %        11.28
    Duration                      usecond        59.78
    L1/TEX Cache Throughput             %        62.73
    L2 Cache Throughput                 %        52.57
    SM Active Cycles                cycle     39650.44
    Compute (SM) Throughput             %        18.94
    ----------------------- ------------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              68
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            2.05
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread           32768
    Waves Per SM                                                1.33
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 255 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 26.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           28
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        24.63
    Achieved Active Warps Per SM           warp        11.82
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

  NTTPhase2(Params *, int, int, unsigned long *, unsigned long *, unsigned long *) (1024, 1, 1)x(32, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         6.20
    SM Frequency            cycle/usecond       702.07
    Elapsed Cycles                  cycle        33480
    Memory Throughput                   %        50.83
    DRAM Throughput                     %        50.83
    Duration                      usecond        47.68
    L1/TEX Cache Throughput             %        51.24
    L2 Cache Throughput                 %        32.20
    SM Active Cycles                cycle     30394.42
    Compute (SM) Throughput             %        27.77
    ----------------------- ------------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            2.05
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread           32768
    Waves Per SM                                                1.33
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 255 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 25.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           24
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        24.92
    Achieved Active Warps Per SM           warp        11.96
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

  Mult(Params *, int, int, unsigned long *, unsigned long *, unsigned long *) (2048, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         4.98
    SM Frequency            cycle/usecond       563.73
    Elapsed Cycles                  cycle        14689
    Memory Throughput                   %        63.73
    DRAM Throughput                     %        63.73
    Duration                      usecond        26.05
    L1/TEX Cache Throughput             %        26.66
    L2 Cache Throughput                 %        43.95
    SM Active Cycles                cycle     12803.25
    Compute (SM) Throughput             %        47.49
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2048
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread          262144
    Waves Per SM                                                3.56
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 25%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 3 full waves and a partial wave of 319 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 25.0% of the total kernel runtime with a lower occupancy of 21.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           21
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.43
    Achieved Active Warps Per SM           warp        37.65
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.57%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

  Mult(Params *, int, int, unsigned long *, unsigned long *, unsigned long *) (2048, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         5.08
    SM Frequency            cycle/usecond       575.12
    Elapsed Cycles                  cycle        14507
    Memory Throughput                   %        64.30
    DRAM Throughput                     %        64.30
    Duration                      usecond        25.22
    L1/TEX Cache Throughput             %        27.32
    L2 Cache Throughput                 %        44.61
    SM Active Cycles                cycle     12494.48
    Compute (SM) Throughput             %        48.09
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2048
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread          262144
    Waves Per SM                                                3.56
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 25%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 3 full waves and a partial wave of 319 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 25.0% of the total kernel runtime with a lower occupancy of 20.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           21
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.95
    Achieved Active Warps Per SM           warp        38.37
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.05%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

  Add(Params *, int, int, unsigned long *, unsigned long *, unsigned long *) (2048, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         5.19
    SM Frequency            cycle/usecond       586.55
    Elapsed Cycles                  cycle        12956
    Memory Throughput                   %        70.24
    DRAM Throughput                     %        70.24
    Duration                      usecond        22.08
    L1/TEX Cache Throughput             %        31.40
    L2 Cache Throughput                 %        49.75
    SM Active Cycles                cycle     10869.06
    Compute (SM) Throughput             %        26.36
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2048
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread          262144
    Waves Per SM                                                3.56
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 25%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 3 full waves and a partial wave of 319 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 25.0% of the total kernel runtime with a lower occupancy of 21.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           21
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.51
    Achieved Active Warps Per SM           warp        37.68
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.49%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

  Add(Params *, int, int, unsigned long *, unsigned long *, unsigned long *) (2048, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         5.21
    SM Frequency            cycle/usecond       588.93
    Elapsed Cycles                  cycle        13046
    Memory Throughput                   %        69.57
    DRAM Throughput                     %        69.57
    Duration                      usecond        22.14
    L1/TEX Cache Throughput             %        31.46
    L2 Cache Throughput                 %        49.41
    SM Active Cycles                cycle     10850.94
    Compute (SM) Throughput             %        26.17
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2048
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread          262144
    Waves Per SM                                                3.56
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 25%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 3 full waves and a partial wave of 319 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 25.0% of the total kernel runtime with a lower occupancy of 21.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           21
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.28
    Achieved Active Warps Per SM           warp        37.57
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.72%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

  NTTPhase1(Params *, int, int, unsigned long *, unsigned long *) (1024, 1, 1)x(32, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         6.31
    SM Frequency            cycle/usecond       714.25
    Elapsed Cycles                  cycle        42907
    Memory Throughput                   %        53.23
    DRAM Throughput                     %        11.47
    Duration                      usecond        60.06
    L1/TEX Cache Throughput             %        63.57
    L2 Cache Throughput                 %        53.23
    SM Active Cycles                cycle        39768
    Compute (SM) Throughput             %        19.19
    ----------------------- ------------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              68
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            2.05
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread           32768
    Waves Per SM                                                1.33
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 255 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 26.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           28
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        24.53
    Achieved Active Warps Per SM           warp        11.77
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

  NTTPhase2(Params *, int, int, unsigned long *, unsigned long *, unsigned long *) (1024, 1, 1)x(32, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         6.19
    SM Frequency            cycle/usecond       700.69
    Elapsed Cycles                  cycle        33485
    Memory Throughput                   %        50.78
    DRAM Throughput                     %        50.78
    Duration                      usecond        47.78
    L1/TEX Cache Throughput             %        50.71
    L2 Cache Throughput                 %        32.19
    SM Active Cycles                cycle     30722.10
    Compute (SM) Throughput             %        27.77
    ----------------------- ------------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            2.05
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread           32768
    Waves Per SM                                                1.33
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 255 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 26.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           24
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        24.65
    Achieved Active Warps Per SM           warp        11.83
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

  Mult(Params *, int, int, unsigned long *, unsigned long *, unsigned long *) (2048, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         4.96
    SM Frequency            cycle/usecond       561.52
    Elapsed Cycles                  cycle        14470
    Memory Throughput                   %        63.54
    DRAM Throughput                     %        63.54
    Duration                      usecond        25.76
    L1/TEX Cache Throughput             %        27.15
    L2 Cache Throughput                 %        44.61
    SM Active Cycles                cycle     12570.73
    Compute (SM) Throughput             %        48.21
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2048
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread          262144
    Waves Per SM                                                3.56
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           21
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        80.08
    Achieved Active Warps Per SM           warp        38.44
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 19.92%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (80.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

  Mult(Params *, int, int, unsigned long *, unsigned long *, unsigned long *) (2048, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         5.06
    SM Frequency            cycle/usecond       573.70
    Elapsed Cycles                  cycle        14510
    Memory Throughput                   %        64.19
    DRAM Throughput                     %        64.19
    Duration                      usecond        25.28
    L1/TEX Cache Throughput             %        27.41
    L2 Cache Throughput                 %        44.48
    SM Active Cycles                cycle     12453.79
    Compute (SM) Throughput             %        48.07
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2048
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread          262144
    Waves Per SM                                                3.56
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 25%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 3 full waves and a partial wave of 319 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 25.0% of the total kernel runtime with a lower occupancy of 20.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           21
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.69
    Achieved Active Warps Per SM           warp        38.25
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.31%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

  Add(Params *, int, int, unsigned long *, unsigned long *, unsigned long *) (2048, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         5.19
    SM Frequency            cycle/usecond       587.65
    Elapsed Cycles                  cycle        12942
    Memory Throughput                   %        70.50
    DRAM Throughput                     %        70.50
    Duration                      usecond        22.02
    L1/TEX Cache Throughput             %        31.28
    L2 Cache Throughput                 %        49.81
    SM Active Cycles                cycle     10910.71
    Compute (SM) Throughput             %        26.38
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2048
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread          262144
    Waves Per SM                                                3.56
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 25%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 3 full waves and a partial wave of 319 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 25.0% of the total kernel runtime with a lower occupancy of 22.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           21
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.47
    Achieved Active Warps Per SM           warp        37.19
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.53%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

  Add(Params *, int, int, unsigned long *, unsigned long *, unsigned long *) (2048, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         5.20
    SM Frequency            cycle/usecond       587.76
    Elapsed Cycles                  cycle        12981
    Memory Throughput                   %        69.97
    DRAM Throughput                     %        69.97
    Duration                      usecond        22.08
    L1/TEX Cache Throughput             %        31.25
    L2 Cache Throughput                 %        49.65
    SM Active Cycles                cycle     10921.73
    Compute (SM) Throughput             %        26.30
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2048
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread          262144
    Waves Per SM                                                3.56
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 25%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 3 full waves and a partial wave of 319 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 25.0% of the total kernel runtime with a lower occupancy of 21.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           21
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.10
    Achieved Active Warps Per SM           warp        37.49
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.9%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

  NTTPhase1(Params *, int, int, unsigned long *, unsigned long *) (1024, 1, 1)x(32, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         6.31
    SM Frequency            cycle/usecond       713.70
    Elapsed Cycles                  cycle        42919
    Memory Throughput                   %        53.28
    DRAM Throughput                     %        11.43
    Duration                      usecond        60.13
    L1/TEX Cache Throughput             %        63.58
    L2 Cache Throughput                 %        53.28
    SM Active Cycles                cycle     39590.54
    Compute (SM) Throughput             %        19.19
    ----------------------- ------------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              68
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            2.05
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread           32768
    Waves Per SM                                                1.33
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 255 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 25.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           28
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        24.88
    Achieved Active Warps Per SM           warp        11.94
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

  NTTPhase2(Params *, int, int, unsigned long *, unsigned long *, unsigned long *) (1024, 1, 1)x(32, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         6.23
    SM Frequency            cycle/usecond       705.26
    Elapsed Cycles                  cycle        33341
    Memory Throughput                   %        51.02
    DRAM Throughput                     %        51.02
    Duration                      usecond        47.26
    L1/TEX Cache Throughput             %        51.54
    L2 Cache Throughput                 %        32.33
    SM Active Cycles                cycle     30228.69
    Compute (SM) Throughput             %        27.89
    ----------------------- ------------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            2.05
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread           32768
    Waves Per SM                                                1.33
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 255 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 24.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           24
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        25.18
    Achieved Active Warps Per SM           warp        12.08
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

  Mult(Params *, int, int, unsigned long *, unsigned long *, unsigned long *) (2048, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         5.11
    SM Frequency            cycle/usecond       578.09
    Elapsed Cycles                  cycle        14840
    Memory Throughput                   %        62.82
    DRAM Throughput                     %        62.82
    Duration                      usecond        25.66
    L1/TEX Cache Throughput             %        27.44
    L2 Cache Throughput                 %        43.44
    SM Active Cycles                cycle        12441
    Compute (SM) Throughput             %        47.01
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2048
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread          262144
    Waves Per SM                                                3.56
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           21
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        80.39
    Achieved Active Warps Per SM           warp        38.59
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 19.61%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (80.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

  Mult(Params *, int, int, unsigned long *, unsigned long *, unsigned long *) (2048, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         5.07
    SM Frequency            cycle/usecond       572.58
    Elapsed Cycles                  cycle        14575
    Memory Throughput                   %        63.63
    DRAM Throughput                     %        63.63
    Duration                      usecond        25.44
    L1/TEX Cache Throughput             %        27.19
    L2 Cache Throughput                 %        44.35
    SM Active Cycles                cycle     12552.88
    Compute (SM) Throughput             %        47.86
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2048
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread          262144
    Waves Per SM                                                3.56
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 25%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 3 full waves and a partial wave of 319 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 25.0% of the total kernel runtime with a lower occupancy of 20.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           21
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.53
    Achieved Active Warps Per SM           warp        38.17
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.47%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

  Add(Params *, int, int, unsigned long *, unsigned long *, unsigned long *) (2048, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         5.32
    SM Frequency            cycle/usecond       602.81
    Elapsed Cycles                  cycle        13124
    Memory Throughput                   %        69.66
    DRAM Throughput                     %        69.66
    Duration                      usecond        21.76
    L1/TEX Cache Throughput             %        31.47
    L2 Cache Throughput                 %        49.20
    SM Active Cycles                cycle     10846.90
    Compute (SM) Throughput             %        26.02
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2048
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread          262144
    Waves Per SM                                                3.56
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 25%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 3 full waves and a partial wave of 319 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 25.0% of the total kernel runtime with a lower occupancy of 22.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           21
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.82
    Achieved Active Warps Per SM           warp        37.35
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.18%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

  Add(Params *, int, int, unsigned long *, unsigned long *, unsigned long *) (2048, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         5.21
    SM Frequency            cycle/usecond       589.81
    Elapsed Cycles                  cycle        12971
    Memory Throughput                   %        70.24
    DRAM Throughput                     %        70.24
    Duration                      usecond        21.98
    L1/TEX Cache Throughput             %        31.14
    L2 Cache Throughput                 %        49.69
    SM Active Cycles                cycle     10960.69
    Compute (SM) Throughput             %        26.32
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2048
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread          262144
    Waves Per SM                                                3.56
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 25%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 3 full waves and a partial wave of 319 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 25.0% of the total kernel runtime with a lower occupancy of 22.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           21
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.52
    Achieved Active Warps Per SM           warp        37.21
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.48%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

  NTTPhase1(Params *, int, int, unsigned long *, unsigned long *) (1024, 1, 1)x(32, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         6.35
    SM Frequency            cycle/usecond       718.36
    Elapsed Cycles                  cycle        43290
    Memory Throughput                   %        52.94
    DRAM Throughput                     %        11.24
    Duration                      usecond        60.26
    L1/TEX Cache Throughput             %        63.00
    L2 Cache Throughput                 %        52.94
    SM Active Cycles                cycle     39425.77
    Compute (SM) Throughput             %        19.02
    ----------------------- ------------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              68
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            2.05
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread           32768
    Waves Per SM                                                1.33
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 255 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 25.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           28
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        24.70
    Achieved Active Warps Per SM           warp        11.86
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

  NTTPhase2(Params *, int, int, unsigned long *, unsigned long *, unsigned long *) (1024, 1, 1)x(32, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         6.25
    SM Frequency            cycle/usecond       707.40
    Elapsed Cycles                  cycle        33307
    Memory Throughput                   %        51.06
    DRAM Throughput                     %        51.06
    Duration                      usecond        47.07
    L1/TEX Cache Throughput             %        51.40
    L2 Cache Throughput                 %        32.36
    SM Active Cycles                cycle     30329.65
    Compute (SM) Throughput             %        27.92
    ----------------------- ------------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            2.05
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread           32768
    Waves Per SM                                                1.33
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 255 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 24.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           24
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        25.04
    Achieved Active Warps Per SM           warp        12.02
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

  Mult(Params *, int, int, unsigned long *, unsigned long *, unsigned long *) (2048, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         5.02
    SM Frequency            cycle/usecond       568.63
    Elapsed Cycles                  cycle        14705
    Memory Throughput                   %        63.09
    DRAM Throughput                     %        63.09
    Duration                      usecond        25.86
    L1/TEX Cache Throughput             %        27.13
    L2 Cache Throughput                 %        43.83
    SM Active Cycles                cycle     12581.06
    Compute (SM) Throughput             %        47.43
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2048
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread          262144
    Waves Per SM                                                3.56
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 25%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 3 full waves and a partial wave of 319 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 25.0% of the total kernel runtime with a lower occupancy of 20.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           21
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.82
    Achieved Active Warps Per SM           warp        38.31
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.18%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

  Mult(Params *, int, int, unsigned long *, unsigned long *, unsigned long *) (2048, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         5.04
    SM Frequency            cycle/usecond       570.54
    Elapsed Cycles                  cycle        14627
    Memory Throughput                   %        63.75
    DRAM Throughput                     %        63.75
    Duration                      usecond        25.63
    L1/TEX Cache Throughput             %        27.37
    L2 Cache Throughput                 %        44.09
    SM Active Cycles                cycle     12473.08
    Compute (SM) Throughput             %        47.67
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2048
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread          262144
    Waves Per SM                                                3.56
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 25%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 3 full waves and a partial wave of 319 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 25.0% of the total kernel runtime with a lower occupancy of 20.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           21
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.39
    Achieved Active Warps Per SM           warp        38.11
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.61%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

  Add(Params *, int, int, unsigned long *, unsigned long *, unsigned long *) (2048, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         5.17
    SM Frequency            cycle/usecond       585.56
    Elapsed Cycles                  cycle        12895
    Memory Throughput                   %        70.75
    DRAM Throughput                     %        70.75
    Duration                      usecond        22.02
    L1/TEX Cache Throughput             %        31.55
    L2 Cache Throughput                 %        49.99
    SM Active Cycles                cycle     10818.19
    Compute (SM) Throughput             %        26.48
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2048
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread          262144
    Waves Per SM                                                3.56
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 25%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 3 full waves and a partial wave of 319 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 25.0% of the total kernel runtime with a lower occupancy of 21.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           21
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.23
    Achieved Active Warps Per SM           warp        37.55
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.77%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

  Add(Params *, int, int, unsigned long *, unsigned long *, unsigned long *) (2048, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         5.10
    SM Frequency            cycle/usecond       578.04
    Elapsed Cycles                  cycle        12881
    Memory Throughput                   %        70.81
    DRAM Throughput                     %        70.81
    Duration                      usecond        22.27
    L1/TEX Cache Throughput             %        31.28
    L2 Cache Throughput                 %        50.12
    SM Active Cycles                cycle     10913.81
    Compute (SM) Throughput             %        26.51
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2048
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread          262144
    Waves Per SM                                                3.56
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 25%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 3 full waves and a partial wave of 319 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 25.0% of the total kernel runtime with a lower occupancy of 21.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           21
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.07
    Achieved Active Warps Per SM           warp        37.47
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.93%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

  NTTPhase1(Params *, int, int, unsigned long *, unsigned long *) (1024, 1, 1)x(32, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         6.35
    SM Frequency            cycle/usecond       718.36
    Elapsed Cycles                  cycle        43610
    Memory Throughput                   %        52.64
    DRAM Throughput                     %        11.26
    Duration                      usecond        60.70
    L1/TEX Cache Throughput             %        62.34
    L2 Cache Throughput                 %        52.64
    SM Active Cycles                cycle     39592.67
    Compute (SM) Throughput             %        18.88
    ----------------------- ------------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              68
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            2.05
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread           32768
    Waves Per SM                                                1.33
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 255 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 26.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           28
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        24.64
    Achieved Active Warps Per SM           warp        11.83
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

  NTTPhase2(Params *, int, int, unsigned long *, unsigned long *, unsigned long *) (1024, 1, 1)x(32, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         6.21
    SM Frequency            cycle/usecond       703.01
    Elapsed Cycles                  cycle        33707
    Memory Throughput                   %        50.51
    DRAM Throughput                     %        50.51
    Duration                      usecond        47.94
    L1/TEX Cache Throughput             %        51.27
    L2 Cache Throughput                 %        31.99
    SM Active Cycles                cycle     30431.10
    Compute (SM) Throughput             %        27.59
    ----------------------- ------------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              80
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            2.05
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread           32768
    Waves Per SM                                                1.33
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 255 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 25.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           24
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        24.75
    Achieved Active Warps Per SM           warp        11.88
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

  Mult(Params *, int, int, unsigned long *, unsigned long *, unsigned long *) (2048, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         5.79
    SM Frequency            cycle/usecond       655.34
    Elapsed Cycles                  cycle        16867
    Memory Throughput                   %        55.45
    DRAM Throughput                     %        55.45
    Duration                      usecond        25.73
    L1/TEX Cache Throughput             %        27.21
    L2 Cache Throughput                 %        38.23
    SM Active Cycles                cycle     12545.46
    Compute (SM) Throughput             %        41.35
    ----------------------- ------------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2048
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread          262144
    Waves Per SM                                                3.56
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 25%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 3 full waves and a partial wave of 319 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 25.0% of the total kernel runtime with a lower occupancy of 21.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           21
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.00
    Achieved Active Warps Per SM           warp        37.92
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21%                                                                                       
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

  Mult(Params *, int, int, unsigned long *, unsigned long *, unsigned long *) (2048, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         5.06
    SM Frequency            cycle/usecond       571.07
    Elapsed Cycles                  cycle        14513
    Memory Throughput                   %        63.56
    DRAM Throughput                     %        63.56
    Duration                      usecond        25.41
    L1/TEX Cache Throughput             %        27.44
    L2 Cache Throughput                 %        44.41
    SM Active Cycles                cycle     12438.31
    Compute (SM) Throughput             %        48.05
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2048
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread          262144
    Waves Per SM                                                3.56
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 25%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 3 full waves and a partial wave of 319 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 25.0% of the total kernel runtime with a lower occupancy of 20.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           21
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.88
    Achieved Active Warps Per SM           warp        38.34
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.12%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

  Add(Params *, int, int, unsigned long *, unsigned long *, unsigned long *) (2048, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.6
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         5.22
    SM Frequency            cycle/usecond       588.77
    Elapsed Cycles                  cycle        12927
    Memory Throughput                   %        70.35
    DRAM Throughput                     %        70.35
    Duration                      usecond        21.95
    L1/TEX Cache Throughput             %        31.63
    L2 Cache Throughput                 %        49.86
    SM Active Cycles                cycle     10792.90
    Compute (SM) Throughput             %        26.41
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2048
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread          262144
    Waves Per SM                                                3.56
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 25%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 3 full waves and a partial wave of 319 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 25.0% of the total kernel runtime with a lower occupancy of 22.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           21
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.79
    Achieved Active Warps Per SM           warp        37.34
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.21%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

